<!DOCTYPE html><html class=no-js lang=zh> <head><meta charset=utf-8><meta content="width=device-width,initial-scale=1" name=viewport><meta content="d.run (DaoCloud Runs Intelligence)，揭示一个新一代软件体系下的全新算力世界，让算力更自由。" name=description><meta content=d.run name=author><link href=https://docs.d.run/blogs/0327-transformer.html rel=canonical><link href=0403-cp-to-profit.html rel=prev><link href=0326-compute-power.html rel=next><link href=../images/favicon.ico rel=icon><meta content="mkdocs-1.6.1, mkdocs-material-9.5.50" name=generator><title>谁将替代 Transformer - d.run 让算力更自由</title><link href=../assets/stylesheets/main.a40c8224.min.css rel=stylesheet><link href=../assets/stylesheets/palette.06af60db.min.css rel=stylesheet><link href=../overrides/assets/stylesheets/main.e13ced4c.min.css rel=stylesheet><link crossorigin href=https://fonts.gstatic.com rel=preconnect><link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback" rel=stylesheet><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body data-md-color-accent=indigo data-md-color-primary=indigo data-md-color-scheme=default dir=ltr> <input autocomplete=off class=md-toggle data-md-toggle=drawer id=__drawer type=checkbox> <input autocomplete=off class=md-toggle data-md-toggle=search id=__search type=checkbox> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a class=md-skip href=#transformer> 跳转至 </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> </div> </aside> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav aria-label=页眉 class="md-header__inner md-grid"> <a aria-label="d.run 让算力更自由" class="md-header__button md-logo" data-md-component=logo href=/ title="d.run 让算力更自由"> <img alt=logo src=../images/DaoCloud.png> </a> <label class="md-header__button md-icon" for=__drawer> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> d.run 让算力更自由 </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> 谁将替代 Transformer </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input aria-label="Switch to light mode" class=md-option data-md-color-accent=indigo data-md-color-media=(prefers-color-scheme) data-md-color-primary=indigo data-md-color-scheme=default id=__palette_0 name=__palette type=radio> <label class="md-header__button md-icon" for=__palette_1 hidden title="Switch to light mode"> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"></path></svg> </label> <input aria-label="Switch to dark mode" class=md-option data-md-color-accent=indigo data-md-color-media="(prefers-color-scheme: light)" data-md-color-primary=indigo data-md-color-scheme=default id=__palette_1 name=__palette type=radio> <label class="md-header__button md-icon" for=__palette_2 hidden title="Switch to dark mode"> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg> </label> <input aria-label="Switch to system preference" class=md-option data-md-color-accent=indigo data-md-color-media="(prefers-color-scheme: dark)" data-md-color-primary=indigo data-md-color-scheme=slate id=__palette_2 name=__palette type=radio> <label class="md-header__button md-icon" for=__palette_0 hidden title="Switch to system preference"> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <div class=md-header__option> <div class=md-select> <button aria-label=选择当前语言 class="md-header__button md-icon"> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="m12.87 15.07-2.54-2.51.03-.03A17.5 17.5 0 0 0 14.07 6H17V4h-7V2H8v2H1v2h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2zm-2.62 7 1.62-4.33L19.12 17z"></path></svg> </button> <div class=md-select__inner> <ul class=md-select__list> <li class=md-select__item> <a class=md-select__link href=/ hreflang=zh> 简体中文 </a> </li> </ul> </div> </div> </div> <label class="md-header__button md-icon" for=__search> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input aria-label=搜索 autocapitalize=off autocomplete=off autocorrect=off class=md-search__input data-md-component=search-query name=query placeholder=搜索 required spellcheck=false type=text> <label class="md-search__icon md-icon" for=__search> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg> </label> <nav aria-label=查找 class=md-search__options> <a aria-label=分享 class="md-search__icon md-icon" data-clipboard data-clipboard-text data-md-component=search-share href=javascript:void(0) tabindex=-1 title=分享> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"></path></svg> </a> <button aria-label=清空当前内容 class="md-search__icon md-icon" tabindex=-1 title=清空当前内容 type=reset> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix tabindex=0> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> 正在初始化搜索引擎 </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a class=md-source data-md-component=source href=https://github.com/d-run/drun-docs title=前往仓库> <div class="md-source__icon md-icon"> <svg viewbox="0 0 496 512" xmlns=http://www.w3.org/2000/svg><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg> </div> <div class=md-source__repository> d-run/drun-docs </div> </a> </div> </nav> <nav aria-label=标签 class=md-tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a class=md-tabs__link href=https://d.run> 首页 </a> </li> <li class=md-tabs__item> <a class=md-tabs__link href=../buy/index.html> d.run 文档 </a> </li> <li class=md-tabs__item> <a class=md-tabs__link href=../eco/index.html> 产品生态 </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a class=md-tabs__link href=index.html> AI 行业新闻 </a> </li> <li class=md-tabs__item> <a class=md-tabs__link href=../open/index.html> 智海拾贝 </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav aria-label=导航栏 class="md-nav md-nav--primary md-nav--lifted" data-md-level=0> <label class=md-nav__title for=__drawer> <a aria-label="d.run 让算力更自由" class="md-nav__button md-logo" data-md-component=logo href=/ title="d.run 让算力更自由"> <img alt=logo src=../images/DaoCloud.png> </a> d.run 让算力更自由 </label> <div class=md-nav__source> <a class=md-source data-md-component=source href=https://github.com/d-run/drun-docs title=前往仓库> <div class="md-source__icon md-icon"> <svg viewbox="0 0 496 512" xmlns=http://www.w3.org/2000/svg><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg> </div> <div class=md-source__repository> d-run/drun-docs </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a class=md-nav__link href=https://d.run> <span class=md-ellipsis> 首页 </span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a class=md-nav__link href=../buy/index.html> <span class=md-ellipsis> d.run 文档 </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a class=md-nav__link href=../eco/index.html> <span class=md-ellipsis> 产品生态 </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input checked class="md-nav__toggle md-toggle" id=__nav_4 type=checkbox> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex> <span class=md-ellipsis> AI 行业新闻 </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=true aria-labelledby=__nav_4_label class=md-nav data-md-level=1> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> AI 行业新闻 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a class=md-nav__link href=index.html> <span class=md-ellipsis> 索引 </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=d.run.html> <span class=md-ellipsis> d.run 是支撑生成式 AI 的理想平台 </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=0702-k8s-for-genai.html> <span class=md-ellipsis> K8s 与生成式 AI 珠联璧合 </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=0514-gpt4o.html> <span class=md-ellipsis> OpenAI GPT-4o 完全免费 </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=0509-model-spec.html> <span class=md-ellipsis> OpenAI 大型语言模型规范 </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=0429-ai-survey.html> <span class=md-ellipsis> 2024大规模AI基础设施形势调研 </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=0410-cnai-wp.html> <span class=md-ellipsis> 云原生人工智能白皮书 </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=0408-after-kimi.html> <span class=md-ellipsis> Kimi火了后国内其他大模型 </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=0407-dbrx.html> <span class=md-ellipsis> DBRX 开源 LLM 介绍 </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=0403-cp-to-profit.html> <span class=md-ellipsis> “AI 流程编排”化算力为“算利” </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" id=__toc type=checkbox> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> 谁将替代 Transformer </span> <span class="md-nav__icon md-icon"></span> </label> <a class="md-nav__link md-nav__link--active" href=0327-transformer.html> <span class=md-ellipsis> 谁将替代 Transformer </span> </a> <nav aria-label=导航 class="md-nav md-nav--secondary"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> 导航 </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a class=md-nav__link href=#transformer_1> <span class=md-ellipsis> Transformer 前世今生 </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#transformer_2> <span class=md-ellipsis> 当前的非 Transformer 架构研究正走向何方？ </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#transformer_3> <span class=md-ellipsis> Transformer 能否被颠覆？ </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=0326-compute-power.html> <span class=md-ellipsis> 金融行业迎来大模型时代 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a class=md-nav__link href=../open/index.html> <span class=md-ellipsis> 智海拾贝 </span> <span class="md-nav__icon md-icon"></span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav aria-label=导航 class="md-nav md-nav--secondary"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> 导航 </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a class=md-nav__link href=#transformer_1> <span class=md-ellipsis> Transformer 前世今生 </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#transformer_2> <span class=md-ellipsis> 当前的非 Transformer 架构研究正走向何方？ </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#transformer_3> <span class=md-ellipsis> Transformer 能否被颠覆？ </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a class="md-content__button md-icon" href=https://github.com/d-run/drun-docs/edit/main/docs/zh/docs/blogs/0327-transformer.md title=查看编辑历史> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M13.5 8H12v5l4.28 2.54.72-1.21-3.5-2.08zM13 3a9 9 0 0 0-9 9H1l3.96 4.03L9 12H6a7 7 0 0 1 7-7 7 7 0 0 1 7 7 7 7 0 0 1-7 7c-1.93 0-3.68-.79-4.94-2.06l-1.42 1.42A8.9 8.9 0 0 0 13 21a9 9 0 0 0 9-9 9 9 0 0 0-9-9"></path></svg> </a> <a class="md-content__button md-icon" href=https://github.com/d-run/drun-docs/edit/main/docs/zh/docs/blogs/0327-transformer.md title=edit.link.title> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"></path></svg> </a> <a class="md-content__button md-icon" href=https://github.com/d-run/drun-docs/edit/main/docs/zh/docs/blogs/0327-transformer.md title=查看源文件> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"></path></svg> </a> <h1 id=transformer>谁将替代 Transformer？<a class=headerlink href=#transformer title="Permanent link">¶</a></h1> <blockquote> <p>文章转载自 <a href=https://mp.weixin.qq.com/s/Q8PIn0FOuXkOT1TiIOuDaA>AI 科技评论</a></p> </blockquote> <p><img alt=图片 src=images/transformer01.png></p> <p>非 Transformer 面临的共同考验依然是证明自己的天花板有多高。</p> <h2 id=transformer_1>Transformer 前世今生<a class=headerlink href=#transformer_1 title="Permanent link">¶</a></h2> <p>2017 年谷歌发表的论文《Attention Is All You Need》成为当下人工智能的一篇圣经，此后席卷全球的人工智能热潮都可以直接追溯到 Transformer 的发明。</p> <p>Transformer 由于其处理局部和长程依赖关系的能力以及可并行化训练的特点，一经问世，逐步取代了过去的 RNN（循环神经网络）与 CNN（卷积神经网络），成为 NLP（自然语言处理）前沿研究的标准范式。</p> <p>今天主流的 AI 模型和产品——OpenAI 的 ChatGPT、谷歌的 Bard、Anthropic 的 Claude，Midjourney、Sora 到国内智谱 AI 的 ChatGLM 大模型、百川智能的 Baichuan 大模型、Kimi chat 等等——都是基于 Transformer 架构。</p> <p>Transformer 已然代表了当今人工智能技术无可争议的黄金标准，其主导地位至今无人能撼动。</p> <p>在 Transformer 大行其道的同时，出现了一些反对的声音，如：“Transformer 的效率并不高”；“Transformer 的天花板很容易窥见”；“Transformer 是很好，但并不能实现 AGI，实现一个 world model（世界模型）”。</p> <p>这是因为 Transformer 的强大之处同时也是它的弱点：Transformer 中固有的自注意力机制（attention）带来了挑战，主要是由于其二次复杂度造成的，这种复杂度使得该架构在涉及长输入序列或资源受限情况下 <strong>计算成本高昂且占用内存</strong> 。</p> <p>简单点说，这意味着当 Transformer 处理的序列长度（例如，段落中的单词数量或图像的大小）增加时，所需的算力就会按该序列的平方增加，从而迅速变得巨大，因此有说法认为“Transformer 效率不高”。这也是当下人工智能热潮引发了全球算力短缺的主要原因。</p> <p>基于 Transformer 的局限性，许多非 Transformer 架构顺势提出，其中包括中国的 RWKV、Meta 的 Mega、微软亚研的 Retnet、Mamba、DeepMind 团队的 Hawk 和 Griffin 等——它们都是在 Transformer 一统大模型研发江湖之后陆续被提出来的。</p> <p>他们大多在原来的 RNN 基础上，针对 Transformer 的缺陷和局限性来做改进，试图研究出所谓的「高效 Transformer」（efficient Transformer）结构，一个更像人类思考的架构。</p> <p>其中 efficient Transformer 是指占用的内存更小、训练和推理过程中的计算成本更小的模型，试图来推翻 Transformer 的霸权。</p> <h2 id=transformer_2>当前的非 Transformer 架构研究正走向何方？<a class=headerlink href=#transformer_2 title="Permanent link">¶</a></h2> <p>现在主流的非 Transformer 研究基本都是针对 attention 机制去优化 full attention 的部分，然后想办法将这一部分变成一个 RNN 模型，以此提高推理的效率。</p> <p>attention 是 Transformer 的核心——Transformer 模型之所以如此强大，是因为它抛弃了之前广泛采用的循环网络和卷积网络，而采用了一种特殊的结构——注意力机制（attention）来建模文本。</p> <p>attention 使模型能够考虑单词之间的关系、不管它们相距多远，并确定段落中哪些单词和短语最值得关注。</p> <p>这种机制使得 Transformer 实现了语言处理的并行化，即同时分析特定文本中的所有单词，而不是按顺序分析。Transformer 的并行化使它们对所读所写的文本有了更全面、更准确的理解，也使得它们比 RNN 具有更高的计算效率和可扩展性。</p> <p>相比之下，循环神经网络（RNNs）面临梯度消失的问题，使得它们难以对长序列进行训练，此外，在训练过程中无法在时间上并行化，进而限制了其可扩展性；卷积神经网络（CNNs）只擅长捕捉局部模式，在长程依赖方面还很欠缺，而这对于许多序列处理任务至关重要。</p> <p>但是 RNNs 的优势在于 RNN 模型做推理时，复杂度是恒定的，所以内存和计算需求是呈线性增长，相对于 Transformer 在序列长度上的内存和计算复杂性呈二次方增长，RNN 的内存与计算需求更低。因此，今天很多非 Transformer 研究都循着“保留 RNN 优势的同时，试图达到 Transformer 性能”的方向去努力。</p> <p><strong>基于这一目标，今天的非 Transformer 技术研究主要分为两个流派：</strong></p> <p>流派一是以 RWKV、 Mamba 和 S4 为代表，它们完全用 recurrent（循环）结构去替代 attention。这种思路是用一个固定的内存记住前面的信息，但目前看来虽然可以记住一定长度，但要达到更长的长度是有难度的。</p> <p>还有一个流派是把 full attention 这种密集结构变得稀疏，例如 Meta 的 Mega，在之后的计算中不再需要算所有 attention 矩阵中的每一个元素，模型效率也随之变高。</p> <p>具体分析各个非 Transformer 模型，其中 RWKV 是国产开源的首个非 Transformer 架构的大语言模型，目前已经迭代至第六代 RWKV-6。RWKV 的作者彭博在 2022 年 5 月开始训练 RWKV-2，当时只有 1 亿（100M）参数规模，后续在 2023 年 3 月又训练出了 RWKV-4 140 亿（14B）的参数版本。</p> <p>彭博曾告诉 AI 科技评论，为什么他要做一个跟 Transformer 架构不同的模型：</p> <p>“因为这个世界本身就不是基于 Transformer 的逻辑去做推理来运转的，这个世界的运转规律是基于类似 RNN 结构的——这个世界的下一秒，不会跟你过去所有的时间、所有的信息相关联，只会跟你的上一秒相关联。而 Transformer 要辨认所有的 token，这是不合理的。”</p> <p>所以 RWKV 用 linear attention（线性注意力机制） 去近似 full attention，试图结合 RNN 和 Transformer 的优点，同时规避两者的缺点，来缓解 Transformer 所带来的内存瓶颈和二次方扩展问题，实现更有效的线性扩展，同时提供并行训练和可扩展性，类似于 Transformer。简而言之，主打高性能、低能耗、占用内存小。</p> <p>而此前讨论较多的 Mamba，其论文作者有两位，一位是卡内基梅隆大学机器学习系助理教授 Albert Gu，另一位是 Together.AI 首席科学家的 Tri Dao。</p> <p>他们在论文中称，Mamba 是一个新的 SSM 架构，在语言建模方面，无论是预训练还是下游评估，他们的 Mamba-3B 模型都优于同等规模的 Transformer 模型，并能与两倍于其规模的 Transformer 模型相媲美，还可以随上下文长度的增加实现线性扩展，其性能在实际数据中可提高到百万 token 长度序列，并实现 5 倍的推理吞吐量提升。</p> <p>一位非 Transformer 研究者告诉 AI 科技评论，Mamba 完全只用 recurrent（循环）结构，不用 attention，所以它在做下一个 token 的预测时， <strong>其内存大小永远固定，并不会随时间增加而增加；但它的问题在于滚动的过程中 memory 非常小，即其外推能力也比较弱。</strong></p> <p>上述研究者认为，微软亚研提出的 RetNet，走的也是完全 recurrent 思路。RetNet 引入了一种多尺度 retention 机制来替代多头注意力，它有三种计算范式：并行、循环和分块循环表征。</p> <p>论文中称，RetNet 的推理成本与长度无关。对于 7B 模型和 8k 序列长度，RetNet 的解码速度是带键值缓存的 Transformers 的 8.4 倍，内存节省 70%。</p> <p>在训练过程中，RetNet 也能够比标准 Transformer 节省 25-50% 的内存，实现 7 倍的加速，并在高度优化的 FlashAttention 方面具有优势。此外，RetNet 的推理延迟对批大小不敏感，从而实现了巨大的吞吐量。</p> <p>Meta 提出的 Mega 则代表了非 Transformer 研究的第二种技术路线。Mega 的思路是把 recurrent 和变稀疏的 attention 矩阵结合起来。</p> <p>Mega 的核心研究人员之一 Max 告诉 AI 科技评论，attention 有它不可替代的作用，只要把它的复杂度限制在一定范围内，就能达到想要的效果。Mega 用了很长时间研究如何把 recurrent 和 attention 结合在一起才能最高效。</p> <p>所以 Mega 还是采用了 attention 结构，只不过把 attention 限制在了一个固定的 window（窗口）范围内，同时结合了类似 Mamba 的滚动记忆形式，只不过 Mega 的滚动形式要简化许多，所以整个计算速度很快。</p> <p>「滚动记忆」是指，所有的 efficient Transformer 都是把 recurrent 循环结构引入到 Transformer 中，类似于模型先看一段历史、记住，再看下一段历史，更新记忆，可能第一段历史记忆就没必要都记住了，忘掉一些，再把第二段需要记住的加到整个历史中，以此不断往前滚动着记忆。</p> <p>这样记忆的好处是模型可以有一个固定长度的滚动记忆，不会随着时间增加而让 memory 也要增加，但它的问题是很多时候，某些特殊任务在最后时刻都不知道前面记忆中有哪些是有用的、哪些是没用的，这种滚动式记忆就很难完成。</p> <p>Mega 在跟 llama 同样的数据上训练，再跟 llama2 去做公平的比较，发现在同样的数据情况下，Mega2 的效果比 llama2 要好很多。同时 Mega 预训练采用 32K 窗口大小，Transformer 用同样 32K 的窗口大小速度比 Mega2 慢很多，如果 window size 再变大，Mega 优势会越来越明显。目前 Mega2 已经训到了 7B 大小。</p> <p>DeepMind 团队提出的 Hawk 和 Griffin 同样认为没有 attention 是不行的，属于 gated linear RNN，跟 Mega 一样属于混合模型。</p> <p>除 RWKV，国内岩芯数智也发布了非 Attention 机制的通用自然语言大模型——Yan 模型。岩芯数智 CTO 刘凡平称，Yan 跟线性的 Attention 和 RNN 没有任何关系，Yan 架构的大模型去除了 Transformer 中高成本的注意力机制，代之以计算量更小、难度更低的线性计算，提高了建模效率和训练速度，实现了效率的提升和成本的降低。</p> <h2 id=transformer_3>Transformer 能否被颠覆？<a class=headerlink href=#transformer_3 title="Permanent link">¶</a></h2> <p>虽然当下非 Transformer 研究提出的并不少，从测评效果上来看，跟同等规模大小的 Transformer 相比，表现普遍超过 Transformer，但它们共同面临的考验和质疑是：当它们的规模被放大到今天 Transformer 模型的大小时，是否还能继续展示出强大的性能和效率提升？</p> <p>其中参数最大的 RWKV 有 140 亿参数，背靠 Meta 的 Mega 有 70 亿参数，而 GPT-3 有 1750 亿参数，GPT-4 传闻有 1.8 万亿参数，这意味着非 Transformer 急需训练出一个千亿模型来证明自己。</p> <p>非 Transformer 研究中最具代表性的 RWKV，已经走得非常靠前——其背后的元始智能目前已经完成了上千万元的种子轮融资；据了解国内已经有一些公司在尝试用 RWKV 来训练模型；过去的一年里，RWKV 在 To C、To B 也有局部落地。</p> <p>然而，有多位投资人告诉 AI 科技评论曾纠结是否要投 RWKV，赌一下非 Transformer 时，因为内部分歧太大——不敢坚信非 Transformer 能跑出来，最后都放弃了。</p> <p>现阶段来看，基于现有硬件的算力基础，用 Transformer 去做端侧大模型的难度很高，还是需要在云上完成计算推理等工作，而且应答速度不如人意，终端用户很难接受。</p> <p>有业内人士告诉 AI 科技评论，“在端侧，RWKV 并不一定是最优解，因为随着半导体发展，AI 芯片越来越进化，未来在硬件、算力、能源上的成本，最终都会被摊平，未来大模型可以轻松地直接跑在终端上，不再需要花费大力气从底层架构来做出改变。未来有一天会达到这样一个临界点的。”</p> <p>RWKV 的方式是从框架层操作，把框架轻量化了以后，可以让模型在本地运算。但也有一位投资人提出观点，认为非 Transformer 的理想状态是必须达到 OpenAI 的水平再来讲轻量化，“而不是为了小而小，为了本地化而本地化”。</p> <p>上述投资人评价 RWKV “麻雀虽小，五脏俱全”，总体体验感能达到 GPT-3.5 的 60 分，但并不知道最后能否达到 GPT 的 80 分、90 分。这也是非 Transformer 的问题所在，即如果舍弃了框架的复杂度、可能会牺牲上限的天花板。</p> <p>有接近 OpenAI 的人士告诉 AI 科技评论，OpenAI 内部其实曾经测试过 RWKV，但后面还是放弃了这一路线，因为“可能从长期来看它的天花板还未显现，实现 AGI 的可能性不大”。</p> <p>证明自己的天花板有多高，成为了所有非 Transformer 架构需要共同面临的考验。</p> <p>一些模型研究人员称，Transformer 做文本大模型还没有达到它的天花板，毕竟 scaling law 还没有失效，Transformer 的瓶颈或许还是在生成序列长度更长的领域，例如在视频生成的多模态领域，而多模态是未来实现 AGI 的必经之路，如此看来，上下文窗口依然是 Transformer 的一个瓶颈。</p> <p>如果像 OpenAI 一样不怕花钱，可以继续推高 Transformer 的 scaling law，但问题在于序列每长两倍就要花四倍的钱，花的时间也是四倍，平方级别的增长使 Transformer 在长序列问题的处理上效率太低，而且资源有上限。</p> <p>据了解，国内前列的大模型公司，基本用的都是 Transformer。但也有猜测称，GPT-5 是否还是沿用 Transformer 架构是个未知，理由是从 GPT-2 之后没再继续开源。但大家更愿意相信 Transformer 的天花板还远。所以国内想要继续追赶 GPT-4、GPT-5，走 Transformer 这条路也未必是错的。大模型时代，大家都在赌。</p> <p>但实现 AGI，Transformer 是否是唯一的路径，也未可知。目前能够确定的，是 Transformer 形成的垄断很难被打破，无论是从资源还是生态，当下的非 Transformer 研究都比不过。</p> <p>据了解，目前研究大模型非 Transformer 新架构的团队，要么在学术界，要么是如 RWKV 这样的创业团队，很少有大公司投入一个大的团队来研究新架构，所以在资源上，跟 Transformer 相比，非 Transformer 研究的差距还很大。</p> <p>此外，挡在前面最大的阻碍是 Transformer 日益坚固的生态护城河。</p> <p>现在，无论是硬件、系统、应用，都是围绕 Transformer 做适配、优化，使得开发其他架构的性价比降低，导致想要开发新的架构越来越难。</p> <p>在测评这块，许多测评设计任务的方式，都在偏向 Transformer 架构，意思是它设计的任务可能只有 Transformer 的模型能做出来，非 Transformer 做不出来、或者难度加大。这种设计能够展示 Transformer 的优势，但对其他架构并不友好。</p> <p>MIT 的博士生、flash-linear-attention 项目负责人杨松霖就曾告诉 AI 科技评论，当下非 Transformer 研究面临的阻碍之一是评估方式——单纯看 Perplexity（困惑度），非 transformer 其实跟 Transformer 的模型相比没有差距，但很多实际能力 （如 in-context copy and retrieval）依然差距很大。她认为当前的非 Transformer 模型缺乏更全面的评估方式，方能改进与 Transformer 之间能力的差距。</p> <p>毫无疑问，当下 Transformer 的地位依然无可撼动，依然是当下最强大的 AI 架构，然而，在回音室效应之外，开发下一代人工智能架构的工作正如火如荼进行着。</p> <p>打破垄断固然不易，但根据科技发展的规律，很难有一个架构能永远一统江湖。未来，非 Transformer 需要继续证明自己的天花板有多高，Transformer 架构也同样如此。</p> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button class="md-top md-icon" data-md-component=top hidden type=button> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg> 回到页面顶部 </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright © 2016 - 2024 d.run </div> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "..", "features": ["content.code.annotate", "content.code.copy", "content.tooltips", "navigation.tabs", "navigation.prune", "navigation.sections", "navigation.tabs.sticky", "navigation.tracking", "navigation.top", "search.highlight", "search.suggest", "search.share", "toc.follow", "navigation.path"], "search": "../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script> <script src=../assets/javascripts/bundle.60a45f97.min.js></script> <script src=../static/stylesheets/zoom_image.js></script> <script src=../overrides/assets/javascripts/bundle.b97a6647.min.js></script> <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?6b117ea770a78bebf27e63b402da0c4e";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script> <script type=module>
      import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10.2.0/+esm'
    </script> <script src="https://console.d.run/drun-copilot/chatbot-sdk.umd.js?ws=302&token=N2ZlNjFkZDItNDkyMy00Y2I1LWJlM2QtZDJlMzQ2YWM5OTE5"></script> <script>document$.subscribe(() => {
            window.update_swagger_ui_iframe_height = function (id) {
                var iFrameID = document.getElementById(id);
                if (iFrameID) {
                    full_height = (iFrameID.contentWindow.document.body.scrollHeight + 80) + "px";
                    iFrameID.height = full_height;
                    iFrameID.style.height = full_height;
                }
            }
        
            let iframe_id_list = []
            var iframes = document.getElementsByClassName("swagger-ui-iframe");
            for (var i = 0; i < iframes.length; i++) { 
                iframe_id_list.push(iframes[i].getAttribute("id"))
            }
        
            let ticking = true;
            
            document.addEventListener('scroll', function(e) {
                if (!ticking) {
                    window.requestAnimationFrame(()=> {
                        let half_vh = window.innerHeight/2;
                        for(var i = 0; i < iframe_id_list.length; i++) {
                            let element = document.getElementById(iframe_id_list[i])
                            if(element==null){
                                return
                            }
                            let diff = element.getBoundingClientRect().top
                            if(element.contentWindow.update_top_val){
                                element.contentWindow.update_top_val(half_vh - diff)
                            }
                        }
                        ticking = false;
                    });
                    ticking = true;
                }
            });
        
            const dark_scheme_name = "slate"
            
            window.scheme = document.body.getAttribute("data-md-color-scheme")
            const options = {
                attributeFilter: ['data-md-color-scheme'],
            };
            function color_scheme_callback(mutations) {
                for (let mutation of mutations) {
                    if (mutation.attributeName === "data-md-color-scheme") {
                        scheme = document.body.getAttribute("data-md-color-scheme")
                        var iframe_list = document.getElementsByClassName("swagger-ui-iframe")
                        for(var i = 0; i < iframe_list.length; i++) {
                            var ele = iframe_list.item(i);
                            if (ele) {
                                if (scheme === dark_scheme_name) {
                                    ele.contentWindow.enable_dark_mode();
                                } else {
                                    ele.contentWindow.disable_dark_mode();
                                }
                            }
                        }
                    }
                }
            }
            observer = new MutationObserver(color_scheme_callback);
            observer.observe(document.body, options);
            })</script></body> </html>