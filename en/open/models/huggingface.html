<!DOCTYPE html><html class=no-js lang=en> <head><meta charset=utf-8><meta content="width=device-width,initial-scale=1" name=viewport><meta content="d.run (DaoCloud Runs Intelligence)ï¼Œæ­ç¤ºä¸€ä¸ªæ–°ä¸€ä»£è½¯ä»¶ä½“ç³»ä¸‹çš„å…¨æ–°ç®—åŠ›ä¸–ç•Œï¼Œè®©ç®—åŠ›æ›´è‡ªç”±ã€‚" name=description><meta content=d.run name=author><link href=https://docs.d.run/en/open/models/huggingface.html rel=canonical><link href=openai.html rel=prev><link href=moonshot.html rel=next><link href=../../../images/favicon.ico rel=icon><meta content="mkdocs-1.6.1, mkdocs-material-9.5.50" name=generator><title>Hugging Face - d.run è®©ç®—åŠ›æ›´è‡ªç”±</title><link href=../../../assets/stylesheets/main.a40c8224.min.css rel=stylesheet><link href=../../../assets/stylesheets/palette.06af60db.min.css rel=stylesheet><link href=../../../overrides/assets/stylesheets/main.e13ced4c.min.css rel=stylesheet><link crossorigin href=https://fonts.gstatic.com rel=preconnect><link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback" rel=stylesheet><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body data-md-color-accent=indigo data-md-color-primary=indigo data-md-color-scheme=default dir=ltr> <input autocomplete=off class=md-toggle data-md-toggle=drawer id=__drawer type=checkbox> <input autocomplete=off class=md-toggle data-md-toggle=search id=__search type=checkbox> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a class=md-skip href=#hugging-face> Skip to content </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> </div> </aside> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav aria-label=Header class="md-header__inner md-grid"> <a aria-label="d.run è®©ç®—åŠ›æ›´è‡ªç”±" class="md-header__button md-logo" data-md-component=logo href=/ title="d.run è®©ç®—åŠ›æ›´è‡ªç”±"> <img alt=logo src=../../../images/DaoCloud.png> </a> <label class="md-header__button md-icon" for=__drawer> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> d.run è®©ç®—åŠ›æ›´è‡ªç”± </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Hugging Face </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input aria-label="Switch to light mode" class=md-option data-md-color-accent=indigo data-md-color-media=(prefers-color-scheme) data-md-color-primary=indigo data-md-color-scheme=default id=__palette_0 name=__palette type=radio> <label class="md-header__button md-icon" for=__palette_1 hidden title="Switch to light mode"> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"></path></svg> </label> <input aria-label="Switch to dark mode" class=md-option data-md-color-accent=indigo data-md-color-media="(prefers-color-scheme: light)" data-md-color-primary=indigo data-md-color-scheme=default id=__palette_1 name=__palette type=radio> <label class="md-header__button md-icon" for=__palette_2 hidden title="Switch to dark mode"> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg> </label> <input aria-label="Switch to system preference" class=md-option data-md-color-accent=indigo data-md-color-media="(prefers-color-scheme: dark)" data-md-color-primary=indigo data-md-color-scheme=slate id=__palette_2 name=__palette type=radio> <label class="md-header__button md-icon" for=__palette_0 hidden title="Switch to system preference"> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <div class=md-header__option> <div class=md-select> <button aria-label="Select language" class="md-header__button md-icon"> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="m12.87 15.07-2.54-2.51.03-.03A17.5 17.5 0 0 0 14.07 6H17V4h-7V2H8v2H1v2h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2zm-2.62 7 1.62-4.33L19.12 17z"></path></svg> </button> <div class=md-select__inner> <ul class=md-select__list> <li class=md-select__item> <a class=md-select__link href=../../../open/models/huggingface.html hreflang=zh> ä¸­æ–‡ </a> </li> <li class=md-select__item> <a class=md-select__link href=huggingface.html hreflang=en> English </a> </li> </ul> </div> </div> </div> <label class="md-header__button md-icon" for=__search> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input aria-label=Search autocapitalize=off autocomplete=off autocorrect=off class=md-search__input data-md-component=search-query name=query placeholder=Search required spellcheck=false type=text> <label class="md-search__icon md-icon" for=__search> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg> </label> <nav aria-label=Search class=md-search__options> <a aria-label=Share class="md-search__icon md-icon" data-clipboard data-clipboard-text data-md-component=search-share href=javascript:void(0) tabindex=-1 title=Share> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"></path></svg> </a> <button aria-label=Clear class="md-search__icon md-icon" tabindex=-1 title=Clear type=reset> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix tabindex=0> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a class=md-source data-md-component=source href=https://github.com/d-run/drun-docs title="Go to repository"> <div class="md-source__icon md-icon"> <svg viewbox="0 0 496 512" xmlns=http://www.w3.org/2000/svg><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg> </div> <div class=md-source__repository> d-run/drun-docs </div> </a> </div> </nav> <nav aria-label=Tabs class=md-tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=https://www.daocloud.io/ class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a class=md-tabs__link href=../../index.html> d.run Documentation </a> </li> <li class=md-tabs__item> <a class=md-tabs__link href=../../blogs/index.html> Blogs </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a class=md-tabs__link href=../index.html> Knowledge from AI Industry </a> </li> <li class=md-tabs__item> <a class=md-tabs__link href=../../contact/index.html> Contact Us </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav aria-label=Navigation class="md-nav md-nav--primary md-nav--lifted" data-md-level=0> <label class=md-nav__title for=__drawer> <a aria-label="d.run è®©ç®—åŠ›æ›´è‡ªç”±" class="md-nav__button md-logo" data-md-component=logo href=/ title="d.run è®©ç®—åŠ›æ›´è‡ªç”±"> <img alt=logo src=../../../images/DaoCloud.png> </a> d.run è®©ç®—åŠ›æ›´è‡ªç”± </label> <div class=md-nav__source> <a class=md-source data-md-component=source href=https://github.com/d-run/drun-docs title="Go to repository"> <div class="md-source__icon md-icon"> <svg viewbox="0 0 496 512" xmlns=http://www.w3.org/2000/svg><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg> </div> <div class=md-source__repository> d-run/drun-docs </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=https://www.daocloud.io/ class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a class=md-nav__link href=../../index.html> <span class=md-ellipsis> d.run Documentation </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a class=md-nav__link href=../../blogs/index.html> <span class=md-ellipsis> Blogs </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input checked class="md-nav__toggle md-toggle" id=__nav_4 type=checkbox> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex> <span class=md-ellipsis> Knowledge from AI Industry </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=true aria-labelledby=__nav_4_label class=md-nav data-md-level=1> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> Knowledge from AI Industry </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a class=md-nav__link href=../index.html> <span class=md-ellipsis> Index </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input checked class="md-nav__toggle md-toggle" id=__nav_4_2 type=checkbox> <label class=md-nav__link for=__nav_4_2 id=__nav_4_2_label tabindex> <span class=md-ellipsis> LLM </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=true aria-labelledby=__nav_4_2_label class=md-nav data-md-level=2> <label class=md-nav__title for=__nav_4_2> <span class="md-nav__icon md-icon"></span> LLM </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a class=md-nav__link href=openai.html> <span class=md-ellipsis> ChatGPT </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" id=__toc type=checkbox> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> Hugging Face </span> <span class="md-nav__icon md-icon"></span> </label> <a class="md-nav__link md-nav__link--active" href=huggingface.html> <span class=md-ellipsis> Hugging Face </span> </a> <nav aria-label=å¯¼èˆª class="md-nav md-nav--secondary"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> å¯¼èˆª </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a class=md-nav__link href=#online-demos> <span class=md-ellipsis> Online Demos </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#customized-support-services-offered-by-hugging-face> <span class=md-ellipsis> Customized Support Services Offered by Hugging Face </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#quick-start> <span class=md-ellipsis> Quick Start </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#why-use-transformers> <span class=md-ellipsis> Why Use Transformers? </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#when-not-to-use-transformers> <span class=md-ellipsis> When Not to Use Transformers? </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#installation> <span class=md-ellipsis> Installation </span> </a> <nav aria-label=Installation class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#using-pip> <span class=md-ellipsis> Using pip </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#using-conda> <span class=md-ellipsis> Using conda </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#model-architectures> <span class=md-ellipsis> Model Architectures </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=moonshot.html> <span class=md-ellipsis> Moonshot </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=claude.html> <span class=md-ellipsis> Claude 3 </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=china.html> <span class=md-ellipsis> å›½å†…å¤§æ¨¡åž‹æ±‡æ€» </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id=__nav_4_3 type=checkbox> <label class=md-nav__link for=__nav_4_3 id=__nav_4_3_label tabindex> <span class=md-ellipsis> Hardware Manufacturers </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=false aria-labelledby=__nav_4_3_label class=md-nav data-md-level=2> <label class=md-nav__title for=__nav_4_3> <span class="md-nav__icon md-icon"></span> Hardware Manufacturers </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a class=md-nav__link href=../hardware/nvidia.html> <span class=md-ellipsis> Nvidia </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=../hardware/huawei.html> <span class=md-ellipsis> Huawei </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=../hardware/illuvatar.html> <span class=md-ellipsis> Iluvatar </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id=__nav_4_4 type=checkbox> <label class=md-nav__link for=__nav_4_4 id=__nav_4_4_label tabindex> <span class=md-ellipsis> Domestic AI Pioneers </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=false aria-labelledby=__nav_4_4_label class=md-nav data-md-level=2> <label class=md-nav__title for=__nav_4_4> <span class="md-nav__icon md-icon"></span> Domestic AI Pioneers </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a class=md-nav__link href=../pioneers/alibaba.html> <span class=md-ellipsis> Alibaba </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=../pioneers/baichuan.html> <span class=md-ellipsis> Baichuan Intelligence </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=../pioneers/baidu.html> <span class=md-ellipsis> Baidu </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=../pioneers/hw99.html> <span class=md-ellipsis> Hanvon Technology </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=../pioneers/iflytek.html> <span class=md-ellipsis> iFlytek </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=../pioneers/megvii.html> <span class=md-ellipsis> Megvii Technology </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=../pioneers/sensetime.html> <span class=md-ellipsis> SenseTime </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=../pioneers/aispeech.html> <span class=md-ellipsis> iFlytek </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=../pioneers/tencent.html> <span class=md-ellipsis> Tencent </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=../pioneers/tusimple.html> <span class=md-ellipsis> TuSimple </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=../pioneers/cloudwalk.html> <span class=md-ellipsis> CloudWalk Technology </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=../pioneers/unisound.html> <span class=md-ellipsis> UniSound </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=../pioneers/bytedance.html> <span class=md-ellipsis> ByteDance </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=../../contact/index.html> <span class=md-ellipsis> Contact Us </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav aria-label=å¯¼èˆª class="md-nav md-nav--secondary"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> å¯¼èˆª </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a class=md-nav__link href=#online-demos> <span class=md-ellipsis> Online Demos </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#customized-support-services-offered-by-hugging-face> <span class=md-ellipsis> Customized Support Services Offered by Hugging Face </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#quick-start> <span class=md-ellipsis> Quick Start </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#why-use-transformers> <span class=md-ellipsis> Why Use Transformers? </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#when-not-to-use-transformers> <span class=md-ellipsis> When Not to Use Transformers? </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#installation> <span class=md-ellipsis> Installation </span> </a> <nav aria-label=Installation class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#using-pip> <span class=md-ellipsis> Using pip </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#using-conda> <span class=md-ellipsis> Using conda </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#model-architectures> <span class=md-ellipsis> Model Architectures </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a class="md-content__button md-icon" href=https://github.com/d-run/drun-docs/edit/main/docs/zh/docs/en/open/models/huggingface.md title=æŸ¥çœ‹ç¼–è¾‘åŽ†å²> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M13.5 8H12v5l4.28 2.54.72-1.21-3.5-2.08zM13 3a9 9 0 0 0-9 9H1l3.96 4.03L9 12H6a7 7 0 0 1 7-7 7 7 0 0 1 7 7 7 7 0 0 1-7 7c-1.93 0-3.68-.79-4.94-2.06l-1.42 1.42A8.9 8.9 0 0 0 13 21a9 9 0 0 0 9-9 9 9 0 0 0-9-9"></path></svg> </a> <a class="md-content__button md-icon" href=https://github.com/d-run/drun-docs/edit/main/docs/zh/docs/en/open/models/huggingface.md title=edit.link.title> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"></path></svg> </a> <a class="md-content__button md-icon" href=https://github.com/d-run/drun-docs/edit/main/docs/zh/docs/en/open/models/huggingface.md title=æŸ¥çœ‹æºæ–‡ä»¶> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"></path></svg> </a> <h1 id=hugging-face>Hugging Face<a class=headerlink href=#hugging-face title="Permanent link">Â¶</a></h1> <div class="admonition info"> <p class=admonition-title>Info</p> <p><a href=https://huggingface.co/ >Hugging Face</a> is the hottest open-source AI community in the field of machine learning, with its <a href=https://github.com/huggingface/transformers>Transformers repository</a> reaching 124,000 stars.</p> </div> <h3 align=center> <p>Advanced natural language processing built for Jax, PyTorch, and TensorFlow</p> </h3> <h3 align=center> <a href=https://hf.co/course><img src=https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/course_banner.png></a> </h3> <p>ðŸ¤— Transformers provides thousands of pre-trained models that support text classification, information extraction, question answering, summarization, translation, and text generation in over 100 languages. Its mission is to make cutting-edge NLP technology accessible to everyone.</p> <p>ðŸ¤— Transformers offers an API for quick downloading and usage, allowing you to apply pre-trained models to given text, fine-tune on your dataset, and then share with the community through the <a href=https://huggingface.co/models>model hub</a>. Additionally, each defined Python module is completely independent, making it easy to modify and quickly research experiments.</p> <p>ðŸ¤— Transformers supports the three most popular deep learning libraries: <a href=https://jax.readthedocs.io/en/latest/ >Jax</a>, <a href=https://pytorch.org/ >PyTorch</a>, and <a href=https://www.tensorflow.org/ >TensorFlow</a> â€” seamlessly integrating with them. You can train your model using one framework and then load and infer with another.</p> <h2 id=online-demos>Online Demos<a class=headerlink href=#online-demos title="Permanent link">Â¶</a></h2> <p>You can directly test most models on the <a href=https://huggingface.co/models>model hub</a> model pages. Hugging Face also offers <a href=https://huggingface.co/pricing>private model hosting, model version management, and inference APIs</a>.</p> <p>Here are some examples:</p> <ul> <li><a href="https://huggingface.co/google-bert/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France">Fill in the blanks with BERT</a></li> <li><a href="https://huggingface.co/dbmdz/electra-large-discriminator-finetuned-conll03-english?text=My+name+is+Sarah+and+I+live+in+London+city">Named entity recognition with Electra</a></li> <li><a href="https://huggingface.co/openai-community/gpt2?text=A+long+time+ago%2C+">Text generation with GPT-2</a></li> <li><a href="https://huggingface.co/FacebookAI/roberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+any+animal">Natural language inference with RoBERTa</a></li> <li><a href="https://huggingface.co/facebook/bart-large-cnn?text=The+tower+is+324+metres+%281%2C063+ft%29+tall%2C+about+the+same+height+as+an+81-storey+building%2C+and+the+tallest+structure+in+Paris.+Its+base+is+square%2C+measuring+125+metres+%28410+ft%29+on+each+side.+During+its+construction%2C+the+Eiffel+Tower+surpassed+the+Washington+Monument+to+become+the+tallest+man-made+structure+in+the+world%2C+a+title+it+held+for+41+years+until+the+Chrysler+Building+in+New+York+City+was+finished+in+1930.+It+was+the+first+structure+to+reach+a+height+of+300+metres.+Due+to+the+addition+of+a+broadcasting+aerial+at+the+top+of+the+tower+in+1957%2C+it+is+now+taller+than+the+Chrysler+Building+by+5.2+metres+%2817+ft%29.+Excluding+transmitters%2C+the+Eiffel+Tower+is+the+second+tallest+free-standing+structure+in+France+after+the+Millau+Viaduct">Text summarization with BART</a></li> <li><a href="https://huggingface.co/distilbert/distilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+divided+into+16%2C000+species">Question answering with DistilBERT</a></li> <li><a href="https://huggingface.co/google-t5/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin">Translation with T5</a></li> </ul> <p><strong><a href=https://transformer.huggingface.co>Write With Transformer</a></strong>, developed by the Hugging Face team, is the official demo for text generation.</p> <h2 id=customized-support-services-offered-by-hugging-face>Customized Support Services Offered by Hugging Face<a class=headerlink href=#customized-support-services-offered-by-hugging-face title="Permanent link">Â¶</a></h2> <p><a href=https://huggingface.co/support target=_blank> <img alt="HuggingFace Expert Acceleration Program" src=https://huggingface.co/front/thumbnails/support.png style="max-width: 600px; border: 1px solid #eee; border-radius: 4px; box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.05);"> </a><br></p> <h2 id=quick-start>Quick Start<a class=headerlink href=#quick-start title="Permanent link">Â¶</a></h2> <p>Hugging Face provides a <code>pipeline</code> API for quickly using models. The pipeline aggregates pre-trained models and corresponding text preprocessing. Hereâ€™s a quick example of using the pipeline to classify sentiment:</p> <div class=highlight><pre><span></span><code><a href=#__codelineno-0-1 id=__codelineno-0-1 name=__codelineno-0-1></a><span class=o>&gt;&gt;&gt;</span> <span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>pipeline</span>
<a href=#__codelineno-0-2 id=__codelineno-0-2 name=__codelineno-0-2></a>
<a href=#__codelineno-0-3 id=__codelineno-0-3 name=__codelineno-0-3></a><span class=c1># Using the sentiment analysis pipeline</span>
<a href=#__codelineno-0-4 id=__codelineno-0-4 name=__codelineno-0-4></a><span class=o>&gt;&gt;&gt;</span> <span class=n>classifier</span> <span class=o>=</span> <span class=n>pipeline</span><span class=p>(</span><span class=s1>'sentiment-analysis'</span><span class=p>)</span>
<a href=#__codelineno-0-5 id=__codelineno-0-5 name=__codelineno-0-5></a><span class=o>&gt;&gt;&gt;</span> <span class=n>classifier</span><span class=p>(</span><span class=s1>'We are very happy to introduce pipeline to the transformers repository.'</span><span class=p>)</span>
<a href=#__codelineno-0-6 id=__codelineno-0-6 name=__codelineno-0-6></a><span class=p>[{</span><span class=s1>'label'</span><span class=p>:</span> <span class=s1>'POSITIVE'</span><span class=p>,</span> <span class=s1>'score'</span><span class=p>:</span> <span class=mf>0.9996980428695679</span><span class=p>}]</span>
</code></pre></div> <p>The second line of code downloads and caches the pre-trained model used by the pipeline, while the third line evaluates the given text. The answer "positive" has a confidence of 99%.</p> <p>Many NLP tasks have out-of-the-box pre-trained pipelines. For example, Hugging Face can easily extract question answers from given text:</p> <div class=highlight><pre><span></span><code><a href=#__codelineno-1-1 id=__codelineno-1-1 name=__codelineno-1-1></a><span class=o>&gt;&gt;&gt;</span> <span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>pipeline</span>
<a href=#__codelineno-1-2 id=__codelineno-1-2 name=__codelineno-1-2></a>
<a href=#__codelineno-1-3 id=__codelineno-1-3 name=__codelineno-1-3></a><span class=c1># Using the question answering pipeline</span>
<a href=#__codelineno-1-4 id=__codelineno-1-4 name=__codelineno-1-4></a><span class=o>&gt;&gt;&gt;</span> <span class=n>question_answerer</span> <span class=o>=</span> <span class=n>pipeline</span><span class=p>(</span><span class=s1>'question-answering'</span><span class=p>)</span>
<a href=#__codelineno-1-5 id=__codelineno-1-5 name=__codelineno-1-5></a><span class=o>&gt;&gt;&gt;</span> <span class=n>question_answerer</span><span class=p>({</span>
<a href=#__codelineno-1-6 id=__codelineno-1-6 name=__codelineno-1-6></a><span class=o>...</span>     <span class=s1>'question'</span><span class=p>:</span> <span class=s1>'What is the name of the repository?'</span><span class=p>,</span>
<a href=#__codelineno-1-7 id=__codelineno-1-7 name=__codelineno-1-7></a><span class=o>...</span>     <span class=s1>'context'</span><span class=p>:</span> <span class=s1>'Pipeline has been included in the huggingface/transformers repository'</span>
<a href=#__codelineno-1-8 id=__codelineno-1-8 name=__codelineno-1-8></a><span class=o>...</span> <span class=p>})</span>
<a href=#__codelineno-1-9 id=__codelineno-1-9 name=__codelineno-1-9></a><span class=p>{</span><span class=s1>'score'</span><span class=p>:</span> <span class=mf>0.30970096588134766</span><span class=p>,</span> <span class=s1>'start'</span><span class=p>:</span> <span class=mi>34</span><span class=p>,</span> <span class=s1>'end'</span><span class=p>:</span> <span class=mi>58</span><span class=p>,</span> <span class=s1>'answer'</span><span class=p>:</span> <span class=s1>'huggingface/transformers'</span><span class=p>}</span>
</code></pre></div> <p>In addition to providing the answer, the pre-trained model also gives the corresponding confidence score, as well as the start and end positions of the answer in the tokenized text. You can learn more about the tasks supported by the pipeline API from <a href=https://huggingface.co/docs/transformers/task_summary>this tutorial</a>.</p> <p>It is also simple to download and use any pre-trained model for your task with just three lines of code. Hereâ€™s an example in PyTorch: <div class=highlight><pre><span></span><code><a href=#__codelineno-2-1 id=__codelineno-2-1 name=__codelineno-2-1></a><span class=o>&gt;&gt;&gt;</span> <span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>AutoTokenizer</span><span class=p>,</span> <span class=n>AutoModel</span>
<a href=#__codelineno-2-2 id=__codelineno-2-2 name=__codelineno-2-2></a>
<a href=#__codelineno-2-3 id=__codelineno-2-3 name=__codelineno-2-3></a><span class=o>&gt;&gt;&gt;</span> <span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>"google-bert/bert-base-uncased"</span><span class=p>)</span>
<a href=#__codelineno-2-4 id=__codelineno-2-4 name=__codelineno-2-4></a><span class=o>&gt;&gt;&gt;</span> <span class=n>model</span> <span class=o>=</span> <span class=n>AutoModel</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>"google-bert/bert-base-uncased"</span><span class=p>)</span>
<a href=#__codelineno-2-5 id=__codelineno-2-5 name=__codelineno-2-5></a>
<a href=#__codelineno-2-6 id=__codelineno-2-6 name=__codelineno-2-6></a><span class=o>&gt;&gt;&gt;</span> <span class=n>inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=s2>"Hello world!"</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>"pt"</span><span class=p>)</span>
<a href=#__codelineno-2-7 id=__codelineno-2-7 name=__codelineno-2-7></a><span class=o>&gt;&gt;&gt;</span> <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=o>**</span><span class=n>inputs</span><span class=p>)</span>
</code></pre></div></p> <p>Hereâ€™s the equivalent TensorFlow code:</p> <div class=highlight><pre><span></span><code><a href=#__codelineno-3-1 id=__codelineno-3-1 name=__codelineno-3-1></a><span class=o>&gt;&gt;&gt;</span> <span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>AutoTokenizer</span><span class=p>,</span> <span class=n>TFAutoModel</span>
<a href=#__codelineno-3-2 id=__codelineno-3-2 name=__codelineno-3-2></a>
<a href=#__codelineno-3-3 id=__codelineno-3-3 name=__codelineno-3-3></a><span class=o>&gt;&gt;&gt;</span> <span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>"google-bert/bert-base-uncased"</span><span class=p>)</span>
<a href=#__codelineno-3-4 id=__codelineno-3-4 name=__codelineno-3-4></a><span class=o>&gt;&gt;&gt;</span> <span class=n>model</span> <span class=o>=</span> <span class=n>TFAutoModel</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>"google-bert/bert-base-uncased"</span><span class=p>)</span>
<a href=#__codelineno-3-5 id=__codelineno-3-5 name=__codelineno-3-5></a>
<a href=#__codelineno-3-6 id=__codelineno-3-6 name=__codelineno-3-6></a><span class=o>&gt;&gt;&gt;</span> <span class=n>inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=s2>"Hello world!"</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>"tf"</span><span class=p>)</span>
<a href=#__codelineno-3-7 id=__codelineno-3-7 name=__codelineno-3-7></a><span class=o>&gt;&gt;&gt;</span> <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=o>**</span><span class=n>inputs</span><span class=p>)</span>
</code></pre></div> <p>The tokenizer provides preprocessing for all pre-trained models and can be called directly on a single string (like in the above example) or on a list. It outputs a dictionary that can be used in downstream code or directly unpacked using the <code>**</code> expression to pass to the model.</p> <p>The model itself is a standard <a href=https://pytorch.org/docs/stable/nn.html#torch.nn.Module>Pytorch <code>nn.Module</code></a> or <a href=https://www.tensorflow.org/api_docs/python/tf/keras/Model>TensorFlow <code>tf.keras.Model</code></a> (depending on your backend), and can be used in a conventional manner. <a href=https://huggingface.co/transformers/training.html>This tutorial</a> explains how to integrate such models into classic PyTorch or TensorFlow training loops, or how to use Hugging Face's <code>Trainer</code> API to quickly fine-tune on a new dataset.</p> <h2 id=why-use-transformers>Why Use Transformers?<a class=headerlink href=#why-use-transformers title="Permanent link">Â¶</a></h2> <ol> <li> <p>User-friendly advanced models:</p> <ul> <li>Excellent performance in NLU and NLG</li> <li>Educational and practical, with low barriers to entry</li> <li>High-level abstractions, requiring knowledge of only three classes</li> <li>Unified API for all models</li> </ul> </li> <li> <p>Lower computational overhead and reduced carbon emissions:</p> <ul> <li>Researchers can share trained models instead of retraining from scratch each time</li> <li>Engineers can reduce computation time and production costs</li> <li>Dozens of model architectures, over 2,000 pre-trained models, and support for more than 100 languages</li> </ul> </li> <li> <p>Comprehensive support for every part of the model lifecycle:</p> <ul> <li>Training advanced models requires just 3 lines of code</li> <li>Models can be easily transferred between different deep learning frameworks</li> <li>Choose the most suitable framework for training, evaluation, and production, with seamless integration</li> </ul> </li> <li> <p>Easily customize exclusive models and use cases for your needs:</p> <ul> <li>Multiple use cases provided for each model architecture to reproduce original paper results</li> <li>Internal structure of models remains transparent and consistent</li> <li>Model files can be used independently for easy modifications and quick experiments</li> </ul> </li> </ol> <h2 id=when-not-to-use-transformers>When Not to Use Transformers?<a class=headerlink href=#when-not-to-use-transformers title="Permanent link">Â¶</a></h2> <ul> <li>This library is not a modular neural network toolbox. The code in the model files is intentionally presented in a raw form without additional abstraction, allowing researchers to quickly iterate and modify without getting lost in abstractions and file navigation.</li> <li>The <code>Trainer</code> API is not compatible with any model; it is optimized for models in this library. If you are looking for a training loop implementation suitable for general machine learning, please look for another library.</li> <li>Although Hugging Face has made significant efforts, the scripts in the <a href=https://github.com/huggingface/transformers/tree/main/examples>examples directory</a> are just examples. They may not be plug-and-play for your specific problem and might require some modifications.</li> </ul> <h2 id=installation>Installation<a class=headerlink href=#installation title="Permanent link">Â¶</a></h2> <h3 id=using-pip>Using pip<a class=headerlink href=#using-pip title="Permanent link">Â¶</a></h3> <p>This repository has been tested with Python 3.8+, Flax 0.4.1+, PyTorch 1.11+, and TensorFlow 2.6+.</p> <p>You can install ðŸ¤— Transformers in a <a href=https://docs.python.org/3/library/venv.html>virtual environment</a>. If you are not familiar with Python's virtual environments, please refer to this <a href=https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/ >user guide</a>.</p> <p>First, create a virtual environment with the version of Python you plan to use and activate it.</p> <p>Then, you need to install either Flax, PyTorch, or TensorFlow. For instructions on installing these frameworks on your platform, see the <a href=https://www.tensorflow.org/install/ >TensorFlow installation page</a>, the <a href=https://pytorch.org/get-started/locally/#start-locally>PyTorch installation page</a>, or the <a href=https://github.com/google/flax#quick-install>Flax installation page</a>.</p> <p>Once one of these backends is successfully installed, you can install ðŸ¤— Transformers as follows:</p> <div class=highlight><pre><span></span><code><a href=#__codelineno-4-1 id=__codelineno-4-1 name=__codelineno-4-1></a>pip<span class=w> </span>install<span class=w> </span>transformers
</code></pre></div> <p>If you want to try out examples or use the latest development code before the official release, you need to <a href=https://huggingface.co/docs/transformers/installation#installing-from-source>install from source</a>.</p> <h3 id=using-conda>Using conda<a class=headerlink href=#using-conda title="Permanent link">Â¶</a></h3> <p>ðŸ¤— Transformers can be installed via conda as follows:</p> <p><code>shell script conda install conda-forge::transformers</code></p> <div class="admonition note"> <p class=admonition-title>Note</p> <p>Installing <code>transformers</code> from the <code>huggingface</code> channel has been deprecated.</p> </div> <p>For instructions on installing either Flax, PyTorch, or TensorFlow via conda, please refer to their respective installation pages.</p> <h2 id=model-architectures>Model Architectures<a class=headerlink href=#model-architectures title="Permanent link">Â¶</a></h2> <p>All model checkpoints supported by ðŸ¤— Transformers are uploaded by <a href=https://huggingface.co/users>users</a> and <a href=https://huggingface.co/organizations>Hugging Face organizations</a> and are seamlessly integrated with the huggingface.co <a href=https://huggingface.co>model hub</a>.</p> <p>Current number of models: <img alt src="https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen"></p> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button class="md-top md-icon" data-md-component=top hidden type=button> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg> Back to top </button> </main> <footer class=md-footer> <nav aria-label=Footer class="md-footer__inner md-grid"> <a aria-label="Previous: ChatGPT" class="md-footer__link md-footer__link--prev" href=openai.html> <div class="md-footer__button md-icon"> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> ChatGPT </div> </div> </a> <a aria-label="Next: Moonshot" class="md-footer__link md-footer__link--next" href=moonshot.html> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> Moonshot </div> </div> <div class="md-footer__button md-icon"> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"></path></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright Â© 2016 - 2024 d.run </div> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../../..", "features": ["content.code.annotate", "content.code.copy", "content.tooltips", "navigation.tabs", "navigation.prune", "navigation.sections", "navigation.tabs.sticky", "navigation.tracking", "navigation.top", "search.highlight", "search.suggest", "search.share", "toc.follow", "navigation.path", "navigation.footer"], "search": "../../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script> <script src=../../../assets/javascripts/bundle.60a45f97.min.js></script> <script src=../../../static/stylesheets/zoom_image.js></script> <script src=../../../overrides/assets/javascripts/bundle.b97a6647.min.js></script> <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?6b117ea770a78bebf27e63b402da0c4e";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script> <script type=module>
      import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10.2.0/+esm'
    </script> <script src="https://console.d.run/drun-copilot/chatbot-sdk.umd.js?ws=302&token=N2ZlNjFkZDItNDkyMy00Y2I1LWJlM2QtZDJlMzQ2YWM5OTE5"></script> <script>document$.subscribe(() => {
            window.update_swagger_ui_iframe_height = function (id) {
                var iFrameID = document.getElementById(id);
                if (iFrameID) {
                    full_height = (iFrameID.contentWindow.document.body.scrollHeight + 80) + "px";
                    iFrameID.height = full_height;
                    iFrameID.style.height = full_height;
                }
            }
        
            let iframe_id_list = []
            var iframes = document.getElementsByClassName("swagger-ui-iframe");
            for (var i = 0; i < iframes.length; i++) { 
                iframe_id_list.push(iframes[i].getAttribute("id"))
            }
        
            let ticking = true;
            
            document.addEventListener('scroll', function(e) {
                if (!ticking) {
                    window.requestAnimationFrame(()=> {
                        let half_vh = window.innerHeight/2;
                        for(var i = 0; i < iframe_id_list.length; i++) {
                            let element = document.getElementById(iframe_id_list[i])
                            if(element==null){
                                return
                            }
                            let diff = element.getBoundingClientRect().top
                            if(element.contentWindow.update_top_val){
                                element.contentWindow.update_top_val(half_vh - diff)
                            }
                        }
                        ticking = false;
                    });
                    ticking = true;
                }
            });
        
            const dark_scheme_name = "slate"
            
            window.scheme = document.body.getAttribute("data-md-color-scheme")
            const options = {
                attributeFilter: ['data-md-color-scheme'],
            };
            function color_scheme_callback(mutations) {
                for (let mutation of mutations) {
                    if (mutation.attributeName === "data-md-color-scheme") {
                        scheme = document.body.getAttribute("data-md-color-scheme")
                        var iframe_list = document.getElementsByClassName("swagger-ui-iframe")
                        for(var i = 0; i < iframe_list.length; i++) {
                            var ele = iframe_list.item(i);
                            if (ele) {
                                if (scheme === dark_scheme_name) {
                                    ele.contentWindow.enable_dark_mode();
                                } else {
                                    ele.contentWindow.disable_dark_mode();
                                }
                            }
                        }
                    }
                }
            }
            observer = new MutationObserver(color_scheme_callback);
            observer.observe(document.body, options);
            })</script></body> </html>