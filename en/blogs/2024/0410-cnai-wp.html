<!DOCTYPE html><html class=no-js lang=en> <head><meta charset=utf-8><meta content="width=device-width,initial-scale=1" name=viewport><meta content="d.run (DaoCloud Runs Intelligence)，揭示一个新一代软件体系下的全新算力世界，让算力更自由。" name=description><meta content=d.run name=author><link href=https://docs.d.run/en/blogs/2024/0410-cnai-wp.html rel=canonical><link href=0429-ai-survey.html rel=prev><link href=0408-after-kimi.html rel=next><link href=../../../images/favicon.ico rel=icon><meta content="mkdocs-1.6.1, mkdocs-material-9.5.50" name=generator><title>Cloud-native Artificial Intelligence White Paper - d.run 让算力更自由</title><link href=../../../assets/stylesheets/main.a40c8224.min.css rel=stylesheet><link href=../../../assets/stylesheets/palette.06af60db.min.css rel=stylesheet><link href=../../../overrides/assets/stylesheets/main.e13ced4c.min.css rel=stylesheet><link crossorigin href=https://fonts.gstatic.com rel=preconnect><link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback" rel=stylesheet><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body data-md-color-accent=indigo data-md-color-primary=indigo data-md-color-scheme=default dir=ltr> <input autocomplete=off class=md-toggle data-md-toggle=drawer id=__drawer type=checkbox> <input autocomplete=off class=md-toggle data-md-toggle=search id=__search type=checkbox> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a class=md-skip href=#cloud-native-artificial-intelligence> Skip to content </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> </div> </aside> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav aria-label=Header class="md-header__inner md-grid"> <a aria-label="d.run 让算力更自由" class="md-header__button md-logo" data-md-component=logo href=/ title="d.run 让算力更自由"> <img alt=logo src=../../../images/DaoCloud.png> </a> <label class="md-header__button md-icon" for=__drawer> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> d.run 让算力更自由 </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Cloud-native Artificial Intelligence White Paper </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input aria-label="Switch to light mode" class=md-option data-md-color-accent=indigo data-md-color-media=(prefers-color-scheme) data-md-color-primary=indigo data-md-color-scheme=default id=__palette_0 name=__palette type=radio> <label class="md-header__button md-icon" for=__palette_1 hidden title="Switch to light mode"> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"></path></svg> </label> <input aria-label="Switch to dark mode" class=md-option data-md-color-accent=indigo data-md-color-media="(prefers-color-scheme: light)" data-md-color-primary=indigo data-md-color-scheme=default id=__palette_1 name=__palette type=radio> <label class="md-header__button md-icon" for=__palette_2 hidden title="Switch to dark mode"> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg> </label> <input aria-label="Switch to system preference" class=md-option data-md-color-accent=indigo data-md-color-media="(prefers-color-scheme: dark)" data-md-color-primary=indigo data-md-color-scheme=slate id=__palette_2 name=__palette type=radio> <label class="md-header__button md-icon" for=__palette_0 hidden title="Switch to system preference"> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <div class=md-header__option> <div class=md-select> <button aria-label="Select language" class="md-header__button md-icon"> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="m12.87 15.07-2.54-2.51.03-.03A17.5 17.5 0 0 0 14.07 6H17V4h-7V2H8v2H1v2h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2zm-2.62 7 1.62-4.33L19.12 17z"></path></svg> </button> <div class=md-select__inner> <ul class=md-select__list> <li class=md-select__item> <a class=md-select__link href=../../../blogs/2024/0410-cnai-wp.html hreflang=zh> 中文 </a> </li> <li class=md-select__item> <a class=md-select__link href=0410-cnai-wp.html hreflang=en> English </a> </li> </ul> </div> </div> </div> <label class="md-header__button md-icon" for=__search> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input aria-label=Search autocapitalize=off autocomplete=off autocorrect=off class=md-search__input data-md-component=search-query name=query placeholder=Search required spellcheck=false type=text> <label class="md-search__icon md-icon" for=__search> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg> </label> <nav aria-label=Search class=md-search__options> <a aria-label=Share class="md-search__icon md-icon" data-clipboard data-clipboard-text data-md-component=search-share href=javascript:void(0) tabindex=-1 title=Share> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"></path></svg> </a> <button aria-label=Clear class="md-search__icon md-icon" tabindex=-1 title=Clear type=reset> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix tabindex=0> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a class=md-source data-md-component=source href=https://github.com/d-run/drun-docs title="Go to repository"> <div class="md-source__icon md-icon"> <svg viewbox="0 0 496 512" xmlns=http://www.w3.org/2000/svg><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg> </div> <div class=md-source__repository> d-run/drun-docs </div> </a> </div> </nav> <nav aria-label=Tabs class=md-tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=https://www.daocloud.io/ class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a class=md-tabs__link href=../../index.html> d.run Documentation </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a class=md-tabs__link href=../index.html> Blogs </a> </li> <li class=md-tabs__item> <a class=md-tabs__link href=../../open/index.html> Knowledge from AI Industry </a> </li> <li class=md-tabs__item> <a class=md-tabs__link href=../../contact/index.html> Contact Us </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav aria-label=Navigation class="md-nav md-nav--primary md-nav--lifted" data-md-level=0> <label class=md-nav__title for=__drawer> <a aria-label="d.run 让算力更自由" class="md-nav__button md-logo" data-md-component=logo href=/ title="d.run 让算力更自由"> <img alt=logo src=../../../images/DaoCloud.png> </a> d.run 让算力更自由 </label> <div class=md-nav__source> <a class=md-source data-md-component=source href=https://github.com/d-run/drun-docs title="Go to repository"> <div class="md-source__icon md-icon"> <svg viewbox="0 0 496 512" xmlns=http://www.w3.org/2000/svg><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg> </div> <div class=md-source__repository> d-run/drun-docs </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=https://www.daocloud.io/ class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a class=md-nav__link href=../../index.html> <span class=md-ellipsis> d.run Documentation </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input checked class="md-nav__toggle md-toggle" id=__nav_3 type=checkbox> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex> <span class=md-ellipsis> Blogs </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=true aria-labelledby=__nav_3_label class=md-nav data-md-level=1> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Blogs </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a class=md-nav__link href=../index.html> <span class=md-ellipsis> Index </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=../2025/0102-ai-trend.html> <span class=md-ellipsis> AI Trend in 2025 </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=d.run.html> <span class=md-ellipsis> d.run is the Ideal Platform for Generative AI </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=0702-k8s-for-genai.html> <span class=md-ellipsis> K8s and Generative AI Make a Perfect Match </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=0514-gpt4o.html> <span class=md-ellipsis> OpenAI GPT-4o is Completely Free </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=0509-model-spec.html> <span class=md-ellipsis> OpenAI LLM Specifications </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=0429-ai-survey.html> <span class=md-ellipsis> 2024 Large-scale AI Infrastructure Survey </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" id=__toc type=checkbox> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> Cloud-native Artificial Intelligence White Paper </span> <span class="md-nav__icon md-icon"></span> </label> <a class="md-nav__link md-nav__link--active" href=0410-cnai-wp.html> <span class=md-ellipsis> Cloud-native Artificial Intelligence White Paper </span> </a> <nav aria-label=导航 class="md-nav md-nav--secondary"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> 导航 </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a class=md-nav__link href=#introduction-to-cloud-native-artificial-intelligence-cnai> <span class=md-ellipsis> Introduction to Cloud Native Artificial Intelligence (CNAI) </span> </a> <nav aria-label="Introduction to Cloud Native Artificial Intelligence (CNAI)" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#the-emergence-of-cloud-native> <span class=md-ellipsis> The Emergence of Cloud Native </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#evolution-of-artificial-intelligence> <span class=md-ellipsis> Evolution of Artificial Intelligence </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#merging-of-cloud-native-and-artificial-intelligence> <span class=md-ellipsis> Merging of Cloud Native and Artificial Intelligence </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#what-is-cloud-native-artificial-intelligence> <span class=md-ellipsis> What is Cloud Native Artificial Intelligence? </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#why-cloud-native-artificial-intelligence> <span class=md-ellipsis> Why Cloud Native Artificial Intelligence? </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#using-ai-to-improve-cloud-native-systems> <span class=md-ellipsis> Using AI to Improve Cloud Native Systems </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#challenges-for-cloud-native-artificial-intelligence> <span class=md-ellipsis> Challenges For Cloud Native Artificial Intelligence </span> </a> <nav aria-label="Challenges For Cloud Native Artificial Intelligence" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#data-preparation> <span class=md-ellipsis> Data Preparation </span> </a> <nav aria-label="Data Preparation" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#data-size> <span class=md-ellipsis> Data Size </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#data-synchronization> <span class=md-ellipsis> Data Synchronization </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#data-governance> <span class=md-ellipsis> Data Governance </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#model-training> <span class=md-ellipsis> Model Training </span> </a> <nav aria-label="Model Training" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#rising-processing-demands> <span class=md-ellipsis> Rising Processing Demands </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#cost-efficiency> <span class=md-ellipsis> Cost Efficiency </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#scalability> <span class=md-ellipsis> Scalability </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#orchestrationscheduling> <span class=md-ellipsis> Orchestration/Scheduling </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#custom-dependencies> <span class=md-ellipsis> Custom Dependencies </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#model-serving> <span class=md-ellipsis> Model Serving </span> </a> <nav aria-label="Model Serving" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#microservice-architecture-and-developer-experience> <span class=md-ellipsis> Microservice Architecture and Developer Experience </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#model-placement> <span class=md-ellipsis> Model Placement </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#resource-allocation> <span class=md-ellipsis> Resource Allocation </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#user-experience> <span class=md-ellipsis> User Experience </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#cross-cutting-concerns> <span class=md-ellipsis> Cross-Cutting Concerns </span> </a> <nav aria-label="Cross-Cutting Concerns" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#reference-implementation> <span class=md-ellipsis> Reference Implementation </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#right-sizing-resource-provisioning> <span class=md-ellipsis> Right-sizing Resource Provisioning </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#cost-control> <span class=md-ellipsis> Cost Control </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#observability> <span class=md-ellipsis> Observability </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#disaster-recovery-and-business-continuity> <span class=md-ellipsis> Disaster Recovery and Business Continuity </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#security-and-compliance-audits> <span class=md-ellipsis> Security and Compliance Audits </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#sustainability> <span class=md-ellipsis> Sustainability </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#education-for-kids> <span class=md-ellipsis> Education for Kids </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#path-forward-with-cloud-native-artificial-intelligence> <span class=md-ellipsis> Path Forward with Cloud Native Artificial Intelligence </span> </a> <nav aria-label="Path Forward with Cloud Native Artificial Intelligence" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#recommendations> <span class=md-ellipsis> Recommendations </span> </a> <nav aria-label=Recommendations class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#flexibility> <span class=md-ellipsis> Flexibility </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#sustainability_1> <span class=md-ellipsis> Sustainability </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#custom-platform-dependencies> <span class=md-ellipsis> Custom Platform Dependencies </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#reference-implementation_1> <span class=md-ellipsis> Reference Implementation </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#industry-acceptance-of-terminology> <span class=md-ellipsis> Industry Acceptance of Terminology </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#evolving-solutions-for-aiml> <span class=md-ellipsis> Evolving Solutions for AI/ML </span> </a> <nav aria-label="Evolving Solutions for AI/ML" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#orchestration-kubeflow> <span class=md-ellipsis> Orchestration - Kubeflow </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#context-vector-databases> <span class=md-ellipsis> Context - Vector Databases </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#observability-openllmetry> <span class=md-ellipsis> Observability - OpenLLMetry </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#opportunities> <span class=md-ellipsis> Opportunities </span> </a> <nav aria-label=Opportunities class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#cncf-project-landscape> <span class=md-ellipsis> CNCF Project Landscape </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#cnai-for-kids-and-students> <span class=md-ellipsis> CNAI for Kids and Students </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#participation> <span class=md-ellipsis> Participation </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#trust-and-safety-safety-by-design> <span class=md-ellipsis> Trust and Safety / Safety By Design </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#the-emergence-of-a-new-engineering-discipline> <span class=md-ellipsis> The Emergence of a New Engineering Discipline </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#artificial-intelligence-for-cloud-native> <span class=md-ellipsis> Artificial Intelligence for Cloud Native </span> </a> <nav aria-label="Artificial Intelligence for Cloud Native" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#natural-language-interface-for-cluster-control> <span class=md-ellipsis> Natural Language Interface for Cluster Control </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#security> <span class=md-ellipsis> Security </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#smarter-orchestrationscheduling> <span class=md-ellipsis> Smarter Orchestration/Scheduling </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#ai-integration-efforts-in-flight-and-under-exploration> <span class=md-ellipsis> AI Integration Efforts in Flight and Under Exploration </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#conclusion> <span class=md-ellipsis> Conclusion </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#appendix> <span class=md-ellipsis> Appendix </span> </a> <nav aria-label=Appendix class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#glossary> <span class=md-ellipsis> Glossary </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#references> <span class=md-ellipsis> References </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=0408-after-kimi.html> <span class=md-ellipsis> Kimi Success and Other Domestic LLM </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=0407-dbrx.html> <span class=md-ellipsis> Introduction to DBRX Open Source LLM </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=0403-cp-to-profit.html> <span class=md-ellipsis> Transforms Compute into Profit </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=0327-transformer.html> <span class=md-ellipsis> Who Will Replace the Transformer </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=0326-compute-power.html> <span class=md-ellipsis> Financial Industry Welcomes the LLM Era </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a class=md-nav__link href=../../open/index.html> <span class=md-ellipsis> Knowledge from AI Industry </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=../../contact/index.html> <span class=md-ellipsis> Contact Us </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav aria-label=导航 class="md-nav md-nav--secondary"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> 导航 </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a class=md-nav__link href=#introduction-to-cloud-native-artificial-intelligence-cnai> <span class=md-ellipsis> Introduction to Cloud Native Artificial Intelligence (CNAI) </span> </a> <nav aria-label="Introduction to Cloud Native Artificial Intelligence (CNAI)" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#the-emergence-of-cloud-native> <span class=md-ellipsis> The Emergence of Cloud Native </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#evolution-of-artificial-intelligence> <span class=md-ellipsis> Evolution of Artificial Intelligence </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#merging-of-cloud-native-and-artificial-intelligence> <span class=md-ellipsis> Merging of Cloud Native and Artificial Intelligence </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#what-is-cloud-native-artificial-intelligence> <span class=md-ellipsis> What is Cloud Native Artificial Intelligence? </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#why-cloud-native-artificial-intelligence> <span class=md-ellipsis> Why Cloud Native Artificial Intelligence? </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#using-ai-to-improve-cloud-native-systems> <span class=md-ellipsis> Using AI to Improve Cloud Native Systems </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#challenges-for-cloud-native-artificial-intelligence> <span class=md-ellipsis> Challenges For Cloud Native Artificial Intelligence </span> </a> <nav aria-label="Challenges For Cloud Native Artificial Intelligence" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#data-preparation> <span class=md-ellipsis> Data Preparation </span> </a> <nav aria-label="Data Preparation" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#data-size> <span class=md-ellipsis> Data Size </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#data-synchronization> <span class=md-ellipsis> Data Synchronization </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#data-governance> <span class=md-ellipsis> Data Governance </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#model-training> <span class=md-ellipsis> Model Training </span> </a> <nav aria-label="Model Training" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#rising-processing-demands> <span class=md-ellipsis> Rising Processing Demands </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#cost-efficiency> <span class=md-ellipsis> Cost Efficiency </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#scalability> <span class=md-ellipsis> Scalability </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#orchestrationscheduling> <span class=md-ellipsis> Orchestration/Scheduling </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#custom-dependencies> <span class=md-ellipsis> Custom Dependencies </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#model-serving> <span class=md-ellipsis> Model Serving </span> </a> <nav aria-label="Model Serving" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#microservice-architecture-and-developer-experience> <span class=md-ellipsis> Microservice Architecture and Developer Experience </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#model-placement> <span class=md-ellipsis> Model Placement </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#resource-allocation> <span class=md-ellipsis> Resource Allocation </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#user-experience> <span class=md-ellipsis> User Experience </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#cross-cutting-concerns> <span class=md-ellipsis> Cross-Cutting Concerns </span> </a> <nav aria-label="Cross-Cutting Concerns" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#reference-implementation> <span class=md-ellipsis> Reference Implementation </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#right-sizing-resource-provisioning> <span class=md-ellipsis> Right-sizing Resource Provisioning </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#cost-control> <span class=md-ellipsis> Cost Control </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#observability> <span class=md-ellipsis> Observability </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#disaster-recovery-and-business-continuity> <span class=md-ellipsis> Disaster Recovery and Business Continuity </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#security-and-compliance-audits> <span class=md-ellipsis> Security and Compliance Audits </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#sustainability> <span class=md-ellipsis> Sustainability </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#education-for-kids> <span class=md-ellipsis> Education for Kids </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#path-forward-with-cloud-native-artificial-intelligence> <span class=md-ellipsis> Path Forward with Cloud Native Artificial Intelligence </span> </a> <nav aria-label="Path Forward with Cloud Native Artificial Intelligence" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#recommendations> <span class=md-ellipsis> Recommendations </span> </a> <nav aria-label=Recommendations class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#flexibility> <span class=md-ellipsis> Flexibility </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#sustainability_1> <span class=md-ellipsis> Sustainability </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#custom-platform-dependencies> <span class=md-ellipsis> Custom Platform Dependencies </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#reference-implementation_1> <span class=md-ellipsis> Reference Implementation </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#industry-acceptance-of-terminology> <span class=md-ellipsis> Industry Acceptance of Terminology </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#evolving-solutions-for-aiml> <span class=md-ellipsis> Evolving Solutions for AI/ML </span> </a> <nav aria-label="Evolving Solutions for AI/ML" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#orchestration-kubeflow> <span class=md-ellipsis> Orchestration - Kubeflow </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#context-vector-databases> <span class=md-ellipsis> Context - Vector Databases </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#observability-openllmetry> <span class=md-ellipsis> Observability - OpenLLMetry </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#opportunities> <span class=md-ellipsis> Opportunities </span> </a> <nav aria-label=Opportunities class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#cncf-project-landscape> <span class=md-ellipsis> CNCF Project Landscape </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#cnai-for-kids-and-students> <span class=md-ellipsis> CNAI for Kids and Students </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#participation> <span class=md-ellipsis> Participation </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#trust-and-safety-safety-by-design> <span class=md-ellipsis> Trust and Safety / Safety By Design </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#the-emergence-of-a-new-engineering-discipline> <span class=md-ellipsis> The Emergence of a New Engineering Discipline </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#artificial-intelligence-for-cloud-native> <span class=md-ellipsis> Artificial Intelligence for Cloud Native </span> </a> <nav aria-label="Artificial Intelligence for Cloud Native" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#natural-language-interface-for-cluster-control> <span class=md-ellipsis> Natural Language Interface for Cluster Control </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#security> <span class=md-ellipsis> Security </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#smarter-orchestrationscheduling> <span class=md-ellipsis> Smarter Orchestration/Scheduling </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#ai-integration-efforts-in-flight-and-under-exploration> <span class=md-ellipsis> AI Integration Efforts in Flight and Under Exploration </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#conclusion> <span class=md-ellipsis> Conclusion </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#appendix> <span class=md-ellipsis> Appendix </span> </a> <nav aria-label=Appendix class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#glossary> <span class=md-ellipsis> Glossary </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#references> <span class=md-ellipsis> References </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a class="md-content__button md-icon" href=https://github.com/d-run/drun-docs/edit/main/docs/zh/docs/en/blogs/2024/0410-cnai-wp.md title=查看编辑历史> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M13.5 8H12v5l4.28 2.54.72-1.21-3.5-2.08zM13 3a9 9 0 0 0-9 9H1l3.96 4.03L9 12H6a7 7 0 0 1 7-7 7 7 0 0 1 7 7 7 7 0 0 1-7 7c-1.93 0-3.68-.79-4.94-2.06l-1.42 1.42A8.9 8.9 0 0 0 13 21a9 9 0 0 0 9-9 9 9 0 0 0-9-9"></path></svg> </a> <a class="md-content__button md-icon" href=https://github.com/d-run/drun-docs/edit/main/docs/zh/docs/en/blogs/2024/0410-cnai-wp.md title=edit.link.title> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"></path></svg> </a> <a class="md-content__button md-icon" href=https://github.com/d-run/drun-docs/edit/main/docs/zh/docs/en/blogs/2024/0410-cnai-wp.md title=查看源文件> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"></path></svg> </a> <h1 id=cloud-native-artificial-intelligence>Cloud Native Artificial Intelligence<a class=headerlink href=#cloud-native-artificial-intelligence title="Permanent link">¶</a></h1> <p>Cloud Native (CN) and Artificial Intelligence (AI) are the most critical technology trends today. Cloud Native technology provides a scalable and reliable platform for running applications. Given recent advances in AI and Machine Learning (ML), it is steadily rising as a dominant cloud workload. While CN technologies readily support certain aspects of AI/ML workloads, challenges and gaps remain, presenting opportunities to innovate and better accommodate.</p> <p>This paper presents a brief overview of the state-of-the-art AI/ML techniques, followed by what CN technologies offer, covering the next challenges and gaps before discussing evolving solutions. The paper will equip engineers and business personnel with the knowledge to understand the changing Cloud Native Artificial Intelligence (CNAI) ecosystem and its opportunities.</p> <p>We suggest a reading path depending on the reader’s background and interest. Exposure to microservices and CN technologies such as Kubernetes (K8s) is assumed. For those without experience in engineering AI systems, we recommend reading from start to finish. For those further along in their AI/ML adoption or delivery journey, per their user persona we suggest diving into the sections pertinent to the challenges they are grappling with or are interested in solving. We also share where society needs to invest in this context.</p> <h2 id=introduction-to-cloud-native-artificial-intelligence-cnai>Introduction to Cloud Native Artificial Intelligence (CNAI)<a class=headerlink href=#introduction-to-cloud-native-artificial-intelligence-cnai title="Permanent link">¶</a></h2> <p>Before we get into CNAI, the coming together of Cloud Native and AI technologies, let us examine briefly the evolution of each.</p> <h3 id=the-emergence-of-cloud-native>The Emergence of Cloud Native<a class=headerlink href=#the-emergence-of-cloud-native title="Permanent link">¶</a></h3> <p>Widely known and used since 2013, the term Cloud Native (CN) saw an increase in popularity with the rise of container technology from LXC to Docker to Kubernetes (K8s). Today, Cloud Native is more broadly an aspirational target of well-balanced systems built using the microservice design pattern that promotes modular design and development with a high degree of re-usability, which also lends itself to deployability, scalability, and resilience.</p> <p>The Cloud Native Computing Foundation defines Cloud Native as: Cloud Native technologies empower organizations to build and run scalable applications in modern, dynamic environments such as public, private, and hybrid clouds. Containers, service meshes, microservices, immutable infrastructure, and declarative APIs exemplify this approach. These techniques enable loosely coupled systems that are resilient, manageable, and observable. Combined with robust automation, they allow engineers to make high-impact changes frequently and predictably with minimal toil.</p> <p>The Cloud Native Computing Foundation seeks to drive the adoption of this paradigm by fostering and sustaining an ecosystem of open source, vendor-neutral projects. We democratize state-of-the-art patterns to make these innovations accessible to everyone.</p> <p>Cloud Native Artificial Intelligence is an evolving extension of Cloud Native.</p> <h3 id=evolution-of-artificial-intelligence>Evolution of Artificial Intelligence<a class=headerlink href=#evolution-of-artificial-intelligence title="Permanent link">¶</a></h3> <p>Artificial Intelligence, first introduced as a term in 1956, is the ability of machines to simulate human intelligence. Over the decades, it has been used in applications such as speech recognition, machine translation, image processing, game playing, and even excelling as a Jeopardy player. But, AI has exploded in mindshare more recently thanks to innovations in artificial neural networks and deep learning, mainly applied to natural language understanding. There are two primary classifications of AI: discriminative and generative.</p> <p>Discriminative AI seeks to learn decision boundaries or classifications, with the knowledge captured as a “model,” which is used to predict new data. For example, classifying email as spam or not, distinguishing between images of cats and dogs, and much more. Discriminative AI is typically used for tasks where the desired output is known (e.g., via Supervised Learning, a form of machine learning). AI excels in sequence prediction, for example, guessing with a high probability what we will type next by analyzing large bodies of existing text, including our personal writing styles.</p> <p>Convolutional Neural Networks (CNNs) were first developed in the 1980s but were only widely used in the early 2000s. In recent years, CNNs have become increasingly popular thanks to their ability to learn from large datasets of images and perform well on various image processing tasks, such as object detection, image classification, and segmentation.</p> <p>Generative AI learns latent structures or representations within data. It enables synthesizing new data using these structures or representations, such as creating stories, music, and visual art from word prompts. Generative AI is used for tasks where the desired output is unknown, or the “correct” output is ill-defined. With Generative AI, AI has transcended into what humans consider creative, original, and sublime. Let us take a closer look at some of AI’s spectacular breakthroughs.</p> <p>Transformers were developed by researchers from the University of Toronto and Google in 2017. Transformers use a specialized mechanism called scaled dot-product attention, which imbues them with a memory-like structure. Transformer-based models are very effective for natural language processing tasks, such as answering questions, summarizing text, and translation. Consequently, they are vital in most Large Language Models (LLM). The most well-known LLM is GPT, the model that powers the popular ChatGPT service.</p> <p>LLMs are trained on massive datasets. They take sequences of prompts, that can be long, to generate context-sensitive responses in addition to being able to be fine-tuned for specialized domains with additional data, be it current affairs, medicine, law, or others. Novel techniques for fine-tuning, such as Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO), have been developed to make LLMs even more compelling.</p> <p>Research and innovation have enabled end-user interactions that are faster, more creative, and more accurate than ever before. Just as significant as the innovations in data science and software is the evolution of infrastructure to power model inference (the process of computing results from AI models) and model training (the process of building an AI model from data). With AI accelerator technology, AI practitioners can iterate faster to deliver higher-quality models in days and weeks versus months. Further, several traditional techniques employed by data scientists and statisticians are being re-evaluated to take advantage of the capabilities of CN systems.</p> <h3 id=merging-of-cloud-native-and-artificial-intelligence>Merging of Cloud Native and Artificial Intelligence<a class=headerlink href=#merging-of-cloud-native-and-artificial-intelligence title="Permanent link">¶</a></h3> <p>As mentioned in the previous section, AI is the broader concept that aims to create systems that can perform tasks akin to humans. Machine learning is a way to learn from and make informed predictions and decisions based on data. It can be thought of as yet another form of automation that involves using algorithms to learn and improve over time without explicit programming. Finally, Data Science, as a multidisciplinary field, melds techniques from statistics, mathematics, and computer science to enact a wide range of activities, from data analysis and interpretation to the application of machine learning algorithms.</p> <p>Thinking about it broadly, we could divide applications for AI, ML, and data science into two broad categories: namely Predictive AI and Generative AI. Predictive AI aims at predicting and analyzing existing patterns or outcomes (e.g., classification, clustering, regression, object detection,etc.). In contrast, generative AI aims at generating new and original content (e.g., LLMs, RAG,etc.). As such, the algorithms and techniques underpinning predictive and generative AI can vary widely.</p> <div class=highlight><pre><span></span><code><a href=#__codelineno-0-1 id=__codelineno-0-1 name=__codelineno-0-1></a>cloud Native AI
<a href=#__codelineno-0-2 id=__codelineno-0-2 name=__codelineno-0-2></a>Proketive Gnererative
<a href=#__codelineno-0-3 id=__codelineno-0-3 name=__codelineno-0-3></a>Workloads cleation Ciect betectio R46
<a href=#__codelineno-0-4 id=__codelineno-0-4 name=__codelineno-0-4></a>chstern Forgcostin LVMS
<a href=#__codelineno-0-5 id=__codelineno-0-5 name=__codelineno-0-5></a>CI CD
<a href=#__codelineno-0-6 id=__codelineno-0-6 name=__codelineno-0-6></a>ML Lifecyele
<a href=#__codelineno-0-7 id=__codelineno-0-7 name=__codelineno-0-7></a>(4/ML/LLM Op)
<a href=#__codelineno-0-8 id=__codelineno-0-8 name=__codelineno-0-8></a>+1→ 几
<a href=#__codelineno-0-9 id=__codelineno-0-9 name=__codelineno-0-9></a>Platform
<a href=#__codelineno-0-10 id=__codelineno-0-10 name=__codelineno-0-10></a>Infrastructure cloud or Opre aws
<a href=#__codelineno-0-11 id=__codelineno-0-11 name=__codelineno-0-11></a>Harelware Accelerators CPU TIU brU
<a href=#__codelineno-0-12 id=__codelineno-0-12 name=__codelineno-0-12></a>(intel) nVIDIA arm AMD "
</code></pre></div> <p>Below is a sample set of areas where predictive and generative AI have distinct needs across computing, networking, and storage:</p> <table> <thead> <tr> <th>Challenges/Need</th> <th>Generative Al</th> <th>Predictive Al</th> </tr> </thead> <tbody> <tr> <td>Computational Power</td> <td>Extremely high. Requires specialized hardware.</td> <td>Moderate to high. General-purpose hardware can suffice.</td> </tr> <tr> <td>Data Volume and Diversity</td> <td>Massive, diverse datasets for training.</td> <td>Specific historical data for prediction.</td> </tr> <tr> <td>Model Training and Fine-tuning</td> <td>Complex, iterative training with specialized compute.</td> <td>Moderate training.</td> </tr> <tr> <td>Scalability and Elasticity</td> <td>Highly scalable and elastic infrastructure (variable and intensive computational demands)</td> <td>Scalability is necessary but lower elasticity demands. Batch processing or event-driven tasks.</td> </tr> <tr> <td>Storage and Throughput</td> <td>High-performance storage with excellent throughput. Diverse data types. Requires high throughput and low- latency access to data.</td> <td>Efficient storage with moderate throughput. It focuses more on data analysis and less on data generation; data is mostly structured.</td> </tr> <tr> <td>Networking</td> <td>High bandwidth and low latency for data transfer and model synchronization (e.g., during distributed training).</td> <td>Consistent and reliable connectivity for data access.</td> </tr> </tbody> </table> <p>In the coming sections, we will explore how to meet the needs that arise from either form, the challenges that come with it, and potential recommendations to employ when faced with such challenges.</p> <h3 id=what-is-cloud-native-artificial-intelligence>What is Cloud Native Artificial Intelligence?<a class=headerlink href=#what-is-cloud-native-artificial-intelligence title="Permanent link">¶</a></h3> <p>Cloud Native Artificial Intelligence allows the construction of practical systems to deploy, run, and scale AI workloads. CNAI solutions address challenges AI application scientists, developers, and deployers face in developing, deploying, running, scaling, and monitoring AI workloads on cloud infrastructure. By leveraging the underlying cloud infrastructure’s computing (e.g., CPUs and GPUs), network, and storage capabilities, as well as providing isolation and controlled sharing mechanisms, it accelerates AI application performance and reduces costs.</p> <p>Figure 2 (below) maps these enabling mechanisms between tooling and techniques.</p> <div class=highlight><pre><span></span><code><a href=#__codelineno-1-1 id=__codelineno-1-1 name=__codelineno-1-1></a>General Orchestration
<a href=#__codelineno-1-2 id=__codelineno-1-2 name=__codelineno-1-2></a>Automates Enables
<a href=#__codelineno-1-3 id=__codelineno-1-3 name=__codelineno-1-3></a>Enhances
<a href=#__codelineno-1-4 id=__codelineno-1-4 name=__codelineno-1-4></a>Enables Workloads Observability
<a href=#__codelineno-1-5 id=__codelineno-1-5 name=__codelineno-1-5></a>Model Delivery(/CD) Provides infra for Enables scalable Facilitates Depends on
<a href=#__codelineno-1-6 id=__codelineno-1-6 name=__codelineno-1-6></a>Distributed Training ModelLLM Observability
<a href=#__codelineno-1-7 id=__codelineno-1-7 name=__codelineno-1-7></a>Data Storage
<a href=#__codelineno-1-8 id=__codelineno-1-8 name=__codelineno-1-8></a>Data Science
<a href=#__codelineno-1-9 id=__codelineno-1-9 name=__codelineno-1-9></a>Model Serving Deploys Implements Affects
<a href=#__codelineno-1-10 id=__codelineno-1-10 name=__codelineno-1-10></a>Automates Vector Databases Data Architecture
<a href=#__codelineno-1-11 id=__codelineno-1-11 name=__codelineno-1-11></a>Auto ML
<a href=#__codelineno-1-12 id=__codelineno-1-12 name=__codelineno-1-12></a>Running AI On Cloud Native Infrastructure
</code></pre></div> <p>The value of Cloud Native for AI is highlighted by articles in the media published by cloud service providers and/or AI companies. The emergence of AI-related offerings by cloud providers and emerging start-ups in this space are crucial indicators of how Cloud Native principles can shape the systems necessary for the AI evolution.</p> <div class=highlight><pre><span></span><code><a href=#__codelineno-2-1 id=__codelineno-2-1 name=__codelineno-2-1></a>OPENAI
<a href=#__codelineno-2-2 id=__codelineno-2-2 name=__codelineno-2-2></a>Scaling Kubernetes to 7,500 nodes
<a href=#__codelineno-2-3 id=__codelineno-2-3 name=__codelineno-2-3></a>HUGGING FACE
<a href=#__codelineno-2-4 id=__codelineno-2-4 name=__codelineno-2-4></a>Hugging Face Collaborates with Microsoft to launch Hugging Face Model Catalog on Azure
</code></pre></div> <p>Cloud Native Artificial Intelligence is an evolving extension of Cloud Native.</p> <p>Kubernetes is an orchestration platform that can be used to deploy and manage containers, which are lightweight, portable, self-contained software units. AI models can be packaged into containers and then deployed to K8s clusters. Containerization is especially crucial for AI models because different models typically require different and often conflicting dependencies. Isolating these dependencies within containers allows for far greater flexibility in model deployments. CN tooling allows for the efficient and scalable deployment of AI models, with ongoing efforts to tailor these for AI workloads specifically.</p> <p>The Kubernetes Scheduler continues to evolve, particularly to better integrate and support sharing Graphics Processing Units (GPUs) that have become highly popular in speeding AI workloads. Beyond supporting applications sharing a GPU and handling multi-tenancy, efforts are underway to support leveraging remote pools of resources outside of Kubernetes.</p> <p>High-quality data is needed to train and test AI models to obtain superior inference. Cloud Native infrastructure can access data through various methods, such as data lakes and warehouses. Many cloud providers offer block, object, and file storage systems that are perfect for providing low-cost, scalable storage. For example, the size of models can run into gigabytes. During the training phase, pulling the model’s checkpoints each time can cause a severe load on networking and storage bandwidth. Treating models as containerized artifacts opens the door for hosting them in OCI <span class=arithmatex>\(OCl^{24}\)</span> registries and enables caching. It further allows applying software supply chain best practices to models such as artifact signing, validation, attestation, and data provenance. Additionally, containerizing models/artifacts facilitate bundling in WebAssembly (WASM) binaries. WASM is a platform-independent, efficient CN approach to inference.</p> <h3 id=why-cloud-native-artificial-intelligence>Why Cloud Native Artificial Intelligence?<a class=headerlink href=#why-cloud-native-artificial-intelligence title="Permanent link">¶</a></h3> <p>With its elastic, always-on infrastructure, the cloud has allowed enterprises, startups, and developers to prototype quickly, offer new services, scale solutions, and much more. It also does so cost-effectively through resource sharing. The average user no longer has to worry about ordering hardware or dealing with logistics like space, power, network connectivity, cooling, software licensing, and installation. AI has similar concerns – rapid prototyping, accessing storage, networking, and computing resources to tackle small and large-scale training and inference tasks.</p> <h3 id=using-ai-to-improve-cloud-native-systems>Using AI to Improve Cloud Native Systems<a class=headerlink href=#using-ai-to-improve-cloud-native-systems title="Permanent link">¶</a></h3> <p>Whether packaged as observability tooling or leveraging LLM capabilities for natural language processing (NLP) of logs, AI-powered solutions/projects are entering the hands of operators and end-users to enhance their productivity and make their lives easier. One such open source Cloud Native Computing Foundation (CNCF) project is K8sGPT, which leverages the pattern recognition and language capabilities of LLM, such as Bedrock, Cohere, and others, to aid K8s operators in their daily work. More significantly though, the symbiosis of CN and AI opens up the ecosystem to new and unforeseen opportunities. For example, we expect a rise in less technical users able to operate and manage complex systems.</p> <h2 id=challenges-for-cloud-native-artificial-intelligence>Challenges For Cloud Native Artificial Intelligence<a class=headerlink href=#challenges-for-cloud-native-artificial-intelligence title="Permanent link">¶</a></h2> <p>It’s important to note that CNAI challenges will vary between the different personas. And, while Cloud Native’s flexible, scalable platform is a promising fit for AI workloads, AI’s scale and latency needs pose challenges and expose gaps in CN technologies while also presenting opportunities. We tease these out in the context of an end-to-end ML pipeline, also referred to in the literature as MLOps. Issues with the traditional trade-offs of time and space, parallelism, and synchronization all surface, exposing ease-ofuse gaps. To summarize, the ML Lifecycle looks as follows:</p> <p>Preparation Data Feature Store ML TrainTune Development Model Storage Mooel Mode! Serving Repeat the Process</p> <p>The typical ML pipeline is comprised of: - Data Preparation (collection, cleaning/pre-processing, feature engineering) - Model Training (model selection, architecture, hyperparameter tuning) - CI/CD, Model Registry (storage) - Model Serving - Observability (usage load, model drift, security)</p> <p>The data volumes involved in training, similarity search, and model size, particularly with LLMs, each drive memory and performance considerations. While CN handles access control and scheduling for CPUs, GPU allocation with adequate sharing is still evolving. The ML training phase is all about search, requiring tracking the performance of intermediate models to determine which to keep and how to tune model parameters further to obtain even greater accuracy. Security is more critical given the sensitivity of the handled data and the models’ intrinsic value. Observability is vital to detect model drift, usage load, and more. Let us dive a little deeper into the challenges in each pipeline stage. The reader is encouraged to consider additional challenges related to their domain and add to the conversation.</p> <h3 id=data-preparation>Data Preparation<a class=headerlink href=#data-preparation title="Permanent link">¶</a></h3> <p>As the first phase in an AI/ML pipeline, data preparation can present various challenges. These can be broadly grouped into three main categories: managing large data sizes, ensuring data synchronization during development and deployment, and adhering to data governance policies.</p> <h4 id=data-size>Data Size<a class=headerlink href=#data-size title="Permanent link">¶</a></h4> <p>The demand for data to build better AI/ML models is increasing faster than Moore’s Law, doubling every 18 months. Whether it’s data management/handling, data processing, or data analysis, there is a rapid escalation in data demands for building AI/ML models. Therefore, distributed Cloud Native computing and efficient data movement and storage become essential to bridge the gap between these computational demands and hardware capabilities.</p> <h4 id=data-synchronization>Data Synchronization<a class=headerlink href=#data-synchronization title="Permanent link">¶</a></h4> <p>Data may need to be sourced from multiple disparate locations in different formats; the developer and production environments, more often than not, are different, and all this is in addition to handling the increased complexity arising from distributed computing, such as partitioning and synchronization. Let us take a closer look at the latter.</p> <p>In data processing systems like Spark, the industry-standard interface, SQL, plays a crucial role in providing users with a familiar uniform experience, whether they are prototyping locally or running large workloads in a distributed manner. However, ML workloads don’t have an industry-standard interface. Consequently, data scientists develop their ML Python scripts with small datasets locally, and then distributed systems engineers rewrite these scripts for distributed execution. If the distributed ML workloads do not function as expected, data scientists might need to debug the issues using their local Python scripts. This process is inefficient and often ineffective. This is true despite the availability of better observability tools and the reproducibility afforded by container technology.</p> <p>Potentially viable solutions exist for resolving this inconsistency between local development and production environments. The first is using an industry-standard interface to support the end-to-end ML lifecycle. For example, users can leverage APIs of native ML frameworks like PyTorch or TensorFlow to create training code and validate it by running it locally in a Python runtime. Then, users can easily reuse the same code and leverage the Python SDK from Kubeflow to run this code locally in a distributed fashion via Kind/Minikube or just as easily scale their training code by deploying it to a remote, large-scale Kubernetes cluster using the same Python SDK. Another option is to use a general-purpose distributed computing engine such as Ray, whose computational abstractions also enable users to run the same Ray scripts seamlessly in local and production environments.</p> <p>Data volume is a cross-cutting issue. It also manifests in the training stage.</p> <h4 id=data-governance>Data Governance<a class=headerlink href=#data-governance title="Permanent link">¶</a></h4> <p>Data governance is crucial to building trust and ensuring responsible AI development. One should consider three critical pillars regarding data governance. 1. Privacy and Security: It is essential to navigate the complex landscape of data privacy regulations such as GDPR and CCPA. Robust security measures should be implemented to safeguard sensitive data used in AI models. Encryption, access controls, and regular vulnerability assessments should be used to protect valuable information. 2. Ownership and Lineage: It is imperative to clearly define who owns and has access to the data throughout the AI lifecycle, from collection to use. Data lineage tracking tools should be utilized to understand how data flows through the system, ensuring transparency and accountability. Doing so helps to prevent unauthorized access and misuse of sensitive information. 3. Mitigating Bias: AI models are only as good as the data they are trained on. Hence, it is essential to actively monitor and address potential biases in the data and algorithms. This includes using diverse datasets, employing fairness metrics, and continuously evaluating the model to ensure it delivers fair and ethical outcomes, including capturing its limitations. Model Cards are evolving to capture these.</p> <p>Data privacy and security is a cross-cutting issue that requires consideration at every stage.</p> <h3 id=model-training>Model Training<a class=headerlink href=#model-training title="Permanent link">¶</a></h3> <p>Model training data volumes have risen exponentially, resulting in a need for distributed processing and accelerators to achieve even more parallelism. Further training is an iterative multi-step process, which makes scaling a complex multi-component coordinated task. We review these aspects in greater detail in this section.</p> <h4 id=rising-processing-demands>Rising Processing Demands<a class=headerlink href=#rising-processing-demands title="Permanent link">¶</a></h4> <p>LLMs are rapidly pushing the boundaries to meet the growing AI/ML training and inference computing demands, and accelerators are becoming popular. These range from GPUs from multiple vendors with different capabilities to Google’s tensor processing units (TPUs), Intel’s Gaudi, and even field-programmable gate arrays (FPGAs). These varied compute resources need virtualization support, drivers, the ability to configure and share them, and CN scheduler enhancements. Further, these accelerators’ limited availability and cost have prompted the exploration of multi-cloud resource banding, and even sky computing.</p> <p>Using CN technology for AI can be complex regarding GPU virtualization and dynamic allocation. Technologies, such as vGPUs, MIG, MPS (see glossary), and Dynamic Resource Allocation (DRA), enable multiple users to share a single GPU while providing isolation and sharing between containers in a pod. They can increase GPU utilization, which in turn reduces costs, in addition to allowing multiple workloads to benefit simultaneously from them. However, implementation requires careful orchestration and management, especially when allocating and deallocating resources dynamically. Close collaboration between the AI and CN engineering teams is necessary to ensure smooth and efficient integration.</p> <h4 id=cost-efficiency>Cost Efficiency<a class=headerlink href=#cost-efficiency title="Permanent link">¶</a></h4> <p>The elasticity and scalability inherent in Cloud Native environments allow organizations to provision and scale resources dynamically based on fluctuating demands. This aspect also applies to AI tasks. However, resource proper sizing and reactive scheduling to meet varying workload demands are even more compelling in the context of accelerators such as GPUs, which are expensive and limited in supply. It drives the need to be able to fractionalize GPUs to utilize them better.</p> <p>Reducing the carbon footprint during model serving can be achieved using an autoscaling serving framework, which dynamically adjusts resources based on demand. KServe, an LF AI&amp;Data Foundation project, provides such functionality. Sustainability can be significantly improved by various means, such as using smaller, more specialized models, using a mixture of experts, and techniques such as compression and distillation. Distributing ML serving into geographical regions powered by renewable or cleaner energy sources can significantly reduce carbon footprint.</p> <p>Responsible development of ML models can include metadata on carbon footprints to aid in tracking and reporting the impact of model emissions on the environment. Additional tooling, such as mlco2 and codecarbon exists, with limitations, to help predict the carbon footprint of new neural networks before physical training.</p> <h4 id=scalability>Scalability<a class=headerlink href=#scalability title="Permanent link">¶</a></h4> <p>AI/ML workflows are complex and characterized by diverse components that run in a distributed environment. In the context of training, this complexity is particularly exacerbated by the data volumes being handled and the need to support multiple rounds of training until model convergence. Coordinating the scaling of various microservices, with each encapsulating specific AI functionalities, demands intricate orchestration to ensure seamless communication and synchronization. Furthermore, the heterogeneity of AI models and frameworks complicates standardization, making creating generic scaling solutions applicable across various applications challenging.</p> <h4 id=orchestrationscheduling>Orchestration/Scheduling<a class=headerlink href=#orchestrationscheduling title="Permanent link">¶</a></h4> <p>As alluded to earlier, Cloud Native tools and projects simplify the orchestration and scheduling of AI workloads by leveraging the inherent features of containerization, microservices, and scalable cloud infrastructure. Complex AI workflows can be decomposed into modular components, making it easier to manage and scale specific functions independently.</p> <p>However, as mentioned earlier, GPUs are a precious and in-demand resource, and the ability to more efficiently manage their sharing and scheduling for GPU-based AI workloads is critical to the success of AI development teams. Well-tested tools for addressing advanced scheduling needs like bin packing, placement, resource contention, and pre-emption will be essential and foundational for cloud native AI to thrive. Better scheduling support is evolving in Kubernetes through efforts such as Yunikorn, Volcano, and Kueue, the latter two addressing batch scheduling, which is particularly valuable for efficient AI/ML training. Training jobs benefit from gang (or group) scheduling, as the container replicas belonging to the job need an all-or-nothing placement policy to function correctly, and those jobs are not easily scaled up or down. Gang scheduling support is an area of opportunity.</p> <h4 id=custom-dependencies>Custom Dependencies<a class=headerlink href=#custom-dependencies title="Permanent link">¶</a></h4> <p>AI applications often rely on specific frameworks and versions of libraries, and these dependencies may not be readily available or compatible with standard container images.</p> <p>Since many AI workloads benefit from GPU acceleration, having the necessary GPU drivers and libraries to support running workloads on GPUs can be challenging, especially when dealing with different vendors and GPU architectures. For example, when running distributed training on NVIDIA devices, one can use NVIDIA Collective Communications Library (NCCL), to take advantage of optimized multi-GPU and multi-node communication primitives. Different versions of the library might lead to different performance. Reproducible builds, a good build hygiene practice for all software, require using versioned dependencies to avoid runtime incompatibilities and performance surprises.</p> <h3 id=model-serving>Model Serving<a class=headerlink href=#model-serving title="Permanent link">¶</a></h3> <p>Model serving differs chiefly from data processing and training because of load variability and often latency requirements. Further, there are considerations of service resiliency in addition to sharing infrastructure to reduce costs. Also, AI model characteristics are distinct, varying significantly across classical ML, Deep Learning (DL), Generative AI (GAI) LLMs, and, more recently, the multi-modal approaches (e.g., text to video). Different workloads necessitate varied support from ML infrastructure. For example, before the emergence of LLMs, model serving typically required only a single GPU. Some users opted for CPU-based inference if the workloads were not latency-sensitive. However, when serving LLMs, the performance bottleneck shifts from being compute-bound to memory-bound due to the autoregressive nature of the Transformer decoder.</p> <p>This section explores how CN supports these facets and what challenges remain.</p> <h4 id=microservice-architecture-and-developer-experience>Microservice Architecture and Developer Experience<a class=headerlink href=#microservice-architecture-and-developer-experience title="Permanent link">¶</a></h4> <p>CN is based on microservice architecture. However, this may pose a challenge for AI, dealing with each stage in the ML pipeline as a separate microservice. Many components may make maintaining and synchronizing their outputs and hand-offs challenging. Even if users only want to play with these solutions on their laptops, they might still need to create tens of Pods. The complexity makes the infrastructure lack the flexibility to adapt to versatile ML workloads.</p> <p>Second, the microservice-based ML infrastructure leads to a fragmented user experience. For example, in their daily workflows, AI Practitioners may need to build container images, write custom resource YAML files, use workflow orchestrators, and so on instead of focusing solely on their ML Python scripts. This complexity also manifests as a steeper learning curve, requiring users to learn many systems outside their expertise and/or interest.</p> <p>Third, the cost increases significantly when integrating each stage from different systems in the ML model lifecycle. The Samsara engineering blog mentions that its ML production pipelines were hosted across several microservices with separate data processing, model inference, and business logic steps. Split infrastructure involved complex management to synchronize resources, slowing the speed of development and model releases. Then, using Ray, Samsara built a unified ML platform that enhanced their production ML pipeline performance, delivering nearly a 50% reduction in total yearly ML inferencing costs for the company, stemming chiefly from resource sharing and eliminating serialization and deserialization across stages.</p> <p>These issues highlight the need for a unified ML infrastructure based on a general-purpose distributed computation engine like Ray. Ray can supplement the existing Cloud Native ecosystem, focusing on computation, allowing the Cloud Native ecosystem to concentrate on deployment and delivery. The Ray/KubeRay community has collaborated extensively with multiple Cloud Native communities, such as Kubeflow, Kueue, Google GKE, and OpenShift.</p> <h4 id=model-placement>Model Placement<a class=headerlink href=#model-placement title="Permanent link">¶</a></h4> <p>Users ideally like to deploy multiple, possibly unrelated, models for inference in a single cluster while also seeking to share the inference framework to reduce costs and obtain model isolation. Further, for resiliency, they want replicas in different failure zones. Kubernetes provides affinity and anti-affinity mechanisms to schedule workloads in different topology domains (e.g., zone, node), but usability improvements can help users take advantage of these features.</p> <h4 id=resource-allocation>Resource Allocation<a class=headerlink href=#resource-allocation title="Permanent link">¶</a></h4> <p>Model serving requires handling, chiefly, the model parameters. The number of parameters and the representation size indicate the memory needed. Unless dealing with a trillion parameter LLM, these typically require only a portion of a GPU. This highlights the need to be able to fractionalize expensive accelerators like GPUs. The DRA project, which is still in alpha, seeks to make GPU scheduling more flexible.</p> <p>Another consideration is response latency, which depends significantly on the use case. For instance, the response latency desired to detect objects on the road in an autonomous driving context is several orders lower than tolerable while creating an image or writing a poem. Additional serving instances may need to be launched for low-latency applications under high-load conditions. These could land on a CPU, GPU, or other computing resource if the desired latency can be honored. Support for such cascading opportunistic scheduling on available resources is still evolving in Kubernetes.</p> <p>Further, event-driven hosting is ideal for not wasting resources and keeping costs down. The Kubernetes Event Driven Autoscaling (KEDA) project is well-suited here, provided the model loading latency is tolerable to still deliver on the end-to-end service latency. An opportunity here is to provide better support for model sharing by delivering models in an Open Container Initiative (OCI) format, an immutable file system that lends itself to sharing. Another solution is to use AI for CN, in particular, to predict use and proactively float or shut down serving instances to handle the expected load.</p> <h3 id=user-experience>User Experience<a class=headerlink href=#user-experience title="Permanent link">¶</a></h3> <p>The hallmark of CN, aka containers, allows portability and reproducibility, while Kubernetes’ APIs and operators, like Kubeflow, simplify the deployment of the AI workloads, making them “write once and run (virtually) anywhere’’ in an easily scalable fashion. Once users transition from traditional batch systems on bare metal or virtualized environments to containers and Kubernetes, they appreciate the benefits of cloud technologies despite their initial adoption challenges. The learning curve, however, can be steep.</p> <p>Let’s consider AI training workloads. Configuring the runtime environment can be time-consuming, particularly when highly customizable libraries are used. The user has the option to use default settings for a plethora of environment variables, but these may yield inferior performance. Once optimized on a given Kubernetes platform for a particular training workload, there are no guarantees it will perform likewise on another platform or training task or container bundle with different libraries included. This affects workload portability and ease of use.</p> <p>The previous paragraph looked at just one stage in an AI pipeline, typically multi-stage, spanning data preparation, training, tuning, serving, and fine-tuning. How can one provide a seamless user experience for AI practitioners who aren’t necessarily savvy with systems or cloud concepts and provide them with a streamlined product experience that eliminates friction in AI development? Giving AI practitioners user-friendly and well known SDKs written in Python that abstract away the complex details of Kubernetes can help increase the adoption of Cloud Native AI tools. Users would like to build ML models using PyTorch and TensorFlow and then quickly and easily deploy them to Kubernetes infrastructure by using simple Python SDKs without worrying about details such as packaging, building Docker images, creating Kubernetes custom resources (e.g., PyTorchJob, TFJob), and scaling those models using complex cloud native tools. A strong product development focus will be required to invent an open source product experience for the MLOps lifecycle, which is much more user friendly.</p> <p>Integrating tools like JupyterLab, which contains space for an IDE-like experience with useful APIs that may exist in AI/ML tools available today (ex., Kubeflow Katib API), would allow ML practitioners to more quickly iterate on their AI development with fewer context switches across multiple user interfaces. JupyterLab’s extensible nature gives ML practitioners a workspace to build, deploy, and monitor AI/ML workloads within a familiar tool without learning new tools and interfaces. It is even possible to use JupyterLab to schedule workflows of code developed in individual AI/ML Notebooks using GUI workflow building tools like Elyra coupled with Kubeflow Pipelines.</p> <p>Big Data, inside and outside the enterprise, is a mainstay of AI. It is essential to consider how to bridge the gap between the Big Data and ML ecosystems. For example, modern Generative AI models require large amounts of data for training. Still, the tools for loading large amounts of data from formats like Iceberg into training frameworks like PyTorch require enhancement, with tools like TorchArrow and PyIceberg demonstrating early promise. Tools used for large-scale data preparation, like Spark, aren’t well connected to the tools in the ML ecosystem. Extra overhead is required to prepare data, build features, store features to disk, and then read those features back into memory for use in training workloads. Solutions like RayData or a data caching microservice built upon Arrow Flight RPC may significantly improve the Input/Output overhead involved with the first phases of training workloads.</p> <p>ML tools are complex, and users typically need help to deploy them on Kubernetes. It is nontrivial to identify and deploy appropriate drivers for GPUs and make them compatible with a user’s AI/ML workloads. The upgrade path for existing ML workloads should be simplified and improved, similar to other Kubernetes control plane components. Users should get clear guidelines on how to keep their AI workloads resilient to Kubernetes upgrades and cluster downtime.</p> <p>Another aspect that affects the ease of use is multi-tenancy, using quotas and namespaces. Non-admin users need help to figure out the system resources available to them. Typically, administrators provide tools (e.g., Grafana dashboards) for observability; when these are lacking, non-expert/non-admin users are left in the lurch.</p> <p>Finally, debugging is challenging, made more so in distributed environments and even more so when the processing pipeline comprises multiple complex services. Hardware and software failure might be more or less explicit and easy to identify to a cloud user, but an AI practitioner may need help to see the complete picture of failure. For example, NCCL termination errors can be vague with any of a multitude of possible causes, each requiring investigation. The user may need to parlay the error message to an administrator for further assistance.</p> <h3 id=cross-cutting-concerns>Cross-Cutting Concerns<a class=headerlink href=#cross-cutting-concerns title="Permanent link">¶</a></h3> <p>In the previous sections, we addressed challenges specific to a stage in the AI pipeline. But others are common to all stages and all software applications, spanning reference implementations, observability, security, and more. For instance, right-sizing resources are valid for processing data, training, or serving. It has resource utilization, cost, and sustainability ramifications. Let us dive a little deeper into them.</p> <h4 id=reference-implementation>Reference Implementation<a class=headerlink href=#reference-implementation title="Permanent link">¶</a></h4> <p>Neither cloud nor AI are easy studies, and getting them to work together after making choices from many tools and projects is non-trivial. Adoption needs to be improved by requiring a reference implementation that meets a majority of simple use cases. Kind for Kubernetes did wonders to help developers get started on their laptops. Jupyter Notebook did likewise for the budding AI/ML developer. We need something similar for an AI/ML pipeline that runs in the cloud.</p> <h4 id=right-sizing-resource-provisioning>Right-sizing Resource Provisioning<a class=headerlink href=#right-sizing-resource-provisioning title="Permanent link">¶</a></h4> <p>AI/ML workloads are resource intensive, especially with LLMs with their billions or trillions of parameters. As discussed earlier, accelerators like GPUs are expensive and in short supply, and it is essential to use the proper size allocation to save resources and control costs. We need to be able to not only timeslice GPUs but also slice or partition them into fractional sections and allocate them judiciously as required by different workloads. In conjunction with the above back-end effort, there is a need for front-end support to request GPU sub-units and configure them while launching workloads.</p> <p>To address this need, Kubernetes introduced a new API, Dynamic Resource Allocation (DRA), as alpha in v1.26. The API provides more flexibility to manage specialized hardware resources, in particular: - Network-attached resources - Arbitrary parameters for resource requests - Arbitrary, resource-specific setup and cleanup actions - Custom matching resource requests with available resources, including handling optional requests.</p> <p>The DRA API offers several advantages compared to existing approaches: - Custom hardware can be added by developing and deploying DRA drivers without needing to modify the core Kubernetes codebase - Vendors can define resource parameters - Resources can be shared between containers and pods</p> <h4 id=cost-control>Cost Control<a class=headerlink href=#cost-control title="Permanent link">¶</a></h4> <p>AI/ML can quickly become a budget black hole. Automating resource allocation and scaling processes to optimize AI cloud costs is essential. Microservices can be scaled individually as needed. Further, it lends itself well to using the Kubernetes auto-scaling feature that will further help right sizing the number of active instances and thus the infrastructure costs. Last, Spot Instances can be leveraged with policies that capture balancing risk with meeting Service Level Agreements (SLAs).</p> <h4 id=observability>Observability<a class=headerlink href=#observability title="Permanent link">¶</a></h4> <p>Observability is valuable across the AI/ML pipeline. CN offers tools like OpenTelemetry and Prometheus that can monitor load, number of accesses, response latency, and more. It is vital to monitor model performance and health in production environments. It is crucial to keep track of model drift to ensure the accuracy and reliability of your AI system. For example, facial recognition systems may experience degradation as more people wore masks during the COVID-19 pandemic. Similarly, a housing price predictor model may diverge from reality due to external factors such as natural disasters or changes in interest rates. Therefore, monitoring your AI models continuously is essential to detect any performance issues and make necessary adjustments.</p> <p>Infrastructure monitoring is essential, especially with long running workloads. As AI training workloads run, anomalies in GPUs and networking may happen at times. Examples are errors in the GPU memory or unreachable nodes, which may result in the job crashing.</p> <p>However, issues that are not immediately identifiable may arise: for instance, training performance may start to degrade without any apparent hardware fault being reported. In these cases, only deep diagnostics could identify the issues. Current metrics do not expose results from deep diagnostics. Therefore, providing tools to detect, avoid, and handle infrastructure issues before, during, and after running AI training jobs becomes crucial.</p> <h4 id=disaster-recovery-and-business-continuity>Disaster Recovery and Business Continuity<a class=headerlink href=#disaster-recovery-and-business-continuity title="Permanent link">¶</a></h4> <p>All production services must be resilient, with backups. AI services are no different. Failed or slow to respond services can cause reputational damage and loss of revenue. Developing a comprehensive disaster recovery plan is essential, which may include data backup, running instances in multiple availability zones, and running multiple instances. Policies can help with these.</p> <h4 id=security-and-compliance-audits>Security and Compliance Audits<a class=headerlink href=#security-and-compliance-audits title="Permanent link">¶</a></h4> <p>All outward facing services, particularly Model Serving instances, need firewall protection, access control, and more. And like any other service, your AI/ML workloads must follow security best practices. These include penetration testing, vulnerability scanning, and compliance checks of the workload domain, such as health care, finance, etc.</p> <p>Tools like Grype and Trivy can scan containerized workloads for vulnerabilities. Kyverno and policy enforcement services can ensure containerized workloads are running at the lowest privilege necessary with minor capabilities needed.</p> <p>An additional layer of security is possible using confidential computing or Trusted Execution Environments (TEE). These hardware-supported environments provide encrypted memory, data integrity protection, and testability. TEEs protect the data and workload from other infrastructure users while in use. AMD, Intel, NVIDIA, and IBM have TEE offerings, and they are becoming available in public clouds. Protecting sensitive data such as health care and financial information and ML models are prime use cases.</p> <h4 id=sustainability>Sustainability<a class=headerlink href=#sustainability title="Permanent link">¶</a></h4> <p>AI/ML model training has always been resource intensive, especially with Large Language Models like GPT-3. Training emissions are comparable to multiple transcontinental flights, while inference emissions add up due to high query volumes. The industry’s trend towards oversized models for market dominance leads to inefficiencies, contributing to energy and resource consumption. More transparency and standardization in reporting the environmental impacts of a model are challenges.</p> <p>Recently, there have been efforts to increase transparency with LLama, while some insights are becoming available concerning water usage for cooling servers running LLMs, like ChatGPT. ChatGPT’s carbon footprint is significant, given its millions of users.</p> <p>The drive for sustainability presents opportunities for innovation. DeepMind’s BCOOLER and smaller, more efficient models like DistilBERT and FlexGen show promise in reducing AI/ML energy consumption. Adopting best practices like efficient ML architectures, optimized processors, and locating cloud computing infrastructure in energy-efficient locations can curb the carbon footprint of ML training. Google has been successful in controlling the energy consumption of its machine learning systems.</p> <h4 id=education-for-kids>Education for Kids<a class=headerlink href=#education-for-kids title="Permanent link">¶</a></h4> <p>Today, technology education mainly focuses on traditional programming languages without AI or computer assistance. Schools typically don’t use modern IDEs that support refactoring, templating, or API assistance and will have students code on a contained website for ease of setup. They also don’t teach the use of AI coding assistance technologies like Github’s Copilot, even though this will become the standard mode of development in the future. Most students aren’t even aware this technology exists. Schools actively dissuade students from using AI technologies like ChatGPT and Copilot due to concerns about cheating. This prevents students from learning how to use AI technologies to augment their work and excel effectively. Because schools paint AI technology in a negative light, studious students get scared off from using it, and the students looking for a way to avoid doing their homework are more likely to use AI.</p> <p>The challenges mentioned above provided us insight into areas of concern when it comes to implementing CNAI systems. Fortunately, CN tooling is facing many challenges head-on. We next consider opportunities that stem from these challenges.</p> <h2 id=path-forward-with-cloud-native-artificial-intelligence>Path Forward with Cloud Native Artificial Intelligence<a class=headerlink href=#path-forward-with-cloud-native-artificial-intelligence title="Permanent link">¶</a></h2> <p>This section provides a forward looking approach to taking the initiative to implement CNAI. We begin with recommendations (or actions), then enumerate existing yet evolving solutions (i.e., CNAI software), and finally consider opportunities for further development.</p> <h3 id=recommendations>Recommendations<a class=headerlink href=#recommendations title="Permanent link">¶</a></h3> <h4 id=flexibility>Flexibility<a class=headerlink href=#flexibility title="Permanent link">¶</a></h4> <p>Sometimes, the variety of options regarding AI can become overwhelming. Fortunately, thanks to many, popular tools and techniques remain valid in this new world. From REST interfaces for interface to cloud based resources and services, CN technologies work well today and will continue to work well as new offerings evolve.</p> <h4 id=sustainability_1>Sustainability<a class=headerlink href=#sustainability_1 title="Permanent link">¶</a></h4> <p>Improving the accountability of AI workload environmental impact is crucial for ecological sustainability, particularly in the cloud native landscape. This can be achieved by supporting projects, methodologies, and taxonomy that help clarify, classify, and catalyze AI workload on ecological sustainability. Additionally, integrating cloud native technologies to optimize AI workload scheduling, autoscaling, and tuning is necessary. Furthermore, advocating for adopting standardized methodologies in environmental impact assessments is vital. It is also important to promote the development and use of energy-efficient AI models and foster transparency in model development and usage, primarily through cloud native stacks such as Kubeflow. Finally, emphasizing the importance of purposeful and efficient AI usage will help minimize unnecessary computational loads.</p> <h4 id=custom-platform-dependencies>Custom Platform Dependencies<a class=headerlink href=#custom-platform-dependencies title="Permanent link">¶</a></h4> <p>We recommend ensuring the Cloud Native environment has the required GPU drivers and supports GPU acceleration for AI workloads. This is crucial as AI applications often depend on specific frameworks and library versions that may not be easily accessible or compatible with standard container images. This will help with the challenge of having various vendors and GPU architectures.</p> <h4 id=reference-implementation_1>Reference Implementation<a class=headerlink href=#reference-implementation_1 title="Permanent link">¶</a></h4> <p>Given the number and complexity of the tools involved in AI development, it may be advisable to consider the value of a Cloud Native, OpenTofu-based reference implementation of a user-friendly combination of various tools that can provide a product-like experience for any team around the world to get started doing AI/ML in the Cloud quickly. Combining the best available open source tools for data preparation, feature store, training, tuning, model registry, and serving can help teams get started doing machine learning quickly and scale up their work efficiently using the power of the Cloud. Consider the value/power of combining a sophisticated set of technologies into a functional and scalable distribution to serve such a purpose. (e.g. JupyterLab, Kubeflow, PyTorch, Spark/Ray/Trino, Iceberg, Feast, MLFlow, Yunikorn, EKS/GKE, S3/GCS, etc.). Such a reference implementation may be extremely valuable for advancing open and responsible AIML development powered by Cloud-based technologies.</p> <h4 id=industry-acceptance-of-terminology>Industry Acceptance of Terminology<a class=headerlink href=#industry-acceptance-of-terminology title="Permanent link">¶</a></h4> <p>As AI becomes ubiquitous, it becomes increasingly complex in some dimensions but simpler in others. For example, terminology evolves, providing businesses with more effortless conversations about AI (e.g., terms such as “repurpose” to reuse existing content). This also applies to more technical terms, such as RAG, Reason, and Refinement.</p> <h3 id=evolving-solutions-for-aiml>Evolving Solutions for AI/ML<a class=headerlink href=#evolving-solutions-for-aiml title="Permanent link">¶</a></h3> <p>The following are just a few examples of specific tools or technologies that have become options to enable AI, including CNAI.</p> <h4 id=orchestration-kubeflow>Orchestration - Kubeflow<a class=headerlink href=#orchestration-kubeflow title="Permanent link">¶</a></h4> <p>Kubeflow is an example of a CNAI tool supporting ML Operations (MLOps). Using technologies such as Kubernetes, stateless architectures, and distributed systems, Kubeflow helps AI/ML communities adopt Cloud Native tools more efficiently. The successful adoption of Kubeflow highlights the successful integration of Cloud Native technologies for AI/ML/DL. Kubeflow has been highly progressive in its ability to apply machine learning concepts to elastic substrates provided by Kubernetes, with many other projects following suit. Kubeflow follows Kubernetes best practices and applies them to the AI/ML space, such as declarative APIs, composability, and portability. Kubeflow implements individual microservices for every stage of the ML lifecycle. For example, Kubeflow Training Operator is used for distributed training, Katib is used for hyperparameter tuning fine-tuning, and Kubeflow KServe is used for model serving. That allows users to integrate individual Kubeflow components into their ML infrastructure or use Kubeflow as an end-to-end ML platform.</p> <h4 id=context-vector-databases>Context - Vector Databases<a class=headerlink href=#context-vector-databases title="Permanent link">¶</a></h4> <p>LLMs are trained with vast volumes of, typically, publicly available data at a point in time. We interact with them via prompts. But to make the responses more valuable without the user having to enter longer or multiple prompts and possibly retrieve more domain-specific responses, it is helpful to “enrich” the prompt. This is where vector databases come in. They are giant, indexed stores of vectors, a mathematical representation of data in numerical form. Embeddings are a specific vector representation of each additional piece of data, often proprietary, domain specific, or newer, that aims to capture relationships and similarities (context) between the data they represent. The user-provided LLM prompt is transformed using the same embedding used by the vector database, and the resulting vector is then used to find similar vectors in the database. They are then merged to provide additional context before feeding into the LLM to generate a response. Multi-modal GenAI systems would handle prompts that might be text, images, audio, or other, with the embedding ability to handle diverse input.</p> <p>Vector databases can be purpose-built or traditional databases with extensions to handle vectors more specifically. Instances may vary in their choice of indexing scheme, distance metric used to compute similarity, and whether and what data compression technique they employ. Some offerings include Redis, Milvus, Faiss, and Weaviate.</p> <h4 id=observability-openllmetry>Observability - OpenLLMetry<a class=headerlink href=#observability-openllmetry title="Permanent link">¶</a></h4> <p>OpenLLMetry is a project that builds on top of OpenTelemetry to enable thorough and vendor-neutral instrumentation for LLM Observability. Because Generative AI is not debuggable in the traditional sense (i.e., you can’t “just step through the code”), developers must turn towards Observability tools and practices to improve their use of Generative AI over time. This data is also often the source of evaluations and fine-tuning workflows.</p> <h3 id=opportunities>Opportunities<a class=headerlink href=#opportunities title="Permanent link">¶</a></h3> <h4 id=cncf-project-landscape>CNCF Project Landscape<a class=headerlink href=#cncf-project-landscape title="Permanent link">¶</a></h4> <p>Several Linux Foundation (LF) groups, including CNCF, LF AI &amp; Data, along with partners such as the AI Alliance, and more, provide a hub for AI projects that both AI and cloud engineers can use. Existing tools, such as the Cloud Native Landscape, give a bird’s eye view into the CN ecosystem. The following figure lists established and evolving projects grouped by their functional area.</p> <div class=highlight><pre><span></span><code><a href=#__codelineno-3-1 id=__codelineno-3-1 name=__codelineno-3-1></a>Kubeflow Trnin Oprater Pytorch DDP
<a href=#__codelineno-3-2 id=__codelineno-3-2 name=__codelineno-3-2></a>Torchx
<a href=#__codelineno-3-3 id=__codelineno-3-3 name=__codelineno-3-3></a>Tensorflow Distbuted DeepSpeed Open MPI Megatron Horovod Apla Distributed Training General Orchestration -Volcano -Kuberay Nivida NeMo Armada
<a href=#__codelineno-3-4 id=__codelineno-3-4 name=__codelineno-3-4></a>Yunikorn
<a href=#__codelineno-3-5 id=__codelineno-3-5 name=__codelineno-3-5></a>Kueue
<a href=#__codelineno-3-6 id=__codelineno-3-6 name=__codelineno-3-6></a>Langfuse Flame
<a href=#__codelineno-3-7 id=__codelineno-3-7 name=__codelineno-3-7></a>Deepchecks Model/LLM Observability
<a href=#__codelineno-3-8 id=__codelineno-3-8 name=__codelineno-3-8></a>OpenLLMetry
<a href=#__codelineno-3-9 id=__codelineno-3-9 name=__codelineno-3-9></a>Kserve
<a href=#__codelineno-3-10 id=__codelineno-3-10 name=__codelineno-3-10></a>Seldon
<a href=#__codelineno-3-11 id=__codelineno-3-11 name=__codelineno-3-11></a>Weaviate Chroma Milus ML Serving YLLM TGI Skypilot
<a href=#__codelineno-3-12 id=__codelineno-3-12 name=__codelineno-3-12></a>Qundrant
<a href=#__codelineno-3-13 id=__codelineno-3-13 name=__codelineno-3-13></a>ElesticSearch Postgres SQIL Redis Externsions Pinecone Vector Databases AI Landscape Cloud Native CI/CD - Delivery Mlflow Kubeflow Pipelines TFX BentoML MLRur
<a href=#__codelineno-3-14 id=__codelineno-3-14 name=__codelineno-3-14></a>ClickHouse
<a href=#__codelineno-3-15 id=__codelineno-3-15 name=__codelineno-3-15></a>Hadoop HDFS Apache HBase Apache Druid Apache Pinot Cassondrna ScyllabB Data Science Jupyter Kabeflow Norebooks PyTorch TensorFlw Apoche Zeppelin
<a href=#__codelineno-3-16 id=__codelineno-3-16 name=__codelineno-3-16></a>Presto
<a href=#__codelineno-3-17 id=__codelineno-3-17 name=__codelineno-3-17></a>Apache Spark Trinc Data Architecture Prometheus Influxdb
<a href=#__codelineno-3-18 id=__codelineno-3-18 name=__codelineno-3-18></a>Apache Flirk Kafko Workload Observability Grafard Weight and Biases (wondb)
<a href=#__codelineno-3-19 id=__codelineno-3-19 name=__codelineno-3-19></a>Pulsar Fluid- -Open Telemetry
<a href=#__codelineno-3-20 id=__codelineno-3-20 name=__codelineno-3-20></a>Memcoched
<a href=#__codelineno-3-21 id=__codelineno-3-21 name=__codelineno-3-21></a>Redis Hyperopt
<a href=#__codelineno-3-22 id=__codelineno-3-22 name=__codelineno-3-22></a>Alluxie Opturnd
<a href=#__codelineno-3-23 id=__codelineno-3-23 name=__codelineno-3-23></a>Apache Superset AutoML Kubeflow Katib
<a href=#__codelineno-3-24 id=__codelineno-3-24 name=__codelineno-3-24></a>NNI
<a href=#__codelineno-3-25 id=__codelineno-3-25 name=__codelineno-3-25></a>Kywero
<a href=#__codelineno-3-26 id=__codelineno-3-26 name=__codelineno-3-26></a>Kyverno-TSON
<a href=#__codelineno-3-27 id=__codelineno-3-27 name=__codelineno-3-27></a>OPA/Gatekeeper Governance &amp; Policy
<a href=#__codelineno-3-28 id=__codelineno-3-28 name=__codelineno-3-28></a>Stacklock Minder
</code></pre></div> <h4 id=cnai-for-kids-and-students>CNAI for Kids and Students<a class=headerlink href=#cnai-for-kids-and-students title="Permanent link">¶</a></h4> <p>Kids already use AI assistive technologies like ChatGPT daily and have no idea how they work. The underpinnings of modern AI, like discriminative and generative AI algorithms, are a black box that kids and even technology savvy parents don’t understand, so it is difficult to take an interest in it. Rather than just taking LLMs like ChatGPT for granted, students’ education should include the basics of neural networks and machine learning algorithms to explain how AI technologies work and how to use them better in their future careers.</p> <p>The Cloud Native community and successful programs like CNCF Kids Day at KubeCon provide educational opportunities on Cloud Native and AI technologies. Introducing kids to AI technologies early will also prevent the diversity, equity, and inclusion issues plaguing computer science. AI is an equalizing technology because people of every race, sexual orientation, and socioeconomic status can experience AI/ML daily and help improve this technology with the proper training and education.</p> <p>The AI/ML revolution is analogous to the dot-com era, where web technology became ubiquitous, and even ordinary workers embraced this technology to improve their business. As AI/ML technology becomes ubiquitous in society, we must ensure that students keep pace with the advances in AI and Cloud Native technologies.</p> <h4 id=participation>Participation<a class=headerlink href=#participation title="Permanent link">¶</a></h4> <p>As AI grows, more opportunities for education and involvement happen. There is room for AI specialists (e.g., Ph.D. in ML to Data Scientists) and AI generalists (e.g., operators and end-users). Educational programs such as MOOCs and certifications have emerged to focus on AI tooling and techniques on all fronts. Professional societies (e.g., ACM and IEEE) and meetups provide chances to meet in person to learn and discuss challenges. Industry groups such as the CNCF, along with Linux Foundation AI, AI Alliance, and others, provide the ability to coordinate projects and protocols at scale.</p> <h4 id=trust-and-safety-safety-by-design>Trust and Safety / Safety By Design<a class=headerlink href=#trust-and-safety-safety-by-design title="Permanent link">¶</a></h4> <p>As we build AI and Cloud Native technology, there is a significant risk of unintended consequences and negative impacts. These can be due to unintentional design issues causing adverse impacts on vulnerable groups, for example, recommending algorithms that inadvertently promote hate-based, violent, extremist material. They can also be due to individuals or groups’ malicious use of systems and/or tools to harm deliberately, such as using Generative AI tools to create misinformation and disinformation campaigns or individuals purposely fine-turning LLMs to produce child sexual abuse material.</p> <p>AI and Cloud Native technology are also at the core of the tooling used by Trust and Safety: “The field and practices employed by digital services to manage content and conduct scans for risks to users and others, mitigate online or other forms of technology-facilitated abuse, advocate for user rights, and protect brand safety.” Systems have been built to deliver every part of the Trust and Safety cycle including identifying and assessing potentially violent behavior, triaging and prioritizing cases, making and recording enforcement decisions, selecting and applying interventions, and gathering threat intelligence. Apart from being central to the safety and health of the internet, these systems can have significant negative impacts if designed without due consideration.</p> <p>Responsible technology is about reducing harm from technology, diversifying the tech pipeline, and ensuring that technology aligns with the public interest. It explores and actively considers tech’s values, unintended consequences, and negative impacts to manage and mitigate risk and harm. As we build AI and Cloud Native technology, we must consider these potential ethical and human rights impacts, optimizing freedom of expression, the right to privacy, the right to life, liberty, and the security of person, and other fundamental universal human rights.</p> <p>The World Economic Forum states: “Safety by Design puts user safety and rights at the center of the design and development of online products and services”. This proactive and preventative approach focuses on embedding safety into the culture and leadership of an organization. It emphasizes accountability and aims to foster more positive, civil, and rewarding online experiences for everyone.</p> <p>There is a growing field of experts to help with these development best practices, such as the Global Internet Forum to Counter Terrorism (GIFCT), The Tech Coalition, and the Internet Society. All Tech is Human curated list of experts in this sector and can provide links to critical resources. The AI Alliance initiative (IBM, Meta, and 50+ institutions) focuses on advancing open innovation and science in AI to propose alternatives to closed AI systems and advance the field of responsible AI (ethics, trust, safety). OpenAI, the organization behind ChatGPT, was initially founded as a non-profit focusing on guaranteeing safety and fairness in AI.</p> <h4 id=the-emergence-of-a-new-engineering-discipline>The Emergence of a New Engineering Discipline<a class=headerlink href=#the-emergence-of-a-new-engineering-discipline title="Permanent link">¶</a></h4> <p>In the last two decades, we have seen how the tech industry has been creating and changing engineering job roles rapidly, depending on their responsibilities. We have witnessed the rise of roles such as DevOps Engineer, SRE Engineer, and Infrastructure Engineer. We foresee the MLDevOps or AI engineer becoming the glue between Data Science, Infrastructure and Development in the next few months or years. It’s important to know that this industry area is developing, and the role titles can fluctuate; only time will tell. Different terms may also become a reality. In the future, that role will need to focus more on AI tooling, infra, and deploying AI chains and agents.</p> <h2 id=artificial-intelligence-for-cloud-native>Artificial Intelligence for Cloud Native<a class=headerlink href=#artificial-intelligence-for-cloud-native title="Permanent link">¶</a></h2> <p>This paper has focused mainly on Cloud Native supporting AI development and usage. But AI can enhance Cloud Native in many ways – from anticipating load and better resource scheduling, particularly with multiple optimization criteria involved, such as power conservation, increased resource utilization, reducing latency, honoring priorities, enhancing security, understanding logs and traces, and much more.</p> <h3 id=natural-language-interface-for-cluster-control>Natural Language Interface for Cluster Control<a class=headerlink href=#natural-language-interface-for-cluster-control title="Permanent link">¶</a></h3> <p>At Cloud Native AI + HPC Day in Chicago in 2023, Kubernetes Controllers with a natural language interface were demonstrated to tackle cluster-related tasks. It used an LLM in that back-end that comprehended user requests and translated them to Kubernetes API calls. It further supported launching chaos tests to ascertain service resiliency, scan for CVEs, and more. It is a precursor to more intuitive orchestration and management of Kubernetes clusters and, in time, lowers the learning curve for administrators and site reliability engineers.</p> <h3 id=security>Security<a class=headerlink href=#security title="Permanent link">¶</a></h3> <p>Machine learning can analyze massive datasets to rapidly identify patterns and predict potential threats or weaknesses in the system. Integrating AI in red teaming accelerates identifying security gaps and allows organizations to strengthen their defenses against emerging cyber threats. ML models that detect anomalous network behavior can just as easily be used in clusters to protect workloads or across a fleet of clusters for edge deployments.</p> <h3 id=smarter-orchestrationscheduling>Smarter Orchestration/Scheduling<a class=headerlink href=#smarter-orchestrationscheduling title="Permanent link">¶</a></h3> <p>AI can analyze historical cluster usage over the day/week/month to identify workload patterns and resource availability, to understand when and how to deploy workloads, whether to scale them horizontally or vertically, when to consolidate workloads on a few nodes to put others into quiescence for power savings or even drop them from the cluster to reduce costs.</p> <p>ML-driven models can optimize task sequencing, automate decision-making processes, and enhance the overall efficiency of workload management. A natural language interface facilitates the whole orchestration and scheduling process. These enhancements would make it easier for organizations to manage and schedule complex workflows in dynamic cloud environments. Processor power models are being built to help plan and optimize for reduced power consumption.</p> <h3 id=ai-integration-efforts-in-flight-and-under-exploration>AI Integration Efforts in Flight and Under Exploration<a class=headerlink href=#ai-integration-efforts-in-flight-and-under-exploration title="Permanent link">¶</a></h3> <ul> <li>Fine-tuned custom LLMs to analyze logs.</li> <li>MLOps pipeline to capture and maintain data provenance.</li> <li>AI semantic conventions to CNCF projects like OpenTelemetry.</li> <li>AI-powered development environments (IDEs) are used to develop and deploy AI applications.</li> </ul> <p>We expect to report on advances in this space in the not-too-distant future.</p> <h2 id=conclusion>Conclusion<a class=headerlink href=#conclusion title="Permanent link">¶</a></h2> <p>Combining Artificial Intelligence (AI) and Cloud Native (CN) technologies offers an excellent opportunity for organizations to develop unprecedented capabilities. With the scalability, resilience, and ease of use of Cloud Native infrastructure, AI models can be trained and deployed more efficiently and at a grander scale. This white paper delves into the intersection of these two areas, discussing the current state of play, the challenges, the opportunities, and potential solutions for organizations to take advantage of this potent combination.</p> <p>While several challenges remain, including managing resource demands for complex AI workloads, ensuring reproducibility and interpretability of AI models, and simplifying user experience for nontechnical practitioners, the Cloud Native ecosystem is continually evolving to address these concerns. Projects like Kubeflow, Ray, and KubeRay pave the way for a more unified and user-friendly experience for running AI workloads in the cloud. Additionally, ongoing research into GPU scheduling, vector databases, and sustainability offers promising solutions for overcoming limitations.</p> <p>As AI and Cloud Native technologies mature, organizations embracing this synergy will be well-positioned to unlock significant competitive advantages. The possibilities are endless, from automating complex tasks and analyzing vast datasets to generating creative content and personalizing user experiences. By investing in the right talent, tools, and infrastructure, organizations can leverage the power of AI and Cloud Native technologies to drive innovation, optimize operations, and deliver exceptional customer experiences.</p> <p>This paper brought to you by the CNCF AI Working Group.</p> <h2 id=appendix>Appendix<a class=headerlink href=#appendix title="Permanent link">¶</a></h2> <h3 id=glossary>Glossary<a class=headerlink href=#glossary title="Permanent link">¶</a></h3> <ul> <li><strong>AI Practitioners</strong>: In the context of this paper, it refers to (not limited to) ML Engineers, Data Scientists, Data Engineers, roles whose primary responsibilities include manipulating relevant data, creating, and optimizing machine learning models.</li> <li><strong>Developers</strong>: In the context of this paper, it refers to (not limited to), Software Engineers, Frontend Engineers, Backend Engineers, Full Stack Engineers, Software Architects, and Software Testers. The roles whose primary responsibility include writing and testing software including user interfaces, microservices, and backend software.</li> <li><strong>Deployers</strong>: In the context of this paper, it refers to (not limited to), DevOps Engineers, Site Reliability Engineers, Infrastructure Engineers, Infrastructure Architects, Application Administrators, Cluster Administrators. The roles whose primary responsibility include deploying software and cloud infrastructure to multiple environments including development, staging and production.</li> <li><strong>DRA</strong>: DRA stands for Dynamic Resource Allocation. It is an API abstraction of general resource claim and provisioning for Pods, allowing 3<sup>rd</sup> party vendors to provide HW/SW resources on demand without having to rewrite the Kubernetes core API.</li> <li><strong>LLM</strong>: “LLM” stands for “Large Language Model.” Large language models are artificial intelligence models trained on vast amounts of text data to understand and generate human-like text. LLMs are a subset of machine learning models specifically designed for natural language processing (NLP) tasks.</li> <li><strong>LLMOps</strong>: LLMOps, which stands for Large Language Model Operations, encompasses the operational aspects tailored specifically for Large Language Models (LLMs). In essence, LLMOps is the adaptation of MLOps principles and tools to the unique requirements of LLM-powered applications, encompassing their entire lifecycle from development to deployment and maintenance.</li> <li><strong>MIG</strong>: Multi-Instance GPU technology is an innovation that allows a single physical GPU (Graphics Processing Unit) to be partitioned into multiple more minor instances, each operating as an independent GPU with its own resources and capabilities. This technology enhances GPU utilization and flexibility in data center and cloud computing environments.</li> <li><strong>MLOps</strong>: MLOps, short for machine learning operations, refers to the practices, methodologies, and tools used to streamline and automate machine learning models’ deployment, monitoring, and management in production environments. MLOps aims to bridge the gap between machine learning development and operations, ensuring that ML models are deployed efficiently, reliably, and at scale. It involves a combination of software engineering principles, DevOps practices, and specialized tools to automate the end-to-end ML lifecycle, including data preparation, model training, model deployment, monitoring, and maintenance. MLOps helps organizations accelerate their ML projects, improve model performance, and maintain consistency and reliability across the ML pipeline.</li> <li><strong>MPS</strong>: MPS stands for Multi-Process Service in the context of GPU computing. MPS technology allows multiple GPU-accelerated applications or processes to share a single physical GPU while maintaining isolation and efficient resource utilization.</li> <li><strong>RAG</strong>: In the context of AI, RAG stands for “Retrieval-Augmented Generation.” It’s a model architecture combining retrieval-based and generative models to produce text. RAG’s generation process is augmented with a retrieval mechanism that helps the model access relevant information from an extensive database or knowledge base. This retrieval component allows the model to incorporate external knowledge into the generation process, improving the quality and relevance of the generated text.</li> <li><strong>vGPU</strong>: vGPU, or Virtual Graphics Processing Unit, technology enables multiple virtual machines (VMs) to share a single physical GPU (Graphics Processing Unit). This technology efficiently utilizes GPU resources in virtualized environments such as cloud computing, data centers, and virtual desktop infrastructure (VDI).</li> </ul> <h3 id=references>References<a class=headerlink href=#references title="Permanent link">¶</a></h3> <ol> <li><a href=https://github.com/cncf/toc/blob/main/DEFINITION.md>https://github.com/cncf/toc/blob/main/DEFINITION.md</a></li> <li><a href=https://en.wikipedia.org/wiki/Microservices>https://en.wikipedia.org/wiki/Microservices</a></li> <li><a href=https://landscape.cncf.io/guide>https://landscape.cncf.io/guide</a></li> <li><a href=https://docs.aws.amazon.com/whitepapers/latest/build-secure-enterprise-ml-platform/personas-for-an-ml-platform.html>https://docs.aws.amazon.com/whitepapers/latest/build-secure-enterprise-ml-platform/personas-for-an-ml-platform.html</a></li> <li>First release of Docker March 20, 2013.</li> <li><a href=https://en.wikipedia.org/wiki/LXC>https://en.wikipedia.org/wiki/LXC</a></li> <li><a href=https://en.wikipedia.org/wiki/Docker_(software)>https://en.wikipedia.org/wiki/Docker_(software)</a></li> <li><a href=https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/44843.pdf>https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/44843.pdf</a></li> <li><a href="https://github.com/cncf/toc/blob/main/DEFINITION.md as of Jul 18, 202">https://github.com/cncf/toc/blob/main/DEFINITION.md as of Jul 18, 202</a></li> <li><a href=https://en.wikipedia.org/wiki/DevOps>https://en.wikipedia.org/wiki/DevOps</a></li> <li><a href=https://about.gitlab.com/topics/gitops/ >https://about.gitlab.com/topics/gitops/</a></li> <li><a href=https://ai100.stanford.edu/2016-report/appendix-i-short-history-ai>https://ai100.stanford.edu/2016-report/appendix-i-short-history-ai</a></li> <li><a href="https://youtu.be/P18EdAKuC1U?si=Dd74AdpbF3EgzVmn">https://youtu.be/P18EdAKuC1U?si=Dd74AdpbF3EgzVmn</a></li> <li><a href=https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf>https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf</a></li> <li><a href=https://arxiv.org/abs/2008.02217>https://arxiv.org/abs/2008.02217</a></li> <li><a href=https://openai.com/chatgpt>https://openai.com/chatgpt</a></li> <li><a href=https://en.wikipedia.org/wiki/Prompt_engineering#Retrieval-augmented_generation>https://en.wikipedia.org/wiki/Prompt_engineering#Retrieval-augmented_generation</a></li> <li><a href=https://github.com/zanetworker/ai-landscape>https://github.com/zanetworker/ai-landscape</a></li> <li><a href=https://openai.com/research/scaling-kubernetes-to-7500-nodes>https://openai.com/research/scaling-kubernetes-to-7500-nodes</a></li> <li><a href=https://huggingface.co/blog/hugging-face-endpoints-on-azure>https://huggingface.co/blog/hugging-face-endpoints-on-azure</a></li> <li><a href=https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/ >https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/</a></li> <li><a href=https://github.com/intel/platform-aware-scheduling/tree/master/gpu-aware-scheduling>https://github.com/intel/platform-aware-scheduling/tree/master/gpu-aware-scheduling</a></li> <li><a href=https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/ >https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/</a></li> <li><a href=https://opencontainers.org/ >https://opencontainers.org/</a></li> <li><a href=https://k8sgpt.ai/ >https://k8sgpt.ai/</a></li> <li><a href=https://docs.aws.amazon.com/whitepapers/latest/build-secure-enterprise-ml-platform/personas-for-an-ml-platform.html>https://docs.aws.amazon.com/whitepapers/latest/build-secure-enterprise-ml-platform/personas-for-an-ml-platform.html</a></li> <li><a href=https://www.ibm.com/topics/machine-learning-pipeline>https://www.ibm.com/topics/machine-learning-pipeline</a></li> <li><a href=https://docs.databricks.com/en/machine-learning/mlops/mlops-workflow.html>https://docs.databricks.com/en/machine-learning/mlops/mlops-workflow.html</a></li> <li><a href=https://cloud-native.slack.com/archives/C05TYJE81SR>https://cloud-native.slack.com/archives/C05TYJE81SR</a></li> <li><a href=https://www.intel.com/content/www/us/en/newsroom/resources/moores-law.html>https://www.intel.com/content/www/us/en/newsroom/resources/moores-law.html</a></li> <li><a href=https://gdpr-info.eu/ >https://gdpr-info.eu/</a></li> <li><a href=https://oag.ca.gov/privacy/ccpa>https://oag.ca.gov/privacy/ccpa</a></li> <li><a href=https://iapp.org/news/a/5-things-to-know-about-ai-model-cards/ >https://iapp.org/news/a/5-things-to-know-about-ai-model-cards/</a></li> <li><a href=https://arxiv.org/abs/2205.07147>https://arxiv.org/abs/2205.07147</a></li> <li><a href=https://kubernetes.io/docs/concepts/scheduling-eviction/dynamic-resource-allocation/ >https://kubernetes.io/docs/concepts/scheduling-eviction/dynamic-resource-allocation/</a></li> <li>Open Source for Sustainability</li> <li><a href=https://github.com/kserve/kserve/ >https://github.com/kserve/kserve/</a></li> <li>[2112.06905] GLaM: Efficient Scaling of Language Models with Mixture-of-Experts</li> <li>A carbon-aware workload dispatcher in multi-cluster Kubernetes environments for Cloud Native Sustainability Week 2023 | IBM Research scheduling-eviction/dynamic-resource-allocation/</li> <li><a href=https://mlco2.github.io/impact/ >https://mlco2.github.io/impact/</a></li> <li><a href=https://codecarbon.io/ >https://codecarbon.io/</a></li> <li><a href=https://yunikorn.apache.org/ >https://yunikorn.apache.org/</a></li> <li><a href=https://volcano.sh/ >https://volcano.sh/</a></li> <li><a href=https://kueue.sigs.k8s.io/ >https://kueue.sigs.k8s.io/</a></li> <li><a href=https://en.wikipedia.org/wiki/Gang_scheduling>https://en.wikipedia.org/wiki/Gang_scheduling</a></li> <li><a href=https://arxiv.org/abs/1706.03762>https://arxiv.org/abs/1706.03762</a></li> <li><a href=https://www.samsara.com/blog/building-a-modern-machine-learning-platform-with-ray>https://www.samsara.com/blog/building-a-modern-machine-learning-platform-with-ray</a></li> <li><a href=https://cloud.google.com/blog/products/ai-machine-learning/build-a-ml-platform-with-kubeflow-and-ray-on-gke>https://cloud.google.com/blog/products/ai-machine-learning/build-a-ml-platform-with-kubeflow-and-ray-on-gke</a></li> <li><a href=https://kueue.sigs.k8s.io/docs/tasks/run_rayjobs/ >https://kueue.sigs.k8s.io/docs/tasks/run_rayjobs/</a></li> <li><a href=https://cloud.google.com/blog/products/containers-kubernetes/use-ray-on-kubernetes-with-kuberay>https://cloud.google.com/blog/products/containers-kubernetes/use-ray-on-kubernetes-with-kuberay</a></li> <li><a href=https://www.redhat.com/en/blog/fine-tuning-and-serving-open-source-foundation-model-red-hat-openshift-ai>https://www.redhat.com/en/blog/fine-tuning-and-serving-open-source-foundation-model-red-hat-openshift-ai</a></li> <li><a href=https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/ >https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/</a></li> <li><a href=https://kubernetes.io/docs/concepts/scheduling-eviction/dynamic-resource-allocation/ >https://kubernetes.io/docs/concepts/scheduling-eviction/dynamic-resource-allocation/</a></li> <li><a href=https://keda.sh/ >https://keda.sh/</a></li> <li><a href=https://en.wikipedia.org/wiki/Open_Container_Initiative>https://en.wikipedia.org/wiki/Open_Container_Initiative</a></li> <li><a href=https://github.com/elyra-ai/elyra>https://github.com/elyra-ai/elyra</a></li> <li><a href=https://pytorch.org/torcharrow/beta/index.html>https://pytorch.org/torcharrow/beta/index.html</a></li> <li><a href=https://py.iceberg.apache.org/ >https://py.iceberg.apache.org/</a></li> <li><a href=https://docs.ray.io/en/latest/data/data.html>https://docs.ray.io/en/latest/data/data.html</a></li> <li><a href=https://kubernetes.io/docs/concepts/ >https://kubernetes.io/docs/concepts/</a></li> <li><a href="https://github.com/kubernetes/ enhancements/tree/master/keps/sig-node/3063-dynamic-resource-allocation">https://github.com/kubernetes/ enhancements/tree/master/keps/sig-node/3063-dynamic-resource-allocation</a></li> <li><a href=https://opentelemetry.io/ >https://opentelemetry.io/</a></li> <li><a href=https://www.cncf.io/projects/prometheus/ >https://www.cncf.io/projects/prometheus/</a></li> <li><a href=https://github.com/anchore/grype>https://github.com/anchore/grype</a></li> <li><a href=https://github.com/aquasecurity/trivy>https://github.com/aquasecurity/trivy</a></li> <li><a href=https://github.com/kyverno/kyverno>https://github.com/kyverno/kyverno</a></li> <li><a href=https://en.wikipedia.org/wiki/Confidential_computing>https://en.wikipedia.org/wiki/Confidential_computing</a></li> <li><a href=https://www.cutter.com/article/large-language-models-whats-environmental-impact>https://www.cutter.com/article/large-language-models-whats-environmental-impact</a></li> <li><a href=https://marksaroufim.substack.com/p/moral-language-models>https://marksaroufim.substack.com/p/moral-language-models</a></li> <li><a href=https://arxiv.org/pdf/2302.13971.pdf>https://arxiv.org/pdf/2302.13971.pdf</a></li> <li><a href=https://analyticsindiamag.com/the-environmental-impact-of-llms/ >https://analyticsindiamag.com/the-environmental-impact-of-llms/</a></li> <li><a href=https://landscape.lfai.foundation/ >https://landscape.lfai.foundation/</a></li> <li>Redis</li> <li>Vector database - Milvus</li> <li>facebookresearch/faiss: A library for efficient similarity search and clustering of dense vectors. (github.com)</li> <li>_</li> <li><a href=https://github.com/traceloop/openllmetry>https://github.com/traceloop/openllmetry</a></li> <li><a href=https://opentelemetry.io/ >https://opentelemetry.io/</a></li> <li><a href=https://lfaidata.foundation/ >https://lfaidata.foundation/</a></li> <li><a href=https://thealliance.ai/ >https://thealliance.ai/</a></li> <li><a href=https://landscape.cncf.io/ >https://landscape.cncf.io/</a></li> <li><a href=https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/program/kids-day/#kids-day>https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/program/kids-day/#kids-day</a></li> <li><a href=https://www.mooc.org/ >https://www.mooc.org/</a></li> <li><a href=https://www.acm.org/ >https://www.acm.org/</a></li> <li><a href=https://www.ieee.org/ >https://www.ieee.org/</a></li> <li><a href=https://www.cncf.io/ >https://www.cncf.io/</a></li> <li><a href=https://thealliance.ai/ >https://thealliance.ai/</a></li> <li>CSAM - child sexual abuse material</li> <li><a href=https://dtspartnership.org/wp-content/uploads/2023/01/DTSP_Trust-Safety-Glossary13023.pdf>https://dtspartnership.org/wp-content/uploads/2023/01/DTSP_Trust-Safety-Glossary13023.pdf</a></li> <li><a href=https://docs.google.com/document/d/1sXo-T3oEGcRTWmJJ_PUWwWyrvELi-mcEB3_27Nj_xkM/ >https://docs.google.com/document/d/1sXo-T3oEGcRTWmJJ_PUWwWyrvELi-mcEB3_27Nj_xkM/</a></li> <li><a href=https://www.ohchr.org/documents/publications/guidingprinciplesbusinesshr_en.pdf>https://www.ohchr.org/documents/publications/guidingprinciplesbusinesshr_en.pdf</a></li> <li><a href=https://www.weforum.org/projects/safety-by-design-sbd/ >https://www.weforum.org/projects/safety-by-design-sbd/</a></li> <li><a href=https://www.gifct.org>https://www.gifct.org</a></li> <li><a href=https://www.technologycoalition.org/ >https://www.technologycoalition.org/</a></li> <li><a href=https://www.internetsociety.org/ >https://www.internetsociety.org/</a></li> <li><a href=https://alltechishuman.org/responsible-tech-organizations>https://alltechishuman.org/responsible-tech-organizations</a></li> <li><a href=https://thealliance.ai/news>https://thealliance.ai/news</a></li> <li><a href=https://openai.com/about>https://openai.com/about</a></li> <li><a href="https://www.youtube.com/watch?v=1oTx7kgGeMg">https://www.youtube.com/watch?v=1oTx7kgGeMg</a></li> <li><a href=https://en.wikipedia.org/wiki/Red_team>https://en.wikipedia.org/wiki/Red_team</a></li> <li><a href=https://github.com/open-telemetry/semantic-conventions/issues/327>https://github.com/open-telemetry/semantic-conventions/issues/327</a></li> </ol> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button class="md-top md-icon" data-md-component=top hidden type=button> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg> Back to top </button> </main> <footer class=md-footer> <nav aria-label=Footer class="md-footer__inner md-grid"> <a aria-label="Previous: 2024 Large-scale AI Infrastructure Survey" class="md-footer__link md-footer__link--prev" href=0429-ai-survey.html> <div class="md-footer__button md-icon"> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> 2024 Large-scale AI Infrastructure Survey </div> </div> </a> <a aria-label="Next: Kimi Success and Other Domestic LLM" class="md-footer__link md-footer__link--next" href=0408-after-kimi.html> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> Kimi Success and Other Domestic LLM </div> </div> <div class="md-footer__button md-icon"> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"></path></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright © 2016 - 2024 d.run </div> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../../..", "features": ["content.code.annotate", "content.code.copy", "content.tooltips", "navigation.tabs", "navigation.prune", "navigation.sections", "navigation.tabs.sticky", "navigation.tracking", "navigation.top", "search.highlight", "search.suggest", "search.share", "toc.follow", "navigation.path", "navigation.footer"], "search": "../../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script> <script src=../../../assets/javascripts/bundle.60a45f97.min.js></script> <script src=../../../static/stylesheets/zoom_image.js></script> <script src=../../../overrides/assets/javascripts/bundle.b97a6647.min.js></script> <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?6b117ea770a78bebf27e63b402da0c4e";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script> <script type=module>
      import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10.2.0/+esm'
    </script> <script src="https://console.d.run/drun-copilot/chatbot-sdk.umd.js?ws=302&token=N2ZlNjFkZDItNDkyMy00Y2I1LWJlM2QtZDJlMzQ2YWM5OTE5"></script> <script>document$.subscribe(() => {
            window.update_swagger_ui_iframe_height = function (id) {
                var iFrameID = document.getElementById(id);
                if (iFrameID) {
                    full_height = (iFrameID.contentWindow.document.body.scrollHeight + 80) + "px";
                    iFrameID.height = full_height;
                    iFrameID.style.height = full_height;
                }
            }
        
            let iframe_id_list = []
            var iframes = document.getElementsByClassName("swagger-ui-iframe");
            for (var i = 0; i < iframes.length; i++) { 
                iframe_id_list.push(iframes[i].getAttribute("id"))
            }
        
            let ticking = true;
            
            document.addEventListener('scroll', function(e) {
                if (!ticking) {
                    window.requestAnimationFrame(()=> {
                        let half_vh = window.innerHeight/2;
                        for(var i = 0; i < iframe_id_list.length; i++) {
                            let element = document.getElementById(iframe_id_list[i])
                            if(element==null){
                                return
                            }
                            let diff = element.getBoundingClientRect().top
                            if(element.contentWindow.update_top_val){
                                element.contentWindow.update_top_val(half_vh - diff)
                            }
                        }
                        ticking = false;
                    });
                    ticking = true;
                }
            });
        
            const dark_scheme_name = "slate"
            
            window.scheme = document.body.getAttribute("data-md-color-scheme")
            const options = {
                attributeFilter: ['data-md-color-scheme'],
            };
            function color_scheme_callback(mutations) {
                for (let mutation of mutations) {
                    if (mutation.attributeName === "data-md-color-scheme") {
                        scheme = document.body.getAttribute("data-md-color-scheme")
                        var iframe_list = document.getElementsByClassName("swagger-ui-iframe")
                        for(var i = 0; i < iframe_list.length; i++) {
                            var ele = iframe_list.item(i);
                            if (ele) {
                                if (scheme === dark_scheme_name) {
                                    ele.contentWindow.enable_dark_mode();
                                } else {
                                    ele.contentWindow.disable_dark_mode();
                                }
                            }
                        }
                    }
                }
            }
            observer = new MutationObserver(color_scheme_callback);
            observer.observe(document.body, options);
            })</script></body> </html>