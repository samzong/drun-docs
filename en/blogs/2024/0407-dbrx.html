<!DOCTYPE html><html class=no-js lang=en> <head><meta charset=utf-8><meta content="width=device-width,initial-scale=1" name=viewport><meta content="d.run (DaoCloud Runs Intelligence)，揭示一个新一代软件体系下的全新算力世界，让算力更自由。" name=description><meta content=d.run name=author><link href=https://docs.d.run/en/blogs/2024/0407-dbrx.html rel=canonical><link href=0408-after-kimi.html rel=prev><link href=0403-cp-to-profit.html rel=next><link href=../../../images/favicon.ico rel=icon><meta content="mkdocs-1.6.1, mkdocs-material-9.5.50" name=generator><title>Introduction to DBRX Open Source LLM - d.run 让算力更自由</title><link href=../../../assets/stylesheets/main.a40c8224.min.css rel=stylesheet><link href=../../../assets/stylesheets/palette.06af60db.min.css rel=stylesheet><link href=../../../overrides/assets/stylesheets/main.e13ced4c.min.css rel=stylesheet><link crossorigin href=https://fonts.gstatic.com rel=preconnect><link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback" rel=stylesheet><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body data-md-color-accent=indigo data-md-color-primary=indigo data-md-color-scheme=default dir=ltr> <input autocomplete=off class=md-toggle data-md-toggle=drawer id=__drawer type=checkbox> <input autocomplete=off class=md-toggle data-md-toggle=search id=__search type=checkbox> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a class=md-skip href=#dbrx-introduction-a-new-powerful-open-source-llm-model> Skip to content </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> </div> </aside> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav aria-label=Header class="md-header__inner md-grid"> <a aria-label="d.run 让算力更自由" class="md-header__button md-logo" data-md-component=logo href=/ title="d.run 让算力更自由"> <img alt=logo src=../../../images/DaoCloud.png> </a> <label class="md-header__button md-icon" for=__drawer> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> d.run 让算力更自由 </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Introduction to DBRX Open Source LLM </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input aria-label="Switch to light mode" class=md-option data-md-color-accent=indigo data-md-color-media=(prefers-color-scheme) data-md-color-primary=indigo data-md-color-scheme=default id=__palette_0 name=__palette type=radio> <label class="md-header__button md-icon" for=__palette_1 hidden title="Switch to light mode"> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"></path></svg> </label> <input aria-label="Switch to dark mode" class=md-option data-md-color-accent=indigo data-md-color-media="(prefers-color-scheme: light)" data-md-color-primary=indigo data-md-color-scheme=default id=__palette_1 name=__palette type=radio> <label class="md-header__button md-icon" for=__palette_2 hidden title="Switch to dark mode"> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg> </label> <input aria-label="Switch to system preference" class=md-option data-md-color-accent=indigo data-md-color-media="(prefers-color-scheme: dark)" data-md-color-primary=indigo data-md-color-scheme=slate id=__palette_2 name=__palette type=radio> <label class="md-header__button md-icon" for=__palette_0 hidden title="Switch to system preference"> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <div class=md-header__option> <div class=md-select> <button aria-label="Select language" class="md-header__button md-icon"> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="m12.87 15.07-2.54-2.51.03-.03A17.5 17.5 0 0 0 14.07 6H17V4h-7V2H8v2H1v2h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2zm-2.62 7 1.62-4.33L19.12 17z"></path></svg> </button> <div class=md-select__inner> <ul class=md-select__list> <li class=md-select__item> <a class=md-select__link href=../../../blogs/2024/0407-dbrx.html hreflang=zh> 中文 </a> </li> <li class=md-select__item> <a class=md-select__link href=0407-dbrx.html hreflang=en> English </a> </li> </ul> </div> </div> </div> <label class="md-header__button md-icon" for=__search> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input aria-label=Search autocapitalize=off autocomplete=off autocorrect=off class=md-search__input data-md-component=search-query name=query placeholder=Search required spellcheck=false type=text> <label class="md-search__icon md-icon" for=__search> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg> </label> <nav aria-label=Search class=md-search__options> <a aria-label=Share class="md-search__icon md-icon" data-clipboard data-clipboard-text data-md-component=search-share href=javascript:void(0) tabindex=-1 title=Share> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"></path></svg> </a> <button aria-label=Clear class="md-search__icon md-icon" tabindex=-1 title=Clear type=reset> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix tabindex=0> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a class=md-source data-md-component=source href=https://github.com/d-run/drun-docs title="Go to repository"> <div class="md-source__icon md-icon"> <svg viewbox="0 0 496 512" xmlns=http://www.w3.org/2000/svg><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg> </div> <div class=md-source__repository> d-run/drun-docs </div> </a> </div> </nav> <nav aria-label=Tabs class=md-tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=https://www.daocloud.io/ class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a class=md-tabs__link href=../../index.html> d.run Documentation </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a class=md-tabs__link href=../index.html> Blogs </a> </li> <li class=md-tabs__item> <a class=md-tabs__link href=../../open/index.html> Knowledge from AI Industry </a> </li> <li class=md-tabs__item> <a class=md-tabs__link href=../../contact/index.html> Contact Us </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav aria-label=Navigation class="md-nav md-nav--primary md-nav--lifted" data-md-level=0> <label class=md-nav__title for=__drawer> <a aria-label="d.run 让算力更自由" class="md-nav__button md-logo" data-md-component=logo href=/ title="d.run 让算力更自由"> <img alt=logo src=../../../images/DaoCloud.png> </a> d.run 让算力更自由 </label> <div class=md-nav__source> <a class=md-source data-md-component=source href=https://github.com/d-run/drun-docs title="Go to repository"> <div class="md-source__icon md-icon"> <svg viewbox="0 0 496 512" xmlns=http://www.w3.org/2000/svg><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg> </div> <div class=md-source__repository> d-run/drun-docs </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=https://www.daocloud.io/ class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a class=md-nav__link href=../../index.html> <span class=md-ellipsis> d.run Documentation </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input checked class="md-nav__toggle md-toggle" id=__nav_3 type=checkbox> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex> <span class=md-ellipsis> Blogs </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=true aria-labelledby=__nav_3_label class=md-nav data-md-level=1> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Blogs </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a class=md-nav__link href=../index.html> <span class=md-ellipsis> Index </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=../2025/0102-ai-trend.html> <span class=md-ellipsis> AI Trend in 2025 </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=d.run.html> <span class=md-ellipsis> d.run is the Ideal Platform for Generative AI </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=0702-k8s-for-genai.html> <span class=md-ellipsis> K8s and Generative AI Make a Perfect Match </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=0514-gpt4o.html> <span class=md-ellipsis> OpenAI GPT-4o is Completely Free </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=0509-model-spec.html> <span class=md-ellipsis> OpenAI LLM Specifications </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=0429-ai-survey.html> <span class=md-ellipsis> 2024 Large-scale AI Infrastructure Survey </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=0410-cnai-wp.html> <span class=md-ellipsis> Cloud-native Artificial Intelligence White Paper </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=0408-after-kimi.html> <span class=md-ellipsis> Kimi Success and Other Domestic LLM </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" id=__toc type=checkbox> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> Introduction to DBRX Open Source LLM </span> <span class="md-nav__icon md-icon"></span> </label> <a class="md-nav__link md-nav__link--active" href=0407-dbrx.html> <span class=md-ellipsis> Introduction to DBRX Open Source LLM </span> </a> <nav aria-label=导航 class="md-nav md-nav--secondary"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> 导航 </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a class=md-nav__link href=#what-is-dbrx> <span class=md-ellipsis> What is DBRX? </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#quality-comparison-with-leading-open-models-in-benchmark-tests> <span class=md-ellipsis> Quality Comparison with Leading Open Models in Benchmark Tests </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#quality-comparison-with-leading-closed-source-models-in-benchmark-tests> <span class=md-ellipsis> Quality Comparison with Leading Closed-Source Models in Benchmark Tests </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#quality-in-long-context-tasks-and-rag> <span class=md-ellipsis> Quality in Long Context Tasks and RAG </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#training-efficiency> <span class=md-ellipsis> Training Efficiency </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#inference-efficiency> <span class=md-ellipsis> Inference Efficiency </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#how-we-built-dbrx> <span class=md-ellipsis> How We Built DBRX </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#getting-started-with-dbrx-on-databricks> <span class=md-ellipsis> Getting Started with DBRX on Databricks </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#conclusion> <span class=md-ellipsis> Conclusion </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#contributions> <span class=md-ellipsis> Contributions </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#references> <span class=md-ellipsis> References </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=0403-cp-to-profit.html> <span class=md-ellipsis> Transforms Compute into Profit </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=0327-transformer.html> <span class=md-ellipsis> Who Will Replace the Transformer </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=0326-compute-power.html> <span class=md-ellipsis> Financial Industry Welcomes the LLM Era </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a class=md-nav__link href=../../open/index.html> <span class=md-ellipsis> Knowledge from AI Industry </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=../../contact/index.html> <span class=md-ellipsis> Contact Us </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc hidden> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav aria-label=导航 class="md-nav md-nav--secondary"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> 导航 </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a class=md-nav__link href=#what-is-dbrx> <span class=md-ellipsis> What is DBRX? </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#quality-comparison-with-leading-open-models-in-benchmark-tests> <span class=md-ellipsis> Quality Comparison with Leading Open Models in Benchmark Tests </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#quality-comparison-with-leading-closed-source-models-in-benchmark-tests> <span class=md-ellipsis> Quality Comparison with Leading Closed-Source Models in Benchmark Tests </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#quality-in-long-context-tasks-and-rag> <span class=md-ellipsis> Quality in Long Context Tasks and RAG </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#training-efficiency> <span class=md-ellipsis> Training Efficiency </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#inference-efficiency> <span class=md-ellipsis> Inference Efficiency </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#how-we-built-dbrx> <span class=md-ellipsis> How We Built DBRX </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#getting-started-with-dbrx-on-databricks> <span class=md-ellipsis> Getting Started with DBRX on Databricks </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#conclusion> <span class=md-ellipsis> Conclusion </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#contributions> <span class=md-ellipsis> Contributions </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#references> <span class=md-ellipsis> References </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a class="md-content__button md-icon" href=https://github.com/d-run/drun-docs/edit/main/docs/zh/docs/en/blogs/2024/0407-dbrx.md title=查看编辑历史> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M13.5 8H12v5l4.28 2.54.72-1.21-3.5-2.08zM13 3a9 9 0 0 0-9 9H1l3.96 4.03L9 12H6a7 7 0 0 1 7-7 7 7 0 0 1 7 7 7 7 0 0 1-7 7c-1.93 0-3.68-.79-4.94-2.06l-1.42 1.42A8.9 8.9 0 0 0 13 21a9 9 0 0 0 9-9 9 9 0 0 0-9-9"></path></svg> </a> <a class="md-content__button md-icon" href=https://github.com/d-run/drun-docs/edit/main/docs/zh/docs/en/blogs/2024/0407-dbrx.md title=edit.link.title> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"></path></svg> </a> <a class="md-content__button md-icon" href=https://github.com/d-run/drun-docs/edit/main/docs/zh/docs/en/blogs/2024/0407-dbrx.md title=查看源文件> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"></path></svg> </a> <h1 id=dbrx-introduction-a-new-powerful-open-source-llm-model>DBRX Introduction: A New, Powerful Open Source LLM Model<a class=headerlink href=#dbrx-introduction-a-new-powerful-open-source-llm-model title="Permanent link">¶</a></h1> <blockquote> <p>Reprinted from <a href=https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm>databricks</a></p> </blockquote> <p><img alt="dbrx blog header" src=../images/dbrx01.png></p> <p>Today, we are excited to introduce DBRX, an open universal LLM created by Databricks. In a series of standard benchmark tests, DBRX has set new technical standards among established open LLMs. Furthermore, it provides capabilities that were previously limited to closed-source model APIs for the open community and enterprises building their own LLMs; according to our measurements, it surpasses GPT-3.5 and competes with Gemini 1.0 Pro. It is a particularly powerful code model that outperforms the specialized CodeLLaMA-70B model in programming, in addition to its advantages as a general-purpose LLM.</p> <p>This technological advancement comes with significant improvements in training and inference performance. With its fine-grained mixture of experts (MoE) architecture, DBRX has made groundbreaking progress in efficiency among open models. Its inference speed is twice that of LLaMA2-70B, while the total and active parameter counts of DBRX are only 40% of Grok-1. When hosted on Mosaic AI Model Serving, DBRX can generate text at a rate of 150 tokens per second per user. Our customers will find that training an MoE model requires about twice the FLOP efficiency compared to training a dense model of the same final model quality. From start to finish, our overall DBRX formula (including pre-training data, model architecture, and optimization strategies) achieves the same quality as our previous generation MPT models with nearly four times the computational resources.</p> <p><img alt="general knowledge infographic" src=../images/dbrx02.png></p> <p>Figure 1: DBRX outperforms established open models in language understanding (MMLU), programming (HumanEval), and mathematics (GSM8K).</p> <p>The weights of the base model (<a href=https://huggingface.co/databricks/dbrx-base>DBRX Base</a>) and fine-tuned model (<a href=https://huggingface.co/databricks/dbrx-instruct>DBRX Instruct</a>) are available on Hugging Face under an open license. Starting today, DBRX is accessible to Databricks customers via API, and Databricks customers can pre-train their own DBRX-like models from scratch or continue training using one of our checkpoints, employing the same tools and scientific methods we used to build the model. DBRX has already been integrated into our GenAI-powered products, where early versions have surpassed GPT-3.5 Turbo in applications such as SQL and are competing with GPT-4 Turbo. It is also a leading model in RAG tasks among open models and GPT-3.5 Turbo.</p> <p>Training mixture of experts models is challenging. We had to overcome various scientific and performance challenges to build a robust pipeline capable of repeatedly training DBRX-like models efficiently. Now that we have achieved this, we possess a unique training stack that allows any enterprise to train world-class MoE base models from scratch. We look forward to sharing this capability with our customers and sharing our lessons learned with the community.</p> <p>Download DBRX from Hugging Face (<a href=https://huggingface.co/databricks/dbrx-base>DBRX Base</a>, <a href=https://huggingface.co/databricks/dbrx-instruct>DBRX Instruct</a>), try DBRX Instruct in our <a href=https://huggingface.co/spaces/databricks/dbrx-instruct>HF Space</a>, or check out our model repository on GitHub: <a href=https://www.github.com/databricks/dbrx>databricks/dbrx</a>.</p> <h2 id=what-is-dbrx>What is DBRX?<a class=headerlink href=#what-is-dbrx title="Permanent link">¶</a></h2> <p>DBRX is a transformer-based decoder-only large language model (LLM) trained using next-token prediction. It employs a fine-grained mixture of experts (MoE) architecture, with a total of 132 billion parameters, of which 36 billion parameters are active for any input. It has been pre-trained on 12 trillion tokens of text and code data. Compared to other open MoE models like Mixtral and Grok-1, DBRX is fine-grained, meaning it uses a larger number of smaller experts. DBRX has 16 experts and selects 4 of them, while Mixtral and Grok-1 have 8 experts and select 2. This provides 65 times more combinations of experts, which we found can improve model quality. DBRX uses rotary positional encoding (RoPE), gated linear units (GLU), and grouped query attention (GQA). It employs the tokenizer from GPT-4, which is available in the <a href=https://github.com/openai/tiktoken>tiktoken</a> repository. We made these choices based on extensive evaluations and scaling experiments.</p> <p>DBRX has been pre-trained on a carefully curated dataset of 12 trillion tokens, with a maximum context length of 32k tokens. We estimate that this data is at least twice as good per token compared to the data used to pre-train the MPT series models. Using the full Databricks toolkit, including Apache Spark™ and Databricks notebooks for data processing, Unity Catalog for data management and governance, and MLflow for experiment tracking, we developed this new dataset. We employed curriculum learning in the pre-training phase, altering the data mix throughout training, which we found significantly enhances model quality.</p> <h2 id=quality-comparison-with-leading-open-models-in-benchmark-tests>Quality Comparison with Leading Open Models in Benchmark Tests<a class=headerlink href=#quality-comparison-with-leading-open-models-in-benchmark-tests title="Permanent link">¶</a></h2> <p>Table 1 shows the quality of DBRX Instruct compared to leading established open models. DBRX Instruct leads in composite benchmark tests, programming and math benchmarks, and MMLU. It surpasses all chat or instruction fine-tuned models on standard benchmark tests.</p> <p><strong>Composite Benchmark Tests.</strong> We evaluated DBRX Instruct and other models on two composite benchmark tests: the <a href=https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard>Hugging Face Open LLM Leaderboard</a> (average scores for ARC-Challenge, HellaSwag, MMLU, TruthfulQA, WinoGrande, and GSM8k) and the <a href=https://github.com/mosaicml/llm-foundry/blob/main/scripts/eval/local_data/EVAL_GAUNTLET.md>Databricks Model Gauntlet</a> (over 30 tasks covering world knowledge, common sense reasoning, language understanding, reading comprehension, symbolic problem-solving, and programming).</p> <p>In the models we evaluated, DBRX Instruct scored highest on both composite benchmark tests: Hugging Face Open LLM Leaderboard (74.5%, with the next highest model being Mixtral Instruct at 72.7%) and Databricks Gauntlet (66.8%, with the next highest model being Mixtral Instruct at 60.7%).</p> <p><strong>Programming and Mathematics.</strong> DBRX Instruct performs particularly well in programming and mathematics. In HumanEval (70.1%, Grok-1 at 63.2%, Mixtral Instruct at 54.8%, and the best-performing variant of LLaMA2-70B at 32.2%) and GSM8k (66.9%, Grok-1 at 62.9%, Mixtral Instruct at 61.1%, and the best-performing variant of LLaMA2-70B at 54.1%), its scores exceed those of all other open models we evaluated. Despite Grok-1 having 2.4 times the number of parameters as DBRX, DBRX outperformed Grok-1 on HumanEval, even though DBRX Instruct was designed for general-purpose use (Meta reported a score of 70.1% on HumanEval for the CodeLLaMA model, which was specialized for programming, yielding a score of 67.8%).</p> <p><strong>MMLU.</strong> DBRX Instruct scored higher on MMLU than all other models we considered, achieving a score of 73.7%.</p> <table> <thead> <tr> <th><strong>Model</strong></th> <th>DBRX Instruct</th> <th>Mixtral Instruct</th> <th>Mixtral Base</th> <th>LLaMA2-70B Chat</th> <th>LLaMA2-70B Base</th> <th>Grok-1</th> </tr> </thead> <tbody> <tr> <td><strong>Open LLM Leaderboard (average of below 6 rows)</strong></td> <td><strong>74.5%</strong></td> <td>72.7%</td> <td>68.4%</td> <td>62.4%</td> <td>67.9%</td> <td>—</td> </tr> <tr> <td><strong>ARC-challenge 25-shot</strong></td> <td>68.9%</td> <td><strong>70.1%</strong></td> <td>66.4%</td> <td>64.6%</td> <td>67.3%</td> <td>—</td> </tr> <tr> <td><strong>HellaSwag 10-shot</strong></td> <td><strong>89.0%</strong></td> <td>87.6%</td> <td>86.5%</td> <td>85.9%</td> <td>87.3%</td> <td>—</td> </tr> <tr> <td><strong>MMLU 5-shot</strong></td> <td><strong>73.7%</strong></td> <td>71.4%</td> <td>71.9%</td> <td>63.9%</td> <td>69.8%</td> <td>73.0%</td> </tr> <tr> <td><strong>Truthful QA 0-shot</strong></td> <td><strong>66.9%</strong></td> <td>65.0%</td> <td>46.8%</td> <td>52.8%</td> <td>44.9%</td> <td>—</td> </tr> <tr> <td><strong>WinoGrande 5-shot</strong></td> <td>81.8%</td> <td>81.1%</td> <td>81.7%</td> <td>80.5%</td> <td><strong>83.7%</strong></td> <td>—</td> </tr> <tr> <td><strong>GSM8k CoT 5-shot maj@13</strong></td> <td><strong>66.9%</strong></td> <td>61.1%</td> <td>57.6%</td> <td>26.7%</td> <td>54.1%</td> <td>62.9% (8-shot)</td> </tr> <tr> <td><strong>Gauntlet v0.34 (average of 30+ diverse tasks)</strong></td> <td><strong>66.8%</strong></td> <td>60.7%</td> <td>56.8%</td> <td>52.8%</td> <td>56.4%</td> <td>—</td> </tr> <tr> <td><strong>HumanEval 0-Shot, pass@1 (Programming)</strong></td> <td><strong>70.1%</strong></td> <td>54.8%</td> <td>40.2%</td> <td>32.2%</td> <td>31.0%</td> <td>63.2%</td> </tr> </tbody> </table> <p>Table 1. Quality of DBRX Instruct compared to leading open models. For details on how the numbers were collected, see the footnotes. Bold and underlined indicate the highest scores.</p> <h2 id=quality-comparison-with-leading-closed-source-models-in-benchmark-tests>Quality Comparison with Leading Closed-Source Models in Benchmark Tests<a class=headerlink href=#quality-comparison-with-leading-closed-source-models-in-benchmark-tests title="Permanent link">¶</a></h2> <p>Table 2 shows the quality of DBRX Instruct compared to leading closed-source models. Based on scores reported by each model's creators, DBRX Instruct surpasses GPT-3.5 (as described in the GPT-4 paper) and competes with Gemini 1.0 Pro and Mistral Medium.</p> <p>In nearly all benchmark tests we considered, DBRX Instruct either surpassed or matched GPT-3.5. DBRX Instruct outperformed GPT-3.5 on MMLU (overall score of 73.7% vs. 70.0% for GPT-3.5), as well as on common sense reasoning tasks like HellaSwag (89.0% vs. 85.5%) and WinoGrande (81.8% vs. 81.6%). DBRX Instruct excels in programming and mathematical reasoning, scoring particularly well on HumanEval (70.1% vs. 48.1%) and GSM8k (72.8% vs. 57.1%). DBRX Instruct competes with Gemini 1.0 Pro and Mistral Medium. DBRX Instruct scores higher than Gemini 1.0 Pro on Inflection Corrected MTBench, MMLU, HellaSwag, and HumanEval, while Gemini 1.0 Pro is stronger on GSM8k. DBRX Instruct and Mistral Medium have similar scores on HellaSwag, while Mistral Medium is stronger on Winogrande and MMLU, and DBRX Instruct is stronger on HumanEval, GSM8k, and Inflection Corrected MTBench.</p> <table> <thead> <tr> <th><strong>Model</strong></th> <th>DBRX Instruct</th> <th><a href=https://arxiv.org/pdf/2303.08774.pdf>GPT-3.5</a></th> <th><a href=https://arxiv.org/pdf/2303.08774.pdf>GPT-4</a></th> <th><a href=https://www.anthropic.com/news/claude-3-family>Claude 3 Haiku</a></th> <th><a href=https://www.anthropic.com/news/claude-3-family>Claude 3 Sonnet</a></th> <th><a href=https://www.anthropic.com/news/claude-3-family>Claude 3 Opus</a></th> <th><a href=https://arxiv.org/abs/2312.11805>Gemini 1.0 Pro</a></th> <th><a href=https://arxiv.org/abs/2403.05530>Gemini 1.5 Pro</a></th> <th><a href=https://docs.mistral.ai/platform/endpoints/ >Mistral Medium</a></th> <th><a href=https://mistral.ai/news/mistral-large/ >Mistral Large</a></th> </tr> </thead> <tbody> <tr> <td><strong>MT Bench (</strong><a href=https://inflection.ai/inflection-2-5><strong>Inflection corrected</strong></a><strong>, n=5)</strong></td> <td>8.39 ± 0.08</td> <td>—</td> <td>—</td> <td>8.41 ± 0.04</td> <td>8.54 ± 0.09</td> <td>9.03 ± 0.06</td> <td>8.23 ± 0.08</td> <td>—</td> <td>8.05 ± 0.12</td> <td>8.90 ± 0.06</td> </tr> <tr> <td><strong>MMLU 5-shot</strong></td> <td>73.7%</td> <td>70.0%</td> <td>86.4%</td> <td>75.2%</td> <td>79.0%</td> <td>86.8%</td> <td>71.8%</td> <td>81.9%</td> <td>75.3%</td> <td>81.2%</td> </tr> <tr> <td><strong>HellaSwag 10-shot</strong></td> <td>89.0%</td> <td>85.5%</td> <td>95.3%</td> <td>85.9%</td> <td>89.0%</td> <td>95.4%</td> <td>84.7%</td> <td>92.5%</td> <td>88.0%</td> <td>89.2%</td> </tr> <tr> <td><strong>HumanEval 0-Shot</strong> <strong>pass@1</strong> <strong>(Programming)</strong></td> <td>70.1% temp=0, N=1</td> <td>48.1%</td> <td>67.0%</td> <td>75.9%</td> <td>73.0%</td> <td>84.9%</td> <td>67.7%</td> <td>71.9%</td> <td>38.4%</td> <td>45.1%</td> </tr> <tr> <td><strong>GSM8k CoT maj@1</strong></td> <td>72.8% (5-shot)</td> <td>57.1% (5-shot)</td> <td>92.0% (5-shot)</td> <td>88.9%</td> <td>92.3%</td> <td>95.0%</td> <td>86.5%(maj1<a class="magiclink magiclink-github magiclink-mention" href=https://github.com/32 title="GitHub User: 32">@32</a>)</td> <td>91.7% (11-shot)</td> <td><a href=https://twitter.com/IntuitMachine/status/1734189967948288464/photo/1>66.7% (5-shot)</a></td> <td>81.0% (5-shot)</td> </tr> <tr> <td><strong>WinoGrande 5-shot</strong></td> <td>81.8%</td> <td>81.6%</td> <td>87.5%</td> <td>—</td> <td>—</td> <td>—</td> <td>—</td> <td>—</td> <td>88.0%</td> <td>86.7%</td> </tr> </tbody> </table> <p>Table 2. Quality of DBRX Instruct compared to leading closed-source models. Except for Inflection Corrected MTBench (data we measured ourselves at the model endpoints), all other numbers are reported by the creators of these models in their respective white papers. For details, see the footnotes.</p> <h2 id=quality-in-long-context-tasks-and-rag>Quality in Long Context Tasks and RAG<a class=headerlink href=#quality-in-long-context-tasks-and-rag title="Permanent link">¶</a></h2> <p>DBRX Instruct used a context window of up to 32K tokens during training. Table 3 compares its performance on a set of long context benchmark tests (KV-Pairs and HotpotQAXL from the <a href=https://arxiv.org/abs/2307.03172>Lost in the Middle</a> paper, which modifies HotPotQA to extend tasks to longer sequence lengths) with Mixtral Instruct and the latest versions of GPT-3.5 Turbo and GPT-4 Turbo APIs. GPT-4 Turbo is generally the best model in these tasks. However, with one exception, DBRX Instruct outperforms GPT-3.5 Turbo across all context lengths and parts of the sequences. Overall, the performance of DBRX Instruct is similar to that of Mixtral Instruct.</p> <table> <thead> <tr> <th><strong>Model</strong></th> <th>DBRX Instruct</th> <th>Mixtral Instruct</th> <th>GPT-3.5 Turbo (API)</th> <th>GPT-4 Turbo (API)</th> </tr> </thead> <tbody> <tr> <td><strong>Answer in the first third of the context</strong></td> <td>45.1%</td> <td>41.3%</td> <td>37.3%*</td> <td><strong>49.3%</strong></td> </tr> <tr> <td><strong>Answer in the middle third of the context</strong></td> <td>45.3%</td> <td>42.7%</td> <td>37.3%*</td> <td><strong>49.0%</strong></td> </tr> <tr> <td><strong>Answer in the last third of the context</strong></td> <td>48.0%</td> <td>44.4%</td> <td>37.0%*</td> <td><strong>50.9%</strong></td> </tr> <tr> <td><strong>2K context</strong></td> <td>59.1%</td> <td>64.6%</td> <td>36.3%</td> <td><strong>69.3%</strong></td> </tr> <tr> <td><strong>4K context</strong></td> <td><strong>65.1%</strong></td> <td>59.9%</td> <td>35.9%</td> <td>63.5%</td> </tr> <tr> <td><strong>8K context</strong></td> <td>59.5%</td> <td>55.3%</td> <td>45.0%</td> <td><strong>61.5%</strong></td> </tr> <tr> <td><strong>16K context</strong></td> <td>27.0%</td> <td>20.1%</td> <td><strong>31.7%</strong></td> <td>26.0%</td> </tr> <tr> <td><strong>32K context</strong></td> <td>19.9%</td> <td>14.0%</td> <td>—</td> <td><strong>28.5%</strong></td> </tr> </tbody> </table> <p>Table 3. Average performance of models on KV-Pairs and HotpotQAXL benchmark tests. Bold indicates the highest score. Underline indicates the highest score excluding GPT-4 Turbo. GPT-3.5 Turbo supports a maximum context length of 16K, so we could not evaluate it on 32K. The averages for the beginning, middle, and end of GPT-3.5 Turbo are based only on contexts not exceeding 16K.</p> <p>Using RAG (retrieval-augmented generation) is one of the most popular methods for leveraging model context. In RAG, content relevant to the prompt is retrieved from a database and provided to the model along with the prompt to give it more information than it would have on its own. Table 4 shows the quality of DBRX in two RAG benchmark tests (Natural Questions and HotPotQA) when the model also provided the top 10 passages retrieved using the embedding model bge-large-en-v1.5 from a Wikipedia article corpus. DBRX Instruct competes with open models like Mixtral Instruct and LLaMA2-70B Chat, as well as the current version of GPT-3.5 Turbo.</p> <table> <thead> <tr> <th><strong>Model</strong></th> <th>DBRX Instruct</th> <th>Mixtral Instruct</th> <th>LLaMa2-70B Chat</th> <th>GPT 3.5 Turbo (API)</th> <th>GPT 4 Turbo (API)</th> </tr> </thead> <tbody> <tr> <td><strong>Natural Questions</strong></td> <td>60.0%</td> <td>59.1%</td> <td>56.5%</td> <td>57.7%</td> <td><strong>63.9%</strong></td> </tr> <tr> <td><strong>HotPotQA</strong></td> <td>55.0%</td> <td>54.2%</td> <td>54.7%</td> <td>53.0%</td> <td><strong>62.9%</strong></td> </tr> </tbody> </table> <p>Table 4. Performance of models when provided with the top 10 passages retrieved from the Wikipedia corpus using bge-large-en-v1.5. Accuracy is measured by matching the model's answers. Bold indicates the highest score. Underline indicates the highest score excluding GPT-4 Turbo.</p> <h2 id=training-efficiency>Training Efficiency<a class=headerlink href=#training-efficiency title="Permanent link">¶</a></h2> <p>Model quality must be viewed in the context of training and usage efficiency. This is especially important at Databricks, as we build models like DBRX to establish processes for customers to train their own foundational models.</p> <p>We found that training mixture of experts models offers significant improvements in training efficiency (Table 5). For example, training a smaller member of the DBRX family, called DBRX MoE-B (23.5B total parameters, 6.6B active parameters), required 1.7 times fewer FLOPs to achieve a score of 45.5% on the Databricks LLM Gauntlet than the FLOPs required for LLaMA2-13B to achieve a score of 43.8%. The number of active parameters in DBRX MoE-B is also only half that of LLaMA2-13B.</p> <p>Overall, our end-to-end LLM pre-training process has become nearly more efficient over the past ten months. On May 5, 2023, we released <a href=https://www.databricks.com/blog/mpt-7b>MPT-7B</a>, a 7B parameter model trained on 1 trillion tokens that achieved a score of 30.9% on the Databricks LLM Gauntlet. A member of the DBRX family, called DBRX MoE-A (7.7B total parameters, 2.2B active parameters), achieved a score of 30.5% on the Databricks Gauntlet, requiring 3.7 times fewer FLOPs than MPT-7B to achieve a score of 30.9%. This efficiency improvement is the result of many enhancements, including the use of MoE architectures, other architectural changes to the network, better optimization strategies, better tokenization, and, importantly, better pre-training data.</p> <p>Independently, better pre-training data has a significant impact on model quality. We trained a 7B model (called DBRX Dense-A) using DBRX pre-training data on 1 trillion tokens. It achieved a score of 39.0% on the Databricks Gauntlet, while MPT-7B scored 30.9%. We estimate that our new pre-training data is at least twice as good per token compared to the data used to train MPT-7B. In other words, we estimate that only half the number of tokens is needed to achieve the same model quality. We confirmed this by training DBRX Dense-A with 500 billion tokens; it outperformed MPT-7B on the Databricks Gauntlet, achieving a score of 32.1%. Besides better data quality, another significant contributor to token efficiency may be the tokenizer from GPT-4, which has a large vocabulary and is considered particularly efficient in token efficiency. These insights about improving data quality directly translate into practices and tools for our customers to train foundational models based on their own data.</p> <table> <thead> <tr> <th><strong>Model</strong></th> <th>Total Parameters</th> <th>Active Parameters</th> <th>Gauntlet Score</th> <th>Relative FLOP</th> </tr> </thead> <tbody> <tr> <td><strong>DBRX MoE-A</strong></td> <td>7.7B</td> <td>2.2B</td> <td>30.5%</td> <td>1x</td> </tr> <tr> <td><strong>MPT-7B (1T tokens)</strong></td> <td>—</td> <td>6.7B</td> <td>30.9%</td> <td>3.7x</td> </tr> <tr> <td><strong>DBRX Dense-A (1T tokens)</strong></td> <td>—</td> <td>6.7B</td> <td>39.0%</td> <td>3.7x</td> </tr> <tr> <td><strong>DBRX Dense-A (500B tokens)</strong></td> <td>—</td> <td>6.7B</td> <td>32.1%</td> <td>1.85x</td> </tr> <tr> <td><strong>DBRX MoE-B</strong></td> <td>23.5B</td> <td>6.6B</td> <td>45.5%</td> <td>1x</td> </tr> <tr> <td><strong>LLaMA2-13B</strong></td> <td>—</td> <td>13.0B</td> <td>43.8%</td> <td>1.7x</td> </tr> </tbody> </table> <p>Table 5. Details of several test articles we used to validate the DBRX MoE architecture and end-to-end training process.</p> <h2 id=inference-efficiency>Inference Efficiency<a class=headerlink href=#inference-efficiency title="Permanent link">¶</a></h2> <p>Figure 2 shows the end-to-end inference efficiency provided for DBRX and similar models using NVIDIA TensorRT-LLM on our optimized service infrastructure and at 16-bit precision. We aim for this benchmark to be as close to actual usage scenarios as possible, including multiple users simultaneously accessing the same inference server. We generate a new user every second, with each user request containing approximately 2000 tokens of prompts and each response containing 256 tokens.</p> <p>In general, MoE models are faster in inference than their total parameter count would suggest. This is because they use relatively fewer parameters for each input. We found that DBRX is no exception in this regard. DBRX's inference throughput is 2 to 3 times higher than that of a non-MoE model with 132B parameters.</p> <p>Inference efficiency and model quality are often trade-offs: larger models typically achieve higher quality, but smaller models are more efficient in inference. Using MoE architecture can achieve better model quality and inference efficiency than dense models usually provide. For example, DBRX outperforms LLaMA2-70B in quality, and due to having approximately half the number of active parameters, DBRX's inference throughput is twice that of LLaMA2-70B (Figure 2). Mixtral is another point on the improved Pareto frontier achieved by MoE models: it is smaller than DBRX, so it scores lower in quality but has higher inference throughput. Users of the Databricks base model API can see DBRX achieving 150 tokens per second on our optimized model service platform, using 8-bit quantization.</p> <p><img alt="dbrx inference efficiency " src=../images/dbrx03.png></p> <p>Figure 2. Inference throughput for various model configurations using NVIDIA TensorRT-LLM at 16-bit precision on our optimized service infrastructure. Models run in tensor parallelism across the nodes. Input prompts contain approximately 2000 prompt tokens, and we generate 256 output tokens. A new user is generated every second.</p> <h2 id=how-we-built-dbrx>How We Built DBRX<a class=headerlink href=#how-we-built-dbrx title="Permanent link">¶</a></h2> <p>DBRX was trained on a 3.2Tbps Infiniband connected by 3072 NVIDIA H100s. The main processes for building DBRX—including pre-training, post-training processing, evaluation, red teaming, and improvements—were conducted over three months. This was based on several months of scientific and dataset research and scaling experiments at Databricks, not to mention Databricks' years of experience in LLM development, including the MPT and Dolly projects, as well as the thousands of models we have built and deployed in production with our customers.</p> <p>To build DBRX, we utilized the same Databricks toolkit available to our customers. We used Unity Catalog to manage and govern our training data. We explored this data using newly acquired Lilac AI. We processed and cleaned the data using Apache Spark™ and Databricks notebooks. We trained DBRX using an optimized version of our open-source training library: MegaBlocks, LLM Foundry, Composer, and Streaming. We managed large-scale model training and fine-tuning across thousands of GPUs using Mosaic AI Training services. We recorded our results using MLflow. We collected human feedback through Mosaic AI Model Serving and Inference Tables to improve quality and safety. We manually experimented with models using the Databricks Playground. We found that Databricks tools excel in their respective uses and that we benefit from them being part of a unified product experience.</p> <h2 id=getting-started-with-dbrx-on-databricks>Getting Started with DBRX on Databricks<a class=headerlink href=#getting-started-with-dbrx-on-databricks title="Permanent link">¶</a></h2> <p>If you want to start using DBRX immediately, you can easily access it through Databricks Mosaic AI <a href=https://docs.databricks.com/en/machine-learning/foundation-models/index.html>Foundation Model APIs</a>. You can get started quickly with our pay-as-you-go pricing and query the model through our <a href=https://docs.databricks.com/en/large-language-models/ai-playground.html>AI Playground</a> chat interface. For production applications, we offer a provisioned throughput option to provide performance guarantees, support fine-tuned models, and ensure additional safety and compliance. To privately host DBRX, you can download the model from the <a href=https://marketplace.databricks.com/details/357c33c9-7cd3-48d2-bb5b-b4a88172d193/Databricks_DBRX-Models>Databricks Marketplace</a> and deploy it on <a href=https://learn.microsoft.com/en-us/azure/databricks/machine-learning/foundation-models/deploy-prov-throughput-foundation-model-apis>Model Serving</a>.</p> <h2 id=conclusion>Conclusion<a class=headerlink href=#conclusion title="Permanent link">¶</a></h2> <p>At Databricks, we believe that every enterprise should be able to take control of its data and destiny in the emerging GenAI world. DBRX is a core pillar of our next-generation GenAI products, and we look forward to the exciting journey our customers will take as they leverage the capabilities of DBRX and the tools we used to build it. Over the past year, we have trained thousands of LLMs with our customers. DBRX is just one example of the powerful and efficient models that Databricks builds, suitable for a variety of applications, from internal functionalities to our customers' ambitious use cases.</p> <p>For any new model, the journey of DBRX is just the beginning; the best work will be done by those who build on it: enterprises and the open community. This is just the beginning of our work on DBRX, and you should expect more results to come.</p> <h2 id=contributions>Contributions<a class=headerlink href=#contributions title="Permanent link">¶</a></h2> <p>The development of DBRX is led by the <a href=https://www.databricks.com/research/mosaic>Mosaic</a> team, which previously built the MPT model series and collaborated with dozens of engineers, lawyers, procurement and finance experts, project managers, marketers, designers, and other contributors across various departments at Databricks. We thank our colleagues, friends, families, and communities for their patience and support over the past months.</p> <p>In creating DBRX, we stand on the shoulders of giants in the open and academic communities. By making DBRX publicly available, we hope to give back to the community and look forward to building greater technologies together in the future. In this context, we are especially grateful for the work and collaboration of <a href="https://scholar.google.com/citations?user=uMzPswkAAAAJ&hl=en">Trevor Gale</a> and his <a href=https://github.com/stanford-futuredata/megablocks>MegaBlocks</a> project (Trevor's PhD advisor is Databricks CTO Matei Zaharia), the <a href=https://pytorch.org/ >PyTorch</a> team and the <a href=https://arxiv.org/abs/2304.11277>FSDP</a> project, <a href=https://www.nvidia.com/ >NVIDIA</a> and the <a href=https://github.com/NVIDIA/TensorRT-LLM>TensorRT-LLM</a> project, the <a href=https://github.com/vllm-project/vllm>vLLM</a> team and project, <a href=https://www.eleuther.ai/ >EleutherAI</a> and their <a href=https://www.eleuther.ai/projects/large-language-model-evaluation>LLM evaluation</a> project, Daniel Smilkov and Nikhil Thorat from <a href=http://www.lilacml.com/ >Lilac AI</a>, and our friends at the <a href=https://allenai.org/ >Allen Institute for Artificial Intelligence (AI2)</a> for their work and collaboration.</p> <h2 id=references>References<a class=headerlink href=#references title="Permanent link">¶</a></h2> <ul> <li><a href=https://huggingface.co/spaces/databricks/dbrx-instruct>Experience DBRX on HuggingFace</a></li> <li><a href=https://huggingface.co/databricks/dbrx-base>Open weights on HuggingFace</a></li> <li><a href=https://github.com/databricks/dbrx>DBRX GitHub Repository</a></li> </ul> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button class="md-top md-icon" data-md-component=top hidden type=button> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg> Back to top </button> </main> <footer class=md-footer> <nav aria-label=Footer class="md-footer__inner md-grid"> <a aria-label="Previous: Kimi Success and Other Domestic LLM" class="md-footer__link md-footer__link--prev" href=0408-after-kimi.html> <div class="md-footer__button md-icon"> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> Kimi Success and Other Domestic LLM </div> </div> </a> <a aria-label="Next: Transforms Compute into Profit" class="md-footer__link md-footer__link--next" href=0403-cp-to-profit.html> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> Transforms Compute into Profit </div> </div> <div class="md-footer__button md-icon"> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"></path></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright © 2016 - 2024 d.run </div> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../../..", "features": ["content.code.annotate", "content.code.copy", "content.tooltips", "navigation.tabs", "navigation.prune", "navigation.sections", "navigation.tabs.sticky", "navigation.tracking", "navigation.top", "search.highlight", "search.suggest", "search.share", "toc.follow", "navigation.path", "navigation.footer"], "search": "../../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script> <script src=../../../assets/javascripts/bundle.60a45f97.min.js></script> <script src=../../../static/stylesheets/zoom_image.js></script> <script src=../../../overrides/assets/javascripts/bundle.b97a6647.min.js></script> <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?6b117ea770a78bebf27e63b402da0c4e";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script> <script type=module>
      import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10.2.0/+esm'
    </script> <script src="https://console.d.run/drun-copilot/chatbot-sdk.umd.js?ws=302&token=N2ZlNjFkZDItNDkyMy00Y2I1LWJlM2QtZDJlMzQ2YWM5OTE5"></script> <script>document$.subscribe(() => {
            window.update_swagger_ui_iframe_height = function (id) {
                var iFrameID = document.getElementById(id);
                if (iFrameID) {
                    full_height = (iFrameID.contentWindow.document.body.scrollHeight + 80) + "px";
                    iFrameID.height = full_height;
                    iFrameID.style.height = full_height;
                }
            }
        
            let iframe_id_list = []
            var iframes = document.getElementsByClassName("swagger-ui-iframe");
            for (var i = 0; i < iframes.length; i++) { 
                iframe_id_list.push(iframes[i].getAttribute("id"))
            }
        
            let ticking = true;
            
            document.addEventListener('scroll', function(e) {
                if (!ticking) {
                    window.requestAnimationFrame(()=> {
                        let half_vh = window.innerHeight/2;
                        for(var i = 0; i < iframe_id_list.length; i++) {
                            let element = document.getElementById(iframe_id_list[i])
                            if(element==null){
                                return
                            }
                            let diff = element.getBoundingClientRect().top
                            if(element.contentWindow.update_top_val){
                                element.contentWindow.update_top_val(half_vh - diff)
                            }
                        }
                        ticking = false;
                    });
                    ticking = true;
                }
            });
        
            const dark_scheme_name = "slate"
            
            window.scheme = document.body.getAttribute("data-md-color-scheme")
            const options = {
                attributeFilter: ['data-md-color-scheme'],
            };
            function color_scheme_callback(mutations) {
                for (let mutation of mutations) {
                    if (mutation.attributeName === "data-md-color-scheme") {
                        scheme = document.body.getAttribute("data-md-color-scheme")
                        var iframe_list = document.getElementsByClassName("swagger-ui-iframe")
                        for(var i = 0; i < iframe_list.length; i++) {
                            var ele = iframe_list.item(i);
                            if (ele) {
                                if (scheme === dark_scheme_name) {
                                    ele.contentWindow.enable_dark_mode();
                                } else {
                                    ele.contentWindow.disable_dark_mode();
                                }
                            }
                        }
                    }
                }
            }
            observer = new MutationObserver(color_scheme_callback);
            observer.observe(document.body, options);
            })</script></body> </html>